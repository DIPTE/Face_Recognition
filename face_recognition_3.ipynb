{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1oLDpqAqCTXt5PGfg_Q9ZnnMrAJwJjTUT",
      "authorship_tag": "ABX9TyO2D0Qz1zaKQ6oksLd7JzFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPTE/face_recognition/blob/master/face_recognition_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJETdM0zg7sY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78f77a0b-ac31-42bc-d41f-e651da8a5386"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SyInndKh4BN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8711806-d2fa-43a4-99ca-3a4cee06f832"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE6Sg5ywkPL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53793a22-4678-4710-97f3-ea73a8ebf24f"
      },
      "source": [
        "!tar -xzvf LFW.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0006.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0003.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0008.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0005.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0007.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0009.jpg\n",
            "LFW/lfw_align_112/Richard_Gere/Richard_Gere_0010.jpg\n",
            "LFW/lfw_align_112/Thabo_Mbeki/\n",
            "LFW/lfw_align_112/Thabo_Mbeki/Thabo_Mbeki_0002.jpg\n",
            "LFW/lfw_align_112/Thabo_Mbeki/Thabo_Mbeki_0001.jpg\n",
            "LFW/lfw_align_112/Thabo_Mbeki/Thabo_Mbeki_0004.jpg\n",
            "LFW/lfw_align_112/Thabo_Mbeki/Thabo_Mbeki_0005.jpg\n",
            "LFW/lfw_align_112/Thabo_Mbeki/Thabo_Mbeki_0003.jpg\n",
            "LFW/lfw_align_112/Robert_McKee/\n",
            "LFW/lfw_align_112/Robert_McKee/Robert_McKee_0001.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0019.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0003.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0018.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0008.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0005.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0011.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0015.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0002.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0007.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0001.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0017.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0010.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0014.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0012.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0020.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0006.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0016.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0004.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0009.jpg\n",
            "LFW/lfw_align_112/Angelina_Jolie/Angelina_Jolie_0013.jpg\n",
            "LFW/lfw_align_112/Caroline_Kennedy/\n",
            "LFW/lfw_align_112/Caroline_Kennedy/Caroline_Kennedy_0001.jpg\n",
            "LFW/lfw_align_112/Caroline_Kennedy/Caroline_Kennedy_0002.jpg\n",
            "LFW/lfw_align_112/Caroline_Kennedy/Caroline_Kennedy_0003.jpg\n",
            "LFW/lfw_align_112/Garry_McCoy/\n",
            "LFW/lfw_align_112/Garry_McCoy/Garry_McCoy_0001.jpg\n",
            "LFW/lfw_align_112/Ben_Howland/\n",
            "LFW/lfw_align_112/Ben_Howland/Ben_Howland_0001.jpg\n",
            "LFW/lfw_align_112/Ben_Howland/Ben_Howland_0002.jpg\n",
            "LFW/lfw_align_112/Ben_Howland/Ben_Howland_0003.jpg\n",
            "LFW/lfw_align_112/Ben_Howland/Ben_Howland_0004.jpg\n",
            "LFW/lfw_align_112/Tyler_Hamilton/\n",
            "LFW/lfw_align_112/Tyler_Hamilton/Tyler_Hamilton_0002.jpg\n",
            "LFW/lfw_align_112/Tyler_Hamilton/Tyler_Hamilton_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Backley/\n",
            "LFW/lfw_align_112/Steve_Backley/Steve_Backley_0002.jpg\n",
            "LFW/lfw_align_112/Steve_Backley/Steve_Backley_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Gatti/\n",
            "LFW/lfw_align_112/Joe_Gatti/Joe_Gatti_0002.jpg\n",
            "LFW/lfw_align_112/Joe_Gatti/Joe_Gatti_0001.jpg\n",
            "LFW/lfw_align_112/James_Wallack/\n",
            "LFW/lfw_align_112/James_Wallack/James_Wallack_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Andrew/\n",
            "LFW/lfw_align_112/Mark_Andrew/Mark_Andrew_0001.jpg\n",
            "LFW/lfw_align_112/Gina_Centrello/\n",
            "LFW/lfw_align_112/Gina_Centrello/Gina_Centrello_0001.jpg\n",
            "LFW/lfw_align_112/Franko_Simatovic/\n",
            "LFW/lfw_align_112/Franko_Simatovic/Franko_Simatovic_0002.jpg\n",
            "LFW/lfw_align_112/Franko_Simatovic/Franko_Simatovic_0001.jpg\n",
            "LFW/lfw_align_112/Amporn_Falise/\n",
            "LFW/lfw_align_112/Amporn_Falise/Amporn_Falise_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Kartman/\n",
            "LFW/lfw_align_112/Charles_Kartman/Charles_Kartman_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Kartman/Charles_Kartman_0002.jpg\n",
            "LFW/lfw_align_112/Jennifer_McCoy/\n",
            "LFW/lfw_align_112/Jennifer_McCoy/Jennifer_McCoy_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Hoellwarth/\n",
            "LFW/lfw_align_112/Martin_Hoellwarth/Martin_Hoellwarth_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Hoellwarth/Martin_Hoellwarth_0002.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0014.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0004.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0018.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0013.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0009.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0012.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0016.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0002.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0005.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0019.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0006.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0001.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0015.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0007.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0011.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0003.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0017.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0010.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0008.jpg\n",
            "LFW/lfw_align_112/Igor_Ivanov/Igor_Ivanov_0020.jpg\n",
            "LFW/lfw_align_112/Robert_Towne/\n",
            "LFW/lfw_align_112/Robert_Towne/Robert_Towne_0001.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0003.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0009.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0002.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0001.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0006.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0007.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0004.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0005.jpg\n",
            "LFW/lfw_align_112/Colin_Farrell/Colin_Farrell_0008.jpg\n",
            "LFW/lfw_align_112/Marsha_Sharp/\n",
            "LFW/lfw_align_112/Marsha_Sharp/Marsha_Sharp_0001.jpg\n",
            "LFW/lfw_align_112/Gerald_Riley/\n",
            "LFW/lfw_align_112/Gerald_Riley/Gerald_Riley_0001.jpg\n",
            "LFW/lfw_align_112/Serge_Melac/\n",
            "LFW/lfw_align_112/Serge_Melac/Serge_Melac_0001.jpg\n",
            "LFW/lfw_align_112/Chuck_Hagel/\n",
            "LFW/lfw_align_112/Chuck_Hagel/Chuck_Hagel_0001.jpg\n",
            "LFW/lfw_align_112/Gore_Verbinski/\n",
            "LFW/lfw_align_112/Gore_Verbinski/Gore_Verbinski_0001.jpg\n",
            "LFW/lfw_align_112/Mireya_Moscoso/\n",
            "LFW/lfw_align_112/Mireya_Moscoso/Mireya_Moscoso_0005.jpg\n",
            "LFW/lfw_align_112/Mireya_Moscoso/Mireya_Moscoso_0003.jpg\n",
            "LFW/lfw_align_112/Mireya_Moscoso/Mireya_Moscoso_0004.jpg\n",
            "LFW/lfw_align_112/Mireya_Moscoso/Mireya_Moscoso_0001.jpg\n",
            "LFW/lfw_align_112/Mireya_Moscoso/Mireya_Moscoso_0002.jpg\n",
            "LFW/lfw_align_112/Ibrahim_Rugova/\n",
            "LFW/lfw_align_112/Ibrahim_Rugova/Ibrahim_Rugova_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Schultz/\n",
            "LFW/lfw_align_112/Peter_Schultz/Peter_Schultz_0001.jpg\n",
            "LFW/lfw_align_112/Timbul_Silaen/\n",
            "LFW/lfw_align_112/Timbul_Silaen/Timbul_Silaen_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Kolanas/\n",
            "LFW/lfw_align_112/Chris_Kolanas/Chris_Kolanas_0001.jpg\n",
            "LFW/lfw_align_112/Adel_Al-Jubeir/\n",
            "LFW/lfw_align_112/Adel_Al-Jubeir/Adel_Al-Jubeir_0002.jpg\n",
            "LFW/lfw_align_112/Adel_Al-Jubeir/Adel_Al-Jubeir_0003.jpg\n",
            "LFW/lfw_align_112/Adel_Al-Jubeir/Adel_Al-Jubeir_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Weiss/\n",
            "LFW/lfw_align_112/Michael_Weiss/Michael_Weiss_0001.jpg\n",
            "LFW/lfw_align_112/Robert_F_Kennedy_Jr/\n",
            "LFW/lfw_align_112/Robert_F_Kennedy_Jr/Robert_F_Kennedy_Jr_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0010.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0011.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0007.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0016.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0014.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0013.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0015.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0009.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0008.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0012.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0004.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0005.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0017.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0003.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0006.jpg\n",
            "LFW/lfw_align_112/Michael_Schumacher/Michael_Schumacher_0018.jpg\n",
            "LFW/lfw_align_112/David_Bisbal/\n",
            "LFW/lfw_align_112/David_Bisbal/David_Bisbal_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Curley/\n",
            "LFW/lfw_align_112/Tim_Curley/Tim_Curley_0001.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0008.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0006.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0005.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0003.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0004.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0002.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0009.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0001.jpg\n",
            "LFW/lfw_align_112/Hugh_Grant/Hugh_Grant_0007.jpg\n",
            "LFW/lfw_align_112/Princess_Stephanie/\n",
            "LFW/lfw_align_112/Princess_Stephanie/Princess_Stephanie_0001.jpg\n",
            "LFW/lfw_align_112/Bono/\n",
            "LFW/lfw_align_112/Bono/Bono_0002.jpg\n",
            "LFW/lfw_align_112/Bono/Bono_0001.jpg\n",
            "LFW/lfw_align_112/Bono/Bono_0003.jpg\n",
            "LFW/lfw_align_112/Lucie_Lapovsky/\n",
            "LFW/lfw_align_112/Lucie_Lapovsky/Lucie_Lapovsky_0001.jpg\n",
            "LFW/lfw_align_112/Joseph_Kabila/\n",
            "LFW/lfw_align_112/Joseph_Kabila/Joseph_Kabila_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Hayes/\n",
            "LFW/lfw_align_112/Bob_Hayes/Bob_Hayes_0001.jpg\n",
            "LFW/lfw_align_112/Derek_Lowe/\n",
            "LFW/lfw_align_112/Derek_Lowe/Derek_Lowe_0001.jpg\n",
            "LFW/lfw_align_112/Derek_Lowe/Derek_Lowe_0002.jpg\n",
            "LFW/lfw_align_112/Todd_Petit/\n",
            "LFW/lfw_align_112/Todd_Petit/Todd_Petit_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Menendez/\n",
            "LFW/lfw_align_112/Bob_Menendez/Bob_Menendez_0001.jpg\n",
            "LFW/lfw_align_112/Guillermo_Ruiz_Polanco/\n",
            "LFW/lfw_align_112/Guillermo_Ruiz_Polanco/Guillermo_Ruiz_Polanco_0001.jpg\n",
            "LFW/lfw_align_112/Guangdong_Ou_Guangyuan/\n",
            "LFW/lfw_align_112/Guangdong_Ou_Guangyuan/Guangdong_Ou_Guangyuan_0001.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0008.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0011.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0003.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0002.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0006.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0001.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0005.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0009.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0004.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0007.jpg\n",
            "LFW/lfw_align_112/Catherine_Zeta-Jones/Catherine_Zeta-Jones_0010.jpg\n",
            "LFW/lfw_align_112/Mike_Thibault/\n",
            "LFW/lfw_align_112/Mike_Thibault/Mike_Thibault_0001.jpg\n",
            "LFW/lfw_align_112/Milton_Wynants/\n",
            "LFW/lfw_align_112/Milton_Wynants/Milton_Wynants_0001.jpg\n",
            "LFW/lfw_align_112/William_Cocksedge/\n",
            "LFW/lfw_align_112/William_Cocksedge/William_Cocksedge_0001.jpg\n",
            "LFW/lfw_align_112/Nathan_Lane/\n",
            "LFW/lfw_align_112/Nathan_Lane/Nathan_Lane_0001.jpg\n",
            "LFW/lfw_align_112/Nathan_Lane/Nathan_Lane_0002.jpg\n",
            "LFW/lfw_align_112/Kathleen_Abernathy/\n",
            "LFW/lfw_align_112/Kathleen_Abernathy/Kathleen_Abernathy_0001.jpg\n",
            "LFW/lfw_align_112/Elena_Tihomirova/\n",
            "LFW/lfw_align_112/Elena_Tihomirova/Elena_Tihomirova_0001.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0003.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0013.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0022.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0015.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0001.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0011.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0012.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0020.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0014.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0019.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0018.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0007.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0010.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0004.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0002.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0005.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0017.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0021.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0006.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0009.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0008.jpg\n",
            "LFW/lfw_align_112/Naomi_Watts/Naomi_Watts_0016.jpg\n",
            "LFW/lfw_align_112/Kenenisa_Bekele/\n",
            "LFW/lfw_align_112/Kenenisa_Bekele/Kenenisa_Bekele_0001.jpg\n",
            "LFW/lfw_align_112/Zoran_Djindjic/\n",
            "LFW/lfw_align_112/Zoran_Djindjic/Zoran_Djindjic_0003.jpg\n",
            "LFW/lfw_align_112/Zoran_Djindjic/Zoran_Djindjic_0004.jpg\n",
            "LFW/lfw_align_112/Zoran_Djindjic/Zoran_Djindjic_0001.jpg\n",
            "LFW/lfw_align_112/Zoran_Djindjic/Zoran_Djindjic_0002.jpg\n",
            "LFW/lfw_align_112/Gianna_Angelopoulos-Daskalaki/\n",
            "LFW/lfw_align_112/Gianna_Angelopoulos-Daskalaki/Gianna_Angelopoulos-Daskalaki_0003.jpg\n",
            "LFW/lfw_align_112/Gianna_Angelopoulos-Daskalaki/Gianna_Angelopoulos-Daskalaki_0001.jpg\n",
            "LFW/lfw_align_112/Gianna_Angelopoulos-Daskalaki/Gianna_Angelopoulos-Daskalaki_0002.jpg\n",
            "LFW/lfw_align_112/Barbra_Streisand/\n",
            "LFW/lfw_align_112/Barbra_Streisand/Barbra_Streisand_0002.jpg\n",
            "LFW/lfw_align_112/Barbra_Streisand/Barbra_Streisand_0003.jpg\n",
            "LFW/lfw_align_112/Barbra_Streisand/Barbra_Streisand_0001.jpg\n",
            "LFW/lfw_align_112/Charlie_Coles/\n",
            "LFW/lfw_align_112/Charlie_Coles/Charlie_Coles_0001.jpg\n",
            "LFW/lfw_align_112/Shigeo_Nagashima/\n",
            "LFW/lfw_align_112/Shigeo_Nagashima/Shigeo_Nagashima_0001.jpg\n",
            "LFW/lfw_align_112/Brandon_Jones/\n",
            "LFW/lfw_align_112/Brandon_Jones/Brandon_Jones_0001.jpg\n",
            "LFW/lfw_align_112/Mohammaed_Ahmad_Al_Jarallah/\n",
            "LFW/lfw_align_112/Mohammaed_Ahmad_Al_Jarallah/Mohammaed_Ahmad_Al_Jarallah_0001.jpg\n",
            "LFW/lfw_align_112/Teruaki_Masumoto/\n",
            "LFW/lfw_align_112/Teruaki_Masumoto/Teruaki_Masumoto_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Ballmer/\n",
            "LFW/lfw_align_112/Steve_Ballmer/Steve_Ballmer_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Ballmer/Steve_Ballmer_0002.jpg\n",
            "LFW/lfw_align_112/Steve_Ballmer/Steve_Ballmer_0003.jpg\n",
            "LFW/lfw_align_112/Demetrin_Veal/\n",
            "LFW/lfw_align_112/Demetrin_Veal/Demetrin_Veal_0001.jpg\n",
            "LFW/lfw_align_112/Xiang_Liu/\n",
            "LFW/lfw_align_112/Xiang_Liu/Xiang_Liu_0001.jpg\n",
            "LFW/lfw_align_112/Leslie_Moonves/\n",
            "LFW/lfw_align_112/Leslie_Moonves/Leslie_Moonves_0001.jpg\n",
            "LFW/lfw_align_112/Leslie_Moonves/Leslie_Moonves_0002.jpg\n",
            "LFW/lfw_align_112/Bryan_Adams/\n",
            "LFW/lfw_align_112/Bryan_Adams/Bryan_Adams_0001.jpg\n",
            "LFW/lfw_align_112/Isabelle_Huppert/\n",
            "LFW/lfw_align_112/Isabelle_Huppert/Isabelle_Huppert_0001.jpg\n",
            "LFW/lfw_align_112/Isabelle_Huppert/Isabelle_Huppert_0002.jpg\n",
            "LFW/lfw_align_112/Sigourney_Weaver/\n",
            "LFW/lfw_align_112/Sigourney_Weaver/Sigourney_Weaver_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Ambrose/\n",
            "LFW/lfw_align_112/Stephen_Ambrose/Stephen_Ambrose_0002.jpg\n",
            "LFW/lfw_align_112/Stephen_Ambrose/Stephen_Ambrose_0001.jpg\n",
            "LFW/lfw_align_112/Eve_Ensler/\n",
            "LFW/lfw_align_112/Eve_Ensler/Eve_Ensler_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0002.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0016.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0010.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0004.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0020.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0012.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0008.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0017.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0019.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0015.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0011.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0003.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0014.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0006.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0018.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0005.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0007.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0013.jpg\n",
            "LFW/lfw_align_112/Paul_Bremer/Paul_Bremer_0009.jpg\n",
            "LFW/lfw_align_112/Miranda_Otto/\n",
            "LFW/lfw_align_112/Miranda_Otto/Miranda_Otto_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Dalton/\n",
            "LFW/lfw_align_112/Scott_Dalton/Scott_Dalton_0001.jpg\n",
            "LFW/lfw_align_112/Eddie_Compass/\n",
            "LFW/lfw_align_112/Eddie_Compass/Eddie_Compass_0001.jpg\n",
            "LFW/lfw_align_112/Arnaud_Clement/\n",
            "LFW/lfw_align_112/Arnaud_Clement/Arnaud_Clement_0001.jpg\n",
            "LFW/lfw_align_112/Arnaud_Clement/Arnaud_Clement_0002.jpg\n",
            "LFW/lfw_align_112/Billy_Boyd/\n",
            "LFW/lfw_align_112/Billy_Boyd/Billy_Boyd_0001.jpg\n",
            "LFW/lfw_align_112/Alexa_Vega/\n",
            "LFW/lfw_align_112/Alexa_Vega/Alexa_Vega_0001.jpg\n",
            "LFW/lfw_align_112/Jan_Pronk/\n",
            "LFW/lfw_align_112/Jan_Pronk/Jan_Pronk_0001.jpg\n",
            "LFW/lfw_align_112/Leon_Silver/\n",
            "LFW/lfw_align_112/Leon_Silver/Leon_Silver_0001.jpg\n",
            "LFW/lfw_align_112/Neil_Goldman/\n",
            "LFW/lfw_align_112/Neil_Goldman/Neil_Goldman_0001.jpg\n",
            "LFW/lfw_align_112/Dario_Franchitti/\n",
            "LFW/lfw_align_112/Dario_Franchitti/Dario_Franchitti_0001.jpg\n",
            "LFW/lfw_align_112/Maria_Shkolnikova/\n",
            "LFW/lfw_align_112/Maria_Shkolnikova/Maria_Shkolnikova_0001.jpg\n",
            "LFW/lfw_align_112/Heinz_Feldmann/\n",
            "LFW/lfw_align_112/Heinz_Feldmann/Heinz_Feldmann_0003.jpg\n",
            "LFW/lfw_align_112/Heinz_Feldmann/Heinz_Feldmann_0002.jpg\n",
            "LFW/lfw_align_112/Heinz_Feldmann/Heinz_Feldmann_0001.jpg\n",
            "LFW/lfw_align_112/Ana_Claudia_Talancon/\n",
            "LFW/lfw_align_112/Ana_Claudia_Talancon/Ana_Claudia_Talancon_0001.jpg\n",
            "LFW/lfw_align_112/Kareena_Kapoor/\n",
            "LFW/lfw_align_112/Kareena_Kapoor/Kareena_Kapoor_0001.jpg\n",
            "LFW/lfw_align_112/John_White/\n",
            "LFW/lfw_align_112/John_White/John_White_0001.jpg\n",
            "LFW/lfw_align_112/Jennifer_Pena/\n",
            "LFW/lfw_align_112/Jennifer_Pena/Jennifer_Pena_0001.jpg\n",
            "LFW/lfw_align_112/Carol_Moseley_Braun/\n",
            "LFW/lfw_align_112/Carol_Moseley_Braun/Carol_Moseley_Braun_0001.jpg\n",
            "LFW/lfw_align_112/Carol_Moseley_Braun/Carol_Moseley_Braun_0002.jpg\n",
            "LFW/lfw_align_112/Gabrielle_Rose/\n",
            "LFW/lfw_align_112/Gabrielle_Rose/Gabrielle_Rose_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Kostelnik/\n",
            "LFW/lfw_align_112/Michael_Kostelnik/Michael_Kostelnik_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Kostelnik/Michael_Kostelnik_0001.jpg\n",
            "LFW/lfw_align_112/Susan_Whelan/\n",
            "LFW/lfw_align_112/Susan_Whelan/Susan_Whelan_0001.jpg\n",
            "LFW/lfw_align_112/Paul-Henri_Mathieu/\n",
            "LFW/lfw_align_112/Paul-Henri_Mathieu/Paul-Henri_Mathieu_0001.jpg\n",
            "LFW/lfw_align_112/Paul-Henri_Mathieu/Paul-Henri_Mathieu_0002.jpg\n",
            "LFW/lfw_align_112/Paul-Henri_Mathieu/Paul-Henri_Mathieu_0003.jpg\n",
            "LFW/lfw_align_112/Richard_Regenhard/\n",
            "LFW/lfw_align_112/Richard_Regenhard/Richard_Regenhard_0001.jpg\n",
            "LFW/lfw_align_112/Petria_Thomas/\n",
            "LFW/lfw_align_112/Petria_Thomas/Petria_Thomas_0002.jpg\n",
            "LFW/lfw_align_112/Petria_Thomas/Petria_Thomas_0001.jpg\n",
            "LFW/lfw_align_112/Petria_Thomas/Petria_Thomas_0003.jpg\n",
            "LFW/lfw_align_112/Franklin_Damann/\n",
            "LFW/lfw_align_112/Franklin_Damann/Franklin_Damann_0001.jpg\n",
            "LFW/lfw_align_112/Greg_Hennigar/\n",
            "LFW/lfw_align_112/Greg_Hennigar/Greg_Hennigar_0001.jpg\n",
            "LFW/lfw_align_112/Hans_Leistritz/\n",
            "LFW/lfw_align_112/Hans_Leistritz/Hans_Leistritz_0001.jpg\n",
            "LFW/lfw_align_112/Leandro_Andrade/\n",
            "LFW/lfw_align_112/Leandro_Andrade/Leandro_Andrade_0001.jpg\n",
            "LFW/lfw_align_112/Bobby_Kielty/\n",
            "LFW/lfw_align_112/Bobby_Kielty/Bobby_Kielty_0001.jpg\n",
            "LFW/lfw_align_112/Nan_Wang/\n",
            "LFW/lfw_align_112/Nan_Wang/Nan_Wang_0001.jpg\n",
            "LFW/lfw_align_112/Nan_Wang/Nan_Wang_0002.jpg\n",
            "LFW/lfw_align_112/Nan_Wang/Nan_Wang_0004.jpg\n",
            "LFW/lfw_align_112/Nan_Wang/Nan_Wang_0003.jpg\n",
            "LFW/lfw_align_112/Daniela_Hantuchova/\n",
            "LFW/lfw_align_112/Daniela_Hantuchova/Daniela_Hantuchova_0001.jpg\n",
            "LFW/lfw_align_112/Daniela_Hantuchova/Daniela_Hantuchova_0002.jpg\n",
            "LFW/lfw_align_112/Peter_Sejna/\n",
            "LFW/lfw_align_112/Peter_Sejna/Peter_Sejna_0001.jpg\n",
            "LFW/lfw_align_112/Saeed_Mortazavi/\n",
            "LFW/lfw_align_112/Saeed_Mortazavi/Saeed_Mortazavi_0001.jpg\n",
            "LFW/lfw_align_112/Pedro_Mahecha/\n",
            "LFW/lfw_align_112/Pedro_Mahecha/Pedro_Mahecha_0001.jpg\n",
            "LFW/lfw_align_112/Valdas_Adamkus/\n",
            "LFW/lfw_align_112/Valdas_Adamkus/Valdas_Adamkus_0002.jpg\n",
            "LFW/lfw_align_112/Valdas_Adamkus/Valdas_Adamkus_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Rolen/\n",
            "LFW/lfw_align_112/Scott_Rolen/Scott_Rolen_0001.jpg\n",
            "LFW/lfw_align_112/Fabiola_Zuluaga/\n",
            "LFW/lfw_align_112/Fabiola_Zuluaga/Fabiola_Zuluaga_0001.jpg\n",
            "LFW/lfw_align_112/Fabiola_Zuluaga/Fabiola_Zuluaga_0002.jpg\n",
            "LFW/lfw_align_112/Marisol_Breton/\n",
            "LFW/lfw_align_112/Marisol_Breton/Marisol_Breton_0001.jpg\n",
            "LFW/lfw_align_112/Mikhail_Khodorkovsky/\n",
            "LFW/lfw_align_112/Mikhail_Khodorkovsky/Mikhail_Khodorkovsky_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Hyatt/\n",
            "LFW/lfw_align_112/Robert_Hyatt/Robert_Hyatt_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Fisher/\n",
            "LFW/lfw_align_112/Mike_Fisher/Mike_Fisher_0001.jpg\n",
            "LFW/lfw_align_112/Chyung_Dai-chul/\n",
            "LFW/lfw_align_112/Chyung_Dai-chul/Chyung_Dai-chul_0001.jpg\n",
            "LFW/lfw_align_112/Danny_Glover/\n",
            "LFW/lfw_align_112/Danny_Glover/Danny_Glover_0001.jpg\n",
            "LFW/lfw_align_112/Bruce_Gebhardt/\n",
            "LFW/lfw_align_112/Bruce_Gebhardt/Bruce_Gebhardt_0001.jpg\n",
            "LFW/lfw_align_112/Mary_McCarty/\n",
            "LFW/lfw_align_112/Mary_McCarty/Mary_McCarty_0001.jpg\n",
            "LFW/lfw_align_112/Placido_Domingo/\n",
            "LFW/lfw_align_112/Placido_Domingo/Placido_Domingo_0001.jpg\n",
            "LFW/lfw_align_112/Placido_Domingo/Placido_Domingo_0003.jpg\n",
            "LFW/lfw_align_112/Placido_Domingo/Placido_Domingo_0002.jpg\n",
            "LFW/lfw_align_112/John_Wolf/\n",
            "LFW/lfw_align_112/John_Wolf/John_Wolf_0001.jpg\n",
            "LFW/lfw_align_112/John_Wolf/John_Wolf_0002.jpg\n",
            "LFW/lfw_align_112/Raza_Rabbani/\n",
            "LFW/lfw_align_112/Raza_Rabbani/Raza_Rabbani_0001.jpg\n",
            "LFW/lfw_align_112/Prince_Edward/\n",
            "LFW/lfw_align_112/Prince_Edward/Prince_Edward_0001.jpg\n",
            "LFW/lfw_align_112/Prince_Edward/Prince_Edward_0002.jpg\n",
            "LFW/lfw_align_112/Penelope_Cruz/\n",
            "LFW/lfw_align_112/Penelope_Cruz/Penelope_Cruz_0001.jpg\n",
            "LFW/lfw_align_112/Penelope_Cruz/Penelope_Cruz_0003.jpg\n",
            "LFW/lfw_align_112/Penelope_Cruz/Penelope_Cruz_0002.jpg\n",
            "LFW/lfw_align_112/Ruben_Studdard/\n",
            "LFW/lfw_align_112/Ruben_Studdard/Ruben_Studdard_0001.jpg\n",
            "LFW/lfw_align_112/Ruben_Studdard/Ruben_Studdard_0002.jpg\n",
            "LFW/lfw_align_112/Sheila_Wellstone/\n",
            "LFW/lfw_align_112/Sheila_Wellstone/Sheila_Wellstone_0001.jpg\n",
            "LFW/lfw_align_112/Sheila_Wellstone/Sheila_Wellstone_0002.jpg\n",
            "LFW/lfw_align_112/Chin-Hui_Tsao/\n",
            "LFW/lfw_align_112/Chin-Hui_Tsao/Chin-Hui_Tsao_0001.jpg\n",
            "LFW/lfw_align_112/John_Perrota/\n",
            "LFW/lfw_align_112/John_Perrota/John_Perrota_0001.jpg\n",
            "LFW/lfw_align_112/Roger_Winter/\n",
            "LFW/lfw_align_112/Roger_Winter/Roger_Winter_0001.jpg\n",
            "LFW/lfw_align_112/Margerry_Bakley/\n",
            "LFW/lfw_align_112/Margerry_Bakley/Margerry_Bakley_0001.jpg\n",
            "LFW/lfw_align_112/Toshihiko_Fukui/\n",
            "LFW/lfw_align_112/Toshihiko_Fukui/Toshihiko_Fukui_0001.jpg\n",
            "LFW/lfw_align_112/Toshihiko_Fukui/Toshihiko_Fukui_0003.jpg\n",
            "LFW/lfw_align_112/Toshihiko_Fukui/Toshihiko_Fukui_0002.jpg\n",
            "LFW/lfw_align_112/Mike_Szymanczyk/\n",
            "LFW/lfw_align_112/Mike_Szymanczyk/Mike_Szymanczyk_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Silas/\n",
            "LFW/lfw_align_112/Stephen_Silas/Stephen_Silas_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Mugabe/\n",
            "LFW/lfw_align_112/Robert_Mugabe/Robert_Mugabe_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Mugabe/Robert_Mugabe_0002.jpg\n",
            "LFW/lfw_align_112/Cathy_Freeman/\n",
            "LFW/lfw_align_112/Cathy_Freeman/Cathy_Freeman_0001.jpg\n",
            "LFW/lfw_align_112/Cathy_Freeman/Cathy_Freeman_0002.jpg\n",
            "LFW/lfw_align_112/Demi_Moore/\n",
            "LFW/lfw_align_112/Demi_Moore/Demi_Moore_0001.jpg\n",
            "LFW/lfw_align_112/Demi_Moore/Demi_Moore_0003.jpg\n",
            "LFW/lfw_align_112/Demi_Moore/Demi_Moore_0002.jpg\n",
            "LFW/lfw_align_112/Demi_Moore/Demi_Moore_0004.jpg\n",
            "LFW/lfw_align_112/Olesya_Bonabarenko/\n",
            "LFW/lfw_align_112/Olesya_Bonabarenko/Olesya_Bonabarenko_0001.jpg\n",
            "LFW/lfw_align_112/Olesya_Bonabarenko/Olesya_Bonabarenko_0002.jpg\n",
            "LFW/lfw_align_112/Chen_Kaige/\n",
            "LFW/lfw_align_112/Chen_Kaige/Chen_Kaige_0001.jpg\n",
            "LFW/lfw_align_112/Miles_Stewart/\n",
            "LFW/lfw_align_112/Miles_Stewart/Miles_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Mariam_Ali_Hassan/\n",
            "LFW/lfw_align_112/Mariam_Ali_Hassan/Mariam_Ali_Hassan_0001.jpg\n",
            "LFW/lfw_align_112/Troy_Aikman/\n",
            "LFW/lfw_align_112/Troy_Aikman/Troy_Aikman_0001.jpg\n",
            "LFW/lfw_align_112/Adrian_Murrell/\n",
            "LFW/lfw_align_112/Adrian_Murrell/Adrian_Murrell_0001.jpg\n",
            "LFW/lfw_align_112/John_Lithgow/\n",
            "LFW/lfw_align_112/John_Lithgow/John_Lithgow_0001.jpg\n",
            "LFW/lfw_align_112/Andy_Bryant/\n",
            "LFW/lfw_align_112/Andy_Bryant/Andy_Bryant_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Austin/\n",
            "LFW/lfw_align_112/Steve_Austin/Steve_Austin_0001.jpg\n",
            "LFW/lfw_align_112/Johnson_Panjaitan/\n",
            "LFW/lfw_align_112/Johnson_Panjaitan/Johnson_Panjaitan_0001.jpg\n",
            "LFW/lfw_align_112/Johnson_Panjaitan/Johnson_Panjaitan_0002.jpg\n",
            "LFW/lfw_align_112/David_Surrett/\n",
            "LFW/lfw_align_112/David_Surrett/David_Surrett_0001.jpg\n",
            "LFW/lfw_align_112/Christy_Turlington/\n",
            "LFW/lfw_align_112/Christy_Turlington/Christy_Turlington_0001.jpg\n",
            "LFW/lfw_align_112/Cheryl_Tiegs/\n",
            "LFW/lfw_align_112/Cheryl_Tiegs/Cheryl_Tiegs_0001.jpg\n",
            "LFW/lfw_align_112/Kathryn_Morris/\n",
            "LFW/lfw_align_112/Kathryn_Morris/Kathryn_Morris_0001.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Smart/\n",
            "LFW/lfw_align_112/Elizabeth_Smart/Elizabeth_Smart_0004.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Smart/Elizabeth_Smart_0003.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Smart/Elizabeth_Smart_0005.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Smart/Elizabeth_Smart_0002.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Smart/Elizabeth_Smart_0001.jpg\n",
            "LFW/lfw_align_112/Judy_Spreckels/\n",
            "LFW/lfw_align_112/Judy_Spreckels/Judy_Spreckels_0001.jpg\n",
            "LFW/lfw_align_112/Pascal_Lamy/\n",
            "LFW/lfw_align_112/Pascal_Lamy/Pascal_Lamy_0002.jpg\n",
            "LFW/lfw_align_112/Pascal_Lamy/Pascal_Lamy_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Keaton/\n",
            "LFW/lfw_align_112/Michael_Keaton/Michael_Keaton_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Keaton/Michael_Keaton_0001.jpg\n",
            "LFW/lfw_align_112/David_Brent/\n",
            "LFW/lfw_align_112/David_Brent/David_Brent_0001.jpg\n",
            "LFW/lfw_align_112/Leandrinho_Barbosa/\n",
            "LFW/lfw_align_112/Leandrinho_Barbosa/Leandrinho_Barbosa_0001.jpg\n",
            "LFW/lfw_align_112/Natalie_Cole/\n",
            "LFW/lfw_align_112/Natalie_Cole/Natalie_Cole_0002.jpg\n",
            "LFW/lfw_align_112/Natalie_Cole/Natalie_Cole_0001.jpg\n",
            "LFW/lfw_align_112/Natalie_Cole/Natalie_Cole_0003.jpg\n",
            "LFW/lfw_align_112/Dennis_Franchione/\n",
            "LFW/lfw_align_112/Dennis_Franchione/Dennis_Franchione_0001.jpg\n",
            "LFW/lfw_align_112/David_Trimble/\n",
            "LFW/lfw_align_112/David_Trimble/David_Trimble_0004.jpg\n",
            "LFW/lfw_align_112/David_Trimble/David_Trimble_0002.jpg\n",
            "LFW/lfw_align_112/David_Trimble/David_Trimble_0001.jpg\n",
            "LFW/lfw_align_112/David_Trimble/David_Trimble_0005.jpg\n",
            "LFW/lfw_align_112/David_Trimble/David_Trimble_0003.jpg\n",
            "LFW/lfw_align_112/Steven_Tyler/\n",
            "LFW/lfw_align_112/Steven_Tyler/Steven_Tyler_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Tannok/\n",
            "LFW/lfw_align_112/Charles_Tannok/Charles_Tannok_0001.jpg\n",
            "LFW/lfw_align_112/John_Garamendi/\n",
            "LFW/lfw_align_112/John_Garamendi/John_Garamendi_0001.jpg\n",
            "LFW/lfw_align_112/John_Garamendi/John_Garamendi_0002.jpg\n",
            "LFW/lfw_align_112/Joe_Carnahan/\n",
            "LFW/lfw_align_112/Joe_Carnahan/Joe_Carnahan_0001.jpg\n",
            "LFW/lfw_align_112/John_Banko/\n",
            "LFW/lfw_align_112/John_Banko/John_Banko_0002.jpg\n",
            "LFW/lfw_align_112/John_Banko/John_Banko_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Bandier/\n",
            "LFW/lfw_align_112/Martin_Bandier/Martin_Bandier_0001.jpg\n",
            "LFW/lfw_align_112/Cyndi_Thompson/\n",
            "LFW/lfw_align_112/Cyndi_Thompson/Cyndi_Thompson_0002.jpg\n",
            "LFW/lfw_align_112/Cyndi_Thompson/Cyndi_Thompson_0001.jpg\n",
            "LFW/lfw_align_112/Mohamed_Seineldin/\n",
            "LFW/lfw_align_112/Mohamed_Seineldin/Mohamed_Seineldin_0001.jpg\n",
            "LFW/lfw_align_112/Lino_Oviedo/\n",
            "LFW/lfw_align_112/Lino_Oviedo/Lino_Oviedo_0002.jpg\n",
            "LFW/lfw_align_112/Lino_Oviedo/Lino_Oviedo_0001.jpg\n",
            "LFW/lfw_align_112/Lino_Oviedo/Lino_Oviedo_0003.jpg\n",
            "LFW/lfw_align_112/Imam_Samudra/\n",
            "LFW/lfw_align_112/Imam_Samudra/Imam_Samudra_0001.jpg\n",
            "LFW/lfw_align_112/Kosuke_Kitajima/\n",
            "LFW/lfw_align_112/Kosuke_Kitajima/Kosuke_Kitajima_0002.jpg\n",
            "LFW/lfw_align_112/Kosuke_Kitajima/Kosuke_Kitajima_0001.jpg\n",
            "LFW/lfw_align_112/John_Travolta/\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0003.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0002.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0004.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0007.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0006.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0001.jpg\n",
            "LFW/lfw_align_112/John_Travolta/John_Travolta_0005.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0010.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0014.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0013.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0007.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0004.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0003.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0005.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0011.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0006.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0009.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0012.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0008.jpg\n",
            "LFW/lfw_align_112/Kim_Clijsters/Kim_Clijsters_0002.jpg\n",
            "LFW/lfw_align_112/Alejandro_Atchugarry/\n",
            "LFW/lfw_align_112/Alejandro_Atchugarry/Alejandro_Atchugarry_0002.jpg\n",
            "LFW/lfw_align_112/Alejandro_Atchugarry/Alejandro_Atchugarry_0001.jpg\n",
            "LFW/lfw_align_112/Barbara_Felt-Miller/\n",
            "LFW/lfw_align_112/Barbara_Felt-Miller/Barbara_Felt-Miller_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Dimon/\n",
            "LFW/lfw_align_112/Jamie_Dimon/Jamie_Dimon_0001.jpg\n",
            "LFW/lfw_align_112/Donald_Trump/\n",
            "LFW/lfw_align_112/Donald_Trump/Donald_Trump_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Yuan-tseh/\n",
            "LFW/lfw_align_112/Lee_Yuan-tseh/Lee_Yuan-tseh_0001.jpg\n",
            "LFW/lfw_align_112/Mohammed_Abu_Sharia/\n",
            "LFW/lfw_align_112/Mohammed_Abu_Sharia/Mohammed_Abu_Sharia_0001.jpg\n",
            "LFW/lfw_align_112/Hisao_Oguchi/\n",
            "LFW/lfw_align_112/Hisao_Oguchi/Hisao_Oguchi_0002.jpg\n",
            "LFW/lfw_align_112/Hisao_Oguchi/Hisao_Oguchi_0001.jpg\n",
            "LFW/lfw_align_112/Ignatius_Wang/\n",
            "LFW/lfw_align_112/Ignatius_Wang/Ignatius_Wang_0001.jpg\n",
            "LFW/lfw_align_112/Neri_Marcore/\n",
            "LFW/lfw_align_112/Neri_Marcore/Neri_Marcore_0001.jpg\n",
            "LFW/lfw_align_112/Neri_Marcore/Neri_Marcore_0002.jpg\n",
            "LFW/lfw_align_112/James_Williams/\n",
            "LFW/lfw_align_112/James_Williams/James_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Brandon_Boyd/\n",
            "LFW/lfw_align_112/Brandon_Boyd/Brandon_Boyd_0001.jpg\n",
            "LFW/lfw_align_112/Brad_Garrett/\n",
            "LFW/lfw_align_112/Brad_Garrett/Brad_Garrett_0003.jpg\n",
            "LFW/lfw_align_112/Brad_Garrett/Brad_Garrett_0002.jpg\n",
            "LFW/lfw_align_112/Brad_Garrett/Brad_Garrett_0004.jpg\n",
            "LFW/lfw_align_112/Brad_Garrett/Brad_Garrett_0001.jpg\n",
            "LFW/lfw_align_112/Bart_Freundlich/\n",
            "LFW/lfw_align_112/Bart_Freundlich/Bart_Freundlich_0001.jpg\n",
            "LFW/lfw_align_112/Jada_Pinkett_Smith/\n",
            "LFW/lfw_align_112/Jada_Pinkett_Smith/Jada_Pinkett_Smith_0002.jpg\n",
            "LFW/lfw_align_112/Jada_Pinkett_Smith/Jada_Pinkett_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Rodrigo_de_la_Cerna/\n",
            "LFW/lfw_align_112/Rodrigo_de_la_Cerna/Rodrigo_de_la_Cerna_0001.jpg\n",
            "LFW/lfw_align_112/Angelo_Genova/\n",
            "LFW/lfw_align_112/Angelo_Genova/Angelo_Genova_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Ryan/\n",
            "LFW/lfw_align_112/Jim_Ryan/Jim_Ryan_0001.jpg\n",
            "LFW/lfw_align_112/Lynn_Redgrave/\n",
            "LFW/lfw_align_112/Lynn_Redgrave/Lynn_Redgrave_0002.jpg\n",
            "LFW/lfw_align_112/Lynn_Redgrave/Lynn_Redgrave_0001.jpg\n",
            "LFW/lfw_align_112/Lynn_Redgrave/Lynn_Redgrave_0003.jpg\n",
            "LFW/lfw_align_112/Carlton_Baugh/\n",
            "LFW/lfw_align_112/Carlton_Baugh/Carlton_Baugh_0001.jpg\n",
            "LFW/lfw_align_112/Stipe_Mesic/\n",
            "LFW/lfw_align_112/Stipe_Mesic/Stipe_Mesic_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Fine/\n",
            "LFW/lfw_align_112/Richard_Fine/Richard_Fine_0001.jpg\n",
            "LFW/lfw_align_112/Valentina_Cervi/\n",
            "LFW/lfw_align_112/Valentina_Cervi/Valentina_Cervi_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Lobinger/\n",
            "LFW/lfw_align_112/Tim_Lobinger/Tim_Lobinger_0001.jpg\n",
            "LFW/lfw_align_112/Henrique_Meirelles/\n",
            "LFW/lfw_align_112/Henrique_Meirelles/Henrique_Meirelles_0002.jpg\n",
            "LFW/lfw_align_112/Henrique_Meirelles/Henrique_Meirelles_0001.jpg\n",
            "LFW/lfw_align_112/Georgina_Bardach/\n",
            "LFW/lfw_align_112/Georgina_Bardach/Georgina_Bardach_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Wycheck/\n",
            "LFW/lfw_align_112/Frank_Wycheck/Frank_Wycheck_0001.jpg\n",
            "LFW/lfw_align_112/Teri_ORourke/\n",
            "LFW/lfw_align_112/Teri_ORourke/Teri_ORourke_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Parsons/\n",
            "LFW/lfw_align_112/Richard_Parsons/Richard_Parsons_0001.jpg\n",
            "LFW/lfw_align_112/Carson_Daly/\n",
            "LFW/lfw_align_112/Carson_Daly/Carson_Daly_0002.jpg\n",
            "LFW/lfw_align_112/Carson_Daly/Carson_Daly_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Penn/\n",
            "LFW/lfw_align_112/Chris_Penn/Chris_Penn_0001.jpg\n",
            "LFW/lfw_align_112/Wesley_Clark/\n",
            "LFW/lfw_align_112/Wesley_Clark/Wesley_Clark_0002.jpg\n",
            "LFW/lfw_align_112/Wesley_Clark/Wesley_Clark_0001.jpg\n",
            "LFW/lfw_align_112/Natasha_Lyonne/\n",
            "LFW/lfw_align_112/Natasha_Lyonne/Natasha_Lyonne_0001.jpg\n",
            "LFW/lfw_align_112/Geoffrey_Davis/\n",
            "LFW/lfw_align_112/Geoffrey_Davis/Geoffrey_Davis_0001.jpg\n",
            "LFW/lfw_align_112/Horace_Newcomb/\n",
            "LFW/lfw_align_112/Horace_Newcomb/Horace_Newcomb_0001.jpg\n",
            "LFW/lfw_align_112/Hayden_Panettiere/\n",
            "LFW/lfw_align_112/Hayden_Panettiere/Hayden_Panettiere_0001.jpg\n",
            "LFW/lfw_align_112/Rick_Carlisle/\n",
            "LFW/lfw_align_112/Rick_Carlisle/Rick_Carlisle_0001.jpg\n",
            "LFW/lfw_align_112/Rick_Carlisle/Rick_Carlisle_0003.jpg\n",
            "LFW/lfw_align_112/Rick_Carlisle/Rick_Carlisle_0004.jpg\n",
            "LFW/lfw_align_112/Rick_Carlisle/Rick_Carlisle_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0012.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0016.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0018.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0017.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0009.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0008.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0003.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0006.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0015.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0004.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0013.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0010.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0011.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0005.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0007.jpg\n",
            "LFW/lfw_align_112/Richard_Myers/Richard_Myers_0014.jpg\n",
            "LFW/lfw_align_112/Jane_Menelaus/\n",
            "LFW/lfw_align_112/Jane_Menelaus/Jane_Menelaus_0001.jpg\n",
            "LFW/lfw_align_112/Harland_Braun/\n",
            "LFW/lfw_align_112/Harland_Braun/Harland_Braun_0001.jpg\n",
            "LFW/lfw_align_112/Boris_Berezovsky/\n",
            "LFW/lfw_align_112/Boris_Berezovsky/Boris_Berezovsky_0002.jpg\n",
            "LFW/lfw_align_112/Boris_Berezovsky/Boris_Berezovsky_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Friedman/\n",
            "LFW/lfw_align_112/Stephen_Friedman/Stephen_Friedman_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Friedman/Stephen_Friedman_0002.jpg\n",
            "LFW/lfw_align_112/Nelson_Mandela/\n",
            "LFW/lfw_align_112/Nelson_Mandela/Nelson_Mandela_0004.jpg\n",
            "LFW/lfw_align_112/Nelson_Mandela/Nelson_Mandela_0002.jpg\n",
            "LFW/lfw_align_112/Nelson_Mandela/Nelson_Mandela_0001.jpg\n",
            "LFW/lfw_align_112/Nelson_Mandela/Nelson_Mandela_0003.jpg\n",
            "LFW/lfw_align_112/Eric_Snow/\n",
            "LFW/lfw_align_112/Eric_Snow/Eric_Snow_0001.jpg\n",
            "LFW/lfw_align_112/Hussam_Mohammed_Amin/\n",
            "LFW/lfw_align_112/Hussam_Mohammed_Amin/Hussam_Mohammed_Amin_0001.jpg\n",
            "LFW/lfw_align_112/Eric_Rosser/\n",
            "LFW/lfw_align_112/Eric_Rosser/Eric_Rosser_0002.jpg\n",
            "LFW/lfw_align_112/Eric_Rosser/Eric_Rosser_0001.jpg\n",
            "LFW/lfw_align_112/Barrett_Jackman/\n",
            "LFW/lfw_align_112/Barrett_Jackman/Barrett_Jackman_0001.jpg\n",
            "LFW/lfw_align_112/Barrett_Jackman/Barrett_Jackman_0002.jpg\n",
            "LFW/lfw_align_112/Anna_Nicole_Smith/\n",
            "LFW/lfw_align_112/Anna_Nicole_Smith/Anna_Nicole_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Anna_Nicole_Smith/Anna_Nicole_Smith_0002.jpg\n",
            "LFW/lfw_align_112/Paul_Johnson/\n",
            "LFW/lfw_align_112/Paul_Johnson/Paul_Johnson_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Hernandez/\n",
            "LFW/lfw_align_112/Chris_Hernandez/Chris_Hernandez_0001.jpg\n",
            "LFW/lfw_align_112/Ryan_Nyquist/\n",
            "LFW/lfw_align_112/Ryan_Nyquist/Ryan_Nyquist_0001.jpg\n",
            "LFW/lfw_align_112/Regina_Ip/\n",
            "LFW/lfw_align_112/Regina_Ip/Regina_Ip_0001.jpg\n",
            "LFW/lfw_align_112/Shannyn_Sossamon/\n",
            "LFW/lfw_align_112/Shannyn_Sossamon/Shannyn_Sossamon_0001.jpg\n",
            "LFW/lfw_align_112/Benjamin_Martinez/\n",
            "LFW/lfw_align_112/Benjamin_Martinez/Benjamin_Martinez_0001.jpg\n",
            "LFW/lfw_align_112/Joey_Buttafuoco/\n",
            "LFW/lfw_align_112/Joey_Buttafuoco/Joey_Buttafuoco_0001.jpg\n",
            "LFW/lfw_align_112/Kirby_Puckett/\n",
            "LFW/lfw_align_112/Kirby_Puckett/Kirby_Puckett_0001.jpg\n",
            "LFW/lfw_align_112/Ellen_DeGeneres/\n",
            "LFW/lfw_align_112/Ellen_DeGeneres/Ellen_DeGeneres_0001.jpg\n",
            "LFW/lfw_align_112/Ellen_DeGeneres/Ellen_DeGeneres_0002.jpg\n",
            "LFW/lfw_align_112/Lord_Hutton/\n",
            "LFW/lfw_align_112/Lord_Hutton/Lord_Hutton_0001.jpg\n",
            "LFW/lfw_align_112/Lord_Hutton/Lord_Hutton_0002.jpg\n",
            "LFW/lfw_align_112/Ernie_Els/\n",
            "LFW/lfw_align_112/Ernie_Els/Ernie_Els_0001.jpg\n",
            "LFW/lfw_align_112/Ernie_Els/Ernie_Els_0004.jpg\n",
            "LFW/lfw_align_112/Ernie_Els/Ernie_Els_0003.jpg\n",
            "LFW/lfw_align_112/Ernie_Els/Ernie_Els_0002.jpg\n",
            "LFW/lfw_align_112/Christopher_Russell/\n",
            "LFW/lfw_align_112/Christopher_Russell/Christopher_Russell_0001.jpg\n",
            "LFW/lfw_align_112/Florecita_Cobian/\n",
            "LFW/lfw_align_112/Florecita_Cobian/Florecita_Cobian_0001.jpg\n",
            "LFW/lfw_align_112/Bruna_Colosio/\n",
            "LFW/lfw_align_112/Bruna_Colosio/Bruna_Colosio_0001.jpg\n",
            "LFW/lfw_align_112/David_Beckham/\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0017.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0024.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0008.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0002.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0011.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0028.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0006.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0030.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0025.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0007.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0022.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0023.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0009.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0029.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0026.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0005.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0012.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0016.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0004.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0031.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0001.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0010.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0027.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0019.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0003.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0014.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0021.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0015.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0013.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0020.jpg\n",
            "LFW/lfw_align_112/David_Beckham/David_Beckham_0018.jpg\n",
            "LFW/lfw_align_112/Rene_Portland/\n",
            "LFW/lfw_align_112/Rene_Portland/Rene_Portland_0001.jpg\n",
            "LFW/lfw_align_112/Lindsay_Lohan/\n",
            "LFW/lfw_align_112/Lindsay_Lohan/Lindsay_Lohan_0001.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0002.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0007.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0006.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0008.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0003.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0004.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0001.jpg\n",
            "LFW/lfw_align_112/Sheryl_Crow/Sheryl_Crow_0005.jpg\n",
            "LFW/lfw_align_112/Camryn_Manheim/\n",
            "LFW/lfw_align_112/Camryn_Manheim/Camryn_Manheim_0001.jpg\n",
            "LFW/lfw_align_112/Robbie_Fowler/\n",
            "LFW/lfw_align_112/Robbie_Fowler/Robbie_Fowler_0002.jpg\n",
            "LFW/lfw_align_112/Robbie_Fowler/Robbie_Fowler_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Pagliuca/\n",
            "LFW/lfw_align_112/Steve_Pagliuca/Steve_Pagliuca_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Taylor/\n",
            "LFW/lfw_align_112/Michael_Taylor/Michael_Taylor_0001.jpg\n",
            "LFW/lfw_align_112/Newton_Carlton_Slawson/\n",
            "LFW/lfw_align_112/Newton_Carlton_Slawson/Newton_Carlton_Slawson_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Baca/\n",
            "LFW/lfw_align_112/Lee_Baca/Lee_Baca_0001.jpg\n",
            "LFW/lfw_align_112/Martha_Stewart/\n",
            "LFW/lfw_align_112/Martha_Stewart/Martha_Stewart_0003.jpg\n",
            "LFW/lfw_align_112/Martha_Stewart/Martha_Stewart_0002.jpg\n",
            "LFW/lfw_align_112/Martha_Stewart/Martha_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Martha_Stewart/Martha_Stewart_0005.jpg\n",
            "LFW/lfw_align_112/Martha_Stewart/Martha_Stewart_0004.jpg\n",
            "LFW/lfw_align_112/Lyudmila_Putin/\n",
            "LFW/lfw_align_112/Lyudmila_Putin/Lyudmila_Putin_0001.jpg\n",
            "LFW/lfw_align_112/Christy_Ferer/\n",
            "LFW/lfw_align_112/Christy_Ferer/Christy_Ferer_0001.jpg\n",
            "LFW/lfw_align_112/Milton_Berle/\n",
            "LFW/lfw_align_112/Milton_Berle/Milton_Berle_0001.jpg\n",
            "LFW/lfw_align_112/Khatol_Mohammad_Zai/\n",
            "LFW/lfw_align_112/Khatol_Mohammad_Zai/Khatol_Mohammad_Zai_0001.jpg\n",
            "LFW/lfw_align_112/Glen_Clark/\n",
            "LFW/lfw_align_112/Glen_Clark/Glen_Clark_0001.jpg\n",
            "LFW/lfw_align_112/Gerald_Barbarito/\n",
            "LFW/lfw_align_112/Gerald_Barbarito/Gerald_Barbarito_0001.jpg\n",
            "LFW/lfw_align_112/Ruth_Stubbs/\n",
            "LFW/lfw_align_112/Ruth_Stubbs/Ruth_Stubbs_0001.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0006.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0002.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0004.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0005.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0003.jpg\n",
            "LFW/lfw_align_112/Costas_Simitis/Costas_Simitis_0001.jpg\n",
            "LFW/lfw_align_112/Jan_van_Breda_Kolff/\n",
            "LFW/lfw_align_112/Jan_van_Breda_Kolff/Jan_van_Breda_Kolff_0001.jpg\n",
            "LFW/lfw_align_112/Jason_Petty/\n",
            "LFW/lfw_align_112/Jason_Petty/Jason_Petty_0001.jpg\n",
            "LFW/lfw_align_112/Wang_Yingfan/\n",
            "LFW/lfw_align_112/Wang_Yingfan/Wang_Yingfan_0001.jpg\n",
            "LFW/lfw_align_112/Wang_Yingfan/Wang_Yingfan_0003.jpg\n",
            "LFW/lfw_align_112/Wang_Yingfan/Wang_Yingfan_0002.jpg\n",
            "LFW/lfw_align_112/Katherine_Harris/\n",
            "LFW/lfw_align_112/Katherine_Harris/Katherine_Harris_0004.jpg\n",
            "LFW/lfw_align_112/Katherine_Harris/Katherine_Harris_0003.jpg\n",
            "LFW/lfw_align_112/Katherine_Harris/Katherine_Harris_0001.jpg\n",
            "LFW/lfw_align_112/Katherine_Harris/Katherine_Harris_0002.jpg\n",
            "LFW/lfw_align_112/Alastair_Johnston/\n",
            "LFW/lfw_align_112/Alastair_Johnston/Alastair_Johnston_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Haslett/\n",
            "LFW/lfw_align_112/Jim_Haslett/Jim_Haslett_0001.jpg\n",
            "LFW/lfw_align_112/Jose_Sarney/\n",
            "LFW/lfw_align_112/Jose_Sarney/Jose_Sarney_0001.jpg\n",
            "LFW/lfw_align_112/Jose_Sarney/Jose_Sarney_0003.jpg\n",
            "LFW/lfw_align_112/Jose_Sarney/Jose_Sarney_0002.jpg\n",
            "LFW/lfw_align_112/Ted_Maher/\n",
            "LFW/lfw_align_112/Ted_Maher/Ted_Maher_0001.jpg\n",
            "LFW/lfw_align_112/Ted_Maher/Ted_Maher_0002.jpg\n",
            "LFW/lfw_align_112/Mitchell_Swartz/\n",
            "LFW/lfw_align_112/Mitchell_Swartz/Mitchell_Swartz_0001.jpg\n",
            "LFW/lfw_align_112/Thomas_Kelly/\n",
            "LFW/lfw_align_112/Thomas_Kelly/Thomas_Kelly_0001.jpg\n",
            "LFW/lfw_align_112/Rodney_Dangerfield/\n",
            "LFW/lfw_align_112/Rodney_Dangerfield/Rodney_Dangerfield_0001.jpg\n",
            "LFW/lfw_align_112/Bryan_Thomas/\n",
            "LFW/lfw_align_112/Bryan_Thomas/Bryan_Thomas_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Lazarus/\n",
            "LFW/lfw_align_112/Mark_Lazarus/Mark_Lazarus_0001.jpg\n",
            "LFW/lfw_align_112/Adam_Herbert/\n",
            "LFW/lfw_align_112/Adam_Herbert/Adam_Herbert_0001.jpg\n",
            "LFW/lfw_align_112/George_P_Bush/\n",
            "LFW/lfw_align_112/George_P_Bush/George_P_Bush_0002.jpg\n",
            "LFW/lfw_align_112/George_P_Bush/George_P_Bush_0001.jpg\n",
            "LFW/lfw_align_112/Margaret_Thatcher/\n",
            "LFW/lfw_align_112/Margaret_Thatcher/Margaret_Thatcher_0002.jpg\n",
            "LFW/lfw_align_112/Margaret_Thatcher/Margaret_Thatcher_0001.jpg\n",
            "LFW/lfw_align_112/Corinne_Coman/\n",
            "LFW/lfw_align_112/Corinne_Coman/Corinne_Coman_0002.jpg\n",
            "LFW/lfw_align_112/Corinne_Coman/Corinne_Coman_0001.jpg\n",
            "LFW/lfw_align_112/Kirsten_Gilham/\n",
            "LFW/lfw_align_112/Kirsten_Gilham/Kirsten_Gilham_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Marc_Olive/\n",
            "LFW/lfw_align_112/Jean-Marc_Olive/Jean-Marc_Olive_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Arnett/\n",
            "LFW/lfw_align_112/Peter_Arnett/Peter_Arnett_0003.jpg\n",
            "LFW/lfw_align_112/Peter_Arnett/Peter_Arnett_0002.jpg\n",
            "LFW/lfw_align_112/Peter_Arnett/Peter_Arnett_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Guerin/\n",
            "LFW/lfw_align_112/Bill_Guerin/Bill_Guerin_0001.jpg\n",
            "LFW/lfw_align_112/Omar_el-Heib/\n",
            "LFW/lfw_align_112/Omar_el-Heib/Omar_el-Heib_0001.jpg\n",
            "LFW/lfw_align_112/Nanni_Moretti/\n",
            "LFW/lfw_align_112/Nanni_Moretti/Nanni_Moretti_0001.jpg\n",
            "LFW/lfw_align_112/Nanni_Moretti/Nanni_Moretti_0002.jpg\n",
            "LFW/lfw_align_112/Warren_Beatty/\n",
            "LFW/lfw_align_112/Warren_Beatty/Warren_Beatty_0001.jpg\n",
            "LFW/lfw_align_112/Warren_Beatty/Warren_Beatty_0002.jpg\n",
            "LFW/lfw_align_112/Lucia_Kenny_Anthony/\n",
            "LFW/lfw_align_112/Lucia_Kenny_Anthony/Lucia_Kenny_Anthony_0001.jpg\n",
            "LFW/lfw_align_112/Sally_Field/\n",
            "LFW/lfw_align_112/Sally_Field/Sally_Field_0001.jpg\n",
            "LFW/lfw_align_112/Sally_Field/Sally_Field_0002.jpg\n",
            "LFW/lfw_align_112/Sally_Field/Sally_Field_0003.jpg\n",
            "LFW/lfw_align_112/Sally_Field/Sally_Field_0004.jpg\n",
            "LFW/lfw_align_112/Joy_Bryant/\n",
            "LFW/lfw_align_112/Joy_Bryant/Joy_Bryant_0001.jpg\n",
            "LFW/lfw_align_112/Nathalie_Dechy/\n",
            "LFW/lfw_align_112/Nathalie_Dechy/Nathalie_Dechy_0001.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0003.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0002.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0004.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0005.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0001.jpg\n",
            "LFW/lfw_align_112/Bulent_Ecevit/Bulent_Ecevit_0006.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0031.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0128.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0093.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0050.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0002.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0035.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0033.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0132.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0011.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0001.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0133.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0067.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0019.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0070.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0085.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0027.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0098.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0136.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0107.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0066.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0053.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0083.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0140.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0139.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0026.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0072.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0124.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0090.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0078.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0005.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0061.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0024.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0063.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0108.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0088.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0020.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0052.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0010.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0051.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0094.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0114.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0091.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0023.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0087.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0075.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0064.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0012.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0102.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0006.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0046.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0073.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0003.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0135.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0074.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0044.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0122.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0059.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0100.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0106.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0037.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0043.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0121.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0089.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0071.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0119.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0040.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0065.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0060.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0131.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0117.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0018.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0076.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0047.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0142.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0109.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0015.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0009.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0056.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0045.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0032.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0016.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0141.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0086.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0028.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0101.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0013.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0130.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0049.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0110.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0129.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0123.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0081.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0034.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0120.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0103.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0054.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0041.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0055.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0007.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0029.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0115.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0079.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0036.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0025.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0014.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0116.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0062.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0077.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0105.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0143.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0125.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0126.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0048.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0021.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0118.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0134.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0144.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0069.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0042.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0038.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0039.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0068.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0017.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0022.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0058.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0097.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0112.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0004.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0057.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0099.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0138.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0095.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0082.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0127.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0080.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0096.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0137.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0111.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0030.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0104.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0113.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0092.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0008.jpg\n",
            "LFW/lfw_align_112/Tony_Blair/Tony_Blair_0084.jpg\n",
            "LFW/lfw_align_112/Scott_Rudin/\n",
            "LFW/lfw_align_112/Scott_Rudin/Scott_Rudin_0002.jpg\n",
            "LFW/lfw_align_112/Scott_Rudin/Scott_Rudin_0001.jpg\n",
            "LFW/lfw_align_112/Uri_Lopolianski/\n",
            "LFW/lfw_align_112/Uri_Lopolianski/Uri_Lopolianski_0001.jpg\n",
            "LFW/lfw_align_112/Emily_Mortimer/\n",
            "LFW/lfw_align_112/Emily_Mortimer/Emily_Mortimer_0001.jpg\n",
            "LFW/lfw_align_112/Ron_Gonzales/\n",
            "LFW/lfw_align_112/Ron_Gonzales/Ron_Gonzales_0001.jpg\n",
            "LFW/lfw_align_112/Nora_Bendijo/\n",
            "LFW/lfw_align_112/Nora_Bendijo/Nora_Bendijo_0001.jpg\n",
            "LFW/lfw_align_112/Nora_Bendijo/Nora_Bendijo_0002.jpg\n",
            "LFW/lfw_align_112/Julius_Barnes/\n",
            "LFW/lfw_align_112/Julius_Barnes/Julius_Barnes_0001.jpg\n",
            "LFW/lfw_align_112/Nobuyuki_Idei/\n",
            "LFW/lfw_align_112/Nobuyuki_Idei/Nobuyuki_Idei_0001.jpg\n",
            "LFW/lfw_align_112/Zafarullah_Khan_Jamali/\n",
            "LFW/lfw_align_112/Zafarullah_Khan_Jamali/Zafarullah_Khan_Jamali_0001.jpg\n",
            "LFW/lfw_align_112/Zafarullah_Khan_Jamali/Zafarullah_Khan_Jamali_0002.jpg\n",
            "LFW/lfw_align_112/Alexander_Rumyantsev/\n",
            "LFW/lfw_align_112/Alexander_Rumyantsev/Alexander_Rumyantsev_0001.jpg\n",
            "LFW/lfw_align_112/Alexander_Rumyantsev/Alexander_Rumyantsev_0002.jpg\n",
            "LFW/lfw_align_112/Clive_Lloyd/\n",
            "LFW/lfw_align_112/Clive_Lloyd/Clive_Lloyd_0001.jpg\n",
            "LFW/lfw_align_112/Mia_Mottley/\n",
            "LFW/lfw_align_112/Mia_Mottley/Mia_Mottley_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Farrar/\n",
            "LFW/lfw_align_112/Mike_Farrar/Mike_Farrar_0001.jpg\n",
            "LFW/lfw_align_112/Shane_Phillips/\n",
            "LFW/lfw_align_112/Shane_Phillips/Shane_Phillips_0001.jpg\n",
            "LFW/lfw_align_112/Jack_Valenti/\n",
            "LFW/lfw_align_112/Jack_Valenti/Jack_Valenti_0001.jpg\n",
            "LFW/lfw_align_112/Cindy_Moll/\n",
            "LFW/lfw_align_112/Cindy_Moll/Cindy_Moll_0001.jpg\n",
            "LFW/lfw_align_112/Clifford_Robinson/\n",
            "LFW/lfw_align_112/Clifford_Robinson/Clifford_Robinson_0001.jpg\n",
            "LFW/lfw_align_112/Nova_Esther_Guthrie/\n",
            "LFW/lfw_align_112/Nova_Esther_Guthrie/Nova_Esther_Guthrie_0001.jpg\n",
            "LFW/lfw_align_112/Barry_Williams/\n",
            "LFW/lfw_align_112/Barry_Williams/Barry_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Cassell/\n",
            "LFW/lfw_align_112/Frank_Cassell/Frank_Cassell_0003.jpg\n",
            "LFW/lfw_align_112/Frank_Cassell/Frank_Cassell_0002.jpg\n",
            "LFW/lfw_align_112/Frank_Cassell/Frank_Cassell_0001.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0001.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0002.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0005.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0004.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0006.jpg\n",
            "LFW/lfw_align_112/Dennis_Hastert/Dennis_Hastert_0003.jpg\n",
            "LFW/lfw_align_112/John_Paul_DeJoria/\n",
            "LFW/lfw_align_112/John_Paul_DeJoria/John_Paul_DeJoria_0001.jpg\n",
            "LFW/lfw_align_112/Pedro_Martinez/\n",
            "LFW/lfw_align_112/Pedro_Martinez/Pedro_Martinez_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Weeks/\n",
            "LFW/lfw_align_112/Kim_Weeks/Kim_Weeks_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Hillary/\n",
            "LFW/lfw_align_112/Peter_Hillary/Peter_Hillary_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Hillary/Peter_Hillary_0002.jpg\n",
            "LFW/lfw_align_112/Ronald_Post/\n",
            "LFW/lfw_align_112/Ronald_Post/Ronald_Post_0001.jpg\n",
            "LFW/lfw_align_112/Max_Baucus/\n",
            "LFW/lfw_align_112/Max_Baucus/Max_Baucus_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Bettany/\n",
            "LFW/lfw_align_112/Paul_Bettany/Paul_Bettany_0001.jpg\n",
            "LFW/lfw_align_112/Greg_Rusedski/\n",
            "LFW/lfw_align_112/Greg_Rusedski/Greg_Rusedski_0003.jpg\n",
            "LFW/lfw_align_112/Greg_Rusedski/Greg_Rusedski_0004.jpg\n",
            "LFW/lfw_align_112/Greg_Rusedski/Greg_Rusedski_0001.jpg\n",
            "LFW/lfw_align_112/Greg_Rusedski/Greg_Rusedski_0002.jpg\n",
            "LFW/lfw_align_112/Joao_Rocha/\n",
            "LFW/lfw_align_112/Joao_Rocha/Joao_Rocha_0001.jpg\n",
            "LFW/lfw_align_112/Gilberto_Rodriguez_Orejuela/\n",
            "LFW/lfw_align_112/Gilberto_Rodriguez_Orejuela/Gilberto_Rodriguez_Orejuela_0004.jpg\n",
            "LFW/lfw_align_112/Gilberto_Rodriguez_Orejuela/Gilberto_Rodriguez_Orejuela_0003.jpg\n",
            "LFW/lfw_align_112/Gilberto_Rodriguez_Orejuela/Gilberto_Rodriguez_Orejuela_0001.jpg\n",
            "LFW/lfw_align_112/Gilberto_Rodriguez_Orejuela/Gilberto_Rodriguez_Orejuela_0002.jpg\n",
            "LFW/lfw_align_112/Vassilis_Xiros/\n",
            "LFW/lfw_align_112/Vassilis_Xiros/Vassilis_Xiros_0001.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0016.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0012.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0005.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0003.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0022.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0010.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0021.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0011.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0018.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0013.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0014.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0007.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0006.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0020.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0004.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0008.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0019.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0009.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0015.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0017.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0001.jpg\n",
            "LFW/lfw_align_112/Lindsay_Davenport/Lindsay_Davenport_0002.jpg\n",
            "LFW/lfw_align_112/Peter_Shaw/\n",
            "LFW/lfw_align_112/Peter_Shaw/Peter_Shaw_0001.jpg\n",
            "LFW/lfw_align_112/Arnie_Boehm/\n",
            "LFW/lfw_align_112/Arnie_Boehm/Arnie_Boehm_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Elliott/\n",
            "LFW/lfw_align_112/Bill_Elliott/Bill_Elliott_0001.jpg\n",
            "LFW/lfw_align_112/Prince_Claus/\n",
            "LFW/lfw_align_112/Prince_Claus/Prince_Claus_0004.jpg\n",
            "LFW/lfw_align_112/Prince_Claus/Prince_Claus_0002.jpg\n",
            "LFW/lfw_align_112/Prince_Claus/Prince_Claus_0001.jpg\n",
            "LFW/lfw_align_112/Prince_Claus/Prince_Claus_0003.jpg\n",
            "LFW/lfw_align_112/Anthony_Garotinho/\n",
            "LFW/lfw_align_112/Anthony_Garotinho/Anthony_Garotinho_0001.jpg\n",
            "LFW/lfw_align_112/Marian_Dolan/\n",
            "LFW/lfw_align_112/Marian_Dolan/Marian_Dolan_0001.jpg\n",
            "LFW/lfw_align_112/John_Howard/\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0006.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0017.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0001.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0013.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0012.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0004.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0015.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0014.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0018.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0019.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0005.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0003.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0010.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0011.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0002.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0008.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0009.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0007.jpg\n",
            "LFW/lfw_align_112/John_Howard/John_Howard_0016.jpg\n",
            "LFW/lfw_align_112/Jose_Carlo_Fernandez/\n",
            "LFW/lfw_align_112/Jose_Carlo_Fernandez/Jose_Carlo_Fernandez_0001.jpg\n",
            "LFW/lfw_align_112/Marc_Leger/\n",
            "LFW/lfw_align_112/Marc_Leger/Marc_Leger_0001.jpg\n",
            "LFW/lfw_align_112/Lesia_Burlak/\n",
            "LFW/lfw_align_112/Lesia_Burlak/Lesia_Burlak_0001.jpg\n",
            "LFW/lfw_align_112/Ben_Braun/\n",
            "LFW/lfw_align_112/Ben_Braun/Ben_Braun_0001.jpg\n",
            "LFW/lfw_align_112/Nicoletta_Braschi/\n",
            "LFW/lfw_align_112/Nicoletta_Braschi/Nicoletta_Braschi_0001.jpg\n",
            "LFW/lfw_align_112/Nicolas_Kiefer/\n",
            "LFW/lfw_align_112/Nicolas_Kiefer/Nicolas_Kiefer_0001.jpg\n",
            "LFW/lfw_align_112/Lachlan_Murdoch/\n",
            "LFW/lfw_align_112/Lachlan_Murdoch/Lachlan_Murdoch_0001.jpg\n",
            "LFW/lfw_align_112/Lazaro_Castro/\n",
            "LFW/lfw_align_112/Lazaro_Castro/Lazaro_Castro_0001.jpg\n",
            "LFW/lfw_align_112/Gus_Frerotte/\n",
            "LFW/lfw_align_112/Gus_Frerotte/Gus_Frerotte_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Juncker/\n",
            "LFW/lfw_align_112/Jean-Claude_Juncker/Jean-Claude_Juncker_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Juncker/Jean-Claude_Juncker_0002.jpg\n",
            "LFW/lfw_align_112/John_Engler/\n",
            "LFW/lfw_align_112/John_Engler/John_Engler_0001.jpg\n",
            "LFW/lfw_align_112/Jesse_James_Leija/\n",
            "LFW/lfw_align_112/Jesse_James_Leija/Jesse_James_Leija_0001.jpg\n",
            "LFW/lfw_align_112/Jesse_James_Leija/Jesse_James_Leija_0002.jpg\n",
            "LFW/lfw_align_112/Ignacio_Antonio_Velasco/\n",
            "LFW/lfw_align_112/Ignacio_Antonio_Velasco/Ignacio_Antonio_Velasco_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Kelly/\n",
            "LFW/lfw_align_112/Tom_Kelly/Tom_Kelly_0001.jpg\n",
            "LFW/lfw_align_112/Kara_Lynn_Joyce/\n",
            "LFW/lfw_align_112/Kara_Lynn_Joyce/Kara_Lynn_Joyce_0001.jpg\n",
            "LFW/lfw_align_112/Rachel_Leigh_Cook/\n",
            "LFW/lfw_align_112/Rachel_Leigh_Cook/Rachel_Leigh_Cook_0001.jpg\n",
            "LFW/lfw_align_112/David_Eldon/\n",
            "LFW/lfw_align_112/David_Eldon/David_Eldon_0001.jpg\n",
            "LFW/lfw_align_112/Stacey_Yamaguchi/\n",
            "LFW/lfw_align_112/Stacey_Yamaguchi/Stacey_Yamaguchi_0001.jpg\n",
            "LFW/lfw_align_112/Se_Hyuk_Joo/\n",
            "LFW/lfw_align_112/Se_Hyuk_Joo/Se_Hyuk_Joo_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Stevens/\n",
            "LFW/lfw_align_112/Gary_Stevens/Gary_Stevens_0001.jpg\n",
            "LFW/lfw_align_112/Kwon_Young-gil/\n",
            "LFW/lfw_align_112/Kwon_Young-gil/Kwon_Young-gil_0001.jpg\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/Carrie-Anne_Moss_0002.jpg\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/Carrie-Anne_Moss_0005.jpg\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/Carrie-Anne_Moss_0004.jpg\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/Carrie-Anne_Moss_0003.jpg\n",
            "LFW/lfw_align_112/Carrie-Anne_Moss/Carrie-Anne_Moss_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Pawlenty/\n",
            "LFW/lfw_align_112/Tim_Pawlenty/Tim_Pawlenty_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Winterbottom/\n",
            "LFW/lfw_align_112/Michael_Winterbottom/Michael_Winterbottom_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Winterbottom/Michael_Winterbottom_0003.jpg\n",
            "LFW/lfw_align_112/Michael_Winterbottom/Michael_Winterbottom_0002.jpg\n",
            "LFW/lfw_align_112/Roberto_Guaterroma/\n",
            "LFW/lfw_align_112/Roberto_Guaterroma/Roberto_Guaterroma_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Eustachy/\n",
            "LFW/lfw_align_112/Larry_Eustachy/Larry_Eustachy_0001.jpg\n",
            "LFW/lfw_align_112/Maha_Habib/\n",
            "LFW/lfw_align_112/Maha_Habib/Maha_Habib_0001.jpg\n",
            "LFW/lfw_align_112/Steven_Craig/\n",
            "LFW/lfw_align_112/Steven_Craig/Steven_Craig_0001.jpg\n",
            "LFW/lfw_align_112/Ed_Case/\n",
            "LFW/lfw_align_112/Ed_Case/Ed_Case_0001.jpg\n",
            "LFW/lfw_align_112/Wang_Fei/\n",
            "LFW/lfw_align_112/Wang_Fei/Wang_Fei_0001.jpg\n",
            "LFW/lfw_align_112/Juan_Carlos_Morales/\n",
            "LFW/lfw_align_112/Juan_Carlos_Morales/Juan_Carlos_Morales_0001.jpg\n",
            "LFW/lfw_align_112/Allyson_Felix/\n",
            "LFW/lfw_align_112/Allyson_Felix/Allyson_Felix_0004.jpg\n",
            "LFW/lfw_align_112/Allyson_Felix/Allyson_Felix_0002.jpg\n",
            "LFW/lfw_align_112/Allyson_Felix/Allyson_Felix_0001.jpg\n",
            "LFW/lfw_align_112/Allyson_Felix/Allyson_Felix_0005.jpg\n",
            "LFW/lfw_align_112/Allyson_Felix/Allyson_Felix_0003.jpg\n",
            "LFW/lfw_align_112/Eva_Dimas/\n",
            "LFW/lfw_align_112/Eva_Dimas/Eva_Dimas_0002.jpg\n",
            "LFW/lfw_align_112/Eva_Dimas/Eva_Dimas_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Cutler/\n",
            "LFW/lfw_align_112/Steve_Cutler/Steve_Cutler_0001.jpg\n",
            "LFW/lfw_align_112/Luis_Figo/\n",
            "LFW/lfw_align_112/Luis_Figo/Luis_Figo_0003.jpg\n",
            "LFW/lfw_align_112/Luis_Figo/Luis_Figo_0001.jpg\n",
            "LFW/lfw_align_112/Luis_Figo/Luis_Figo_0004.jpg\n",
            "LFW/lfw_align_112/Luis_Figo/Luis_Figo_0002.jpg\n",
            "LFW/lfw_align_112/Ed_Sullivan/\n",
            "LFW/lfw_align_112/Ed_Sullivan/Ed_Sullivan_0001.jpg\n",
            "LFW/lfw_align_112/Kurt_Warner/\n",
            "LFW/lfw_align_112/Kurt_Warner/Kurt_Warner_0005.jpg\n",
            "LFW/lfw_align_112/Kurt_Warner/Kurt_Warner_0004.jpg\n",
            "LFW/lfw_align_112/Kurt_Warner/Kurt_Warner_0003.jpg\n",
            "LFW/lfw_align_112/Kurt_Warner/Kurt_Warner_0002.jpg\n",
            "LFW/lfw_align_112/Kurt_Warner/Kurt_Warner_0001.jpg\n",
            "LFW/lfw_align_112/Dan_Monson/\n",
            "LFW/lfw_align_112/Dan_Monson/Dan_Monson_0001.jpg\n",
            "LFW/lfw_align_112/Cecilia_Cheung/\n",
            "LFW/lfw_align_112/Cecilia_Cheung/Cecilia_Cheung_0001.jpg\n",
            "LFW/lfw_align_112/Jake_Brace/\n",
            "LFW/lfw_align_112/Jake_Brace/Jake_Brace_0001.jpg\n",
            "LFW/lfw_align_112/Alejandro_Avila/\n",
            "LFW/lfw_align_112/Alejandro_Avila/Alejandro_Avila_0001.jpg\n",
            "LFW/lfw_align_112/Alejandro_Avila/Alejandro_Avila_0003.jpg\n",
            "LFW/lfw_align_112/Alejandro_Avila/Alejandro_Avila_0002.jpg\n",
            "LFW/lfw_align_112/Brandon_Lloyd/\n",
            "LFW/lfw_align_112/Brandon_Lloyd/Brandon_Lloyd_0001.jpg\n",
            "LFW/lfw_align_112/Gerrit_Zalm/\n",
            "LFW/lfw_align_112/Gerrit_Zalm/Gerrit_Zalm_0001.jpg\n",
            "LFW/lfw_align_112/Sue_Guevara/\n",
            "LFW/lfw_align_112/Sue_Guevara/Sue_Guevara_0001.jpg\n",
            "LFW/lfw_align_112/Linda_Amicangioli/\n",
            "LFW/lfw_align_112/Linda_Amicangioli/Linda_Amicangioli_0001.jpg\n",
            "LFW/lfw_align_112/Alan_Tang_Kwong-wing/\n",
            "LFW/lfw_align_112/Alan_Tang_Kwong-wing/Alan_Tang_Kwong-wing_0001.jpg\n",
            "LFW/lfw_align_112/Adriana_Lima/\n",
            "LFW/lfw_align_112/Adriana_Lima/Adriana_Lima_0001.jpg\n",
            "LFW/lfw_align_112/Marsah_Ambrosius/\n",
            "LFW/lfw_align_112/Marsah_Ambrosius/Marsah_Ambrosius_0001.jpg\n",
            "LFW/lfw_align_112/Gordon_Cooper/\n",
            "LFW/lfw_align_112/Gordon_Cooper/Gordon_Cooper_0001.jpg\n",
            "LFW/lfw_align_112/Rob_Ramsay/\n",
            "LFW/lfw_align_112/Rob_Ramsay/Rob_Ramsay_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Norton-Taylor/\n",
            "LFW/lfw_align_112/Richard_Norton-Taylor/Richard_Norton-Taylor_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Norton-Taylor/Richard_Norton-Taylor_0001.jpg\n",
            "LFW/lfw_align_112/Sergei_Yastrzhembsky/\n",
            "LFW/lfw_align_112/Sergei_Yastrzhembsky/Sergei_Yastrzhembsky_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Reiser/\n",
            "LFW/lfw_align_112/Paul_Reiser/Paul_Reiser_0001.jpg\n",
            "LFW/lfw_align_112/Curtis_Rodriguez/\n",
            "LFW/lfw_align_112/Curtis_Rodriguez/Curtis_Rodriguez_0001.jpg\n",
            "LFW/lfw_align_112/Jessica_Lynch/\n",
            "LFW/lfw_align_112/Jessica_Lynch/Jessica_Lynch_0001.jpg\n",
            "LFW/lfw_align_112/Jessica_Lynch/Jessica_Lynch_0002.jpg\n",
            "LFW/lfw_align_112/Park_Jung_Sung/\n",
            "LFW/lfw_align_112/Park_Jung_Sung/Park_Jung_Sung_0001.jpg\n",
            "LFW/lfw_align_112/John_Lisowski/\n",
            "LFW/lfw_align_112/John_Lisowski/John_Lisowski_0001.jpg\n",
            "LFW/lfw_align_112/Katie_Holmes/\n",
            "LFW/lfw_align_112/Katie_Holmes/Katie_Holmes_0001.jpg\n",
            "LFW/lfw_align_112/James_Roberts/\n",
            "LFW/lfw_align_112/James_Roberts/James_Roberts_0001.jpg\n",
            "LFW/lfw_align_112/Alberto_Fujimori/\n",
            "LFW/lfw_align_112/Alberto_Fujimori/Alberto_Fujimori_0001.jpg\n",
            "LFW/lfw_align_112/Alberto_Fujimori/Alberto_Fujimori_0002.jpg\n",
            "LFW/lfw_align_112/Thomas_Wyman/\n",
            "LFW/lfw_align_112/Thomas_Wyman/Thomas_Wyman_0001.jpg\n",
            "LFW/lfw_align_112/Thomas_Wyman/Thomas_Wyman_0002.jpg\n",
            "LFW/lfw_align_112/Alberto_Acosta/\n",
            "LFW/lfw_align_112/Alberto_Acosta/Alberto_Acosta_0001.jpg\n",
            "LFW/lfw_align_112/Alexandra_Pelosi/\n",
            "LFW/lfw_align_112/Alexandra_Pelosi/Alexandra_Pelosi_0001.jpg\n",
            "LFW/lfw_align_112/Sandra_Day_OConner/\n",
            "LFW/lfw_align_112/Sandra_Day_OConner/Sandra_Day_OConner_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Craddick/\n",
            "LFW/lfw_align_112/Tom_Craddick/Tom_Craddick_0004.jpg\n",
            "LFW/lfw_align_112/Tom_Craddick/Tom_Craddick_0003.jpg\n",
            "LFW/lfw_align_112/Tom_Craddick/Tom_Craddick_0002.jpg\n",
            "LFW/lfw_align_112/Tom_Craddick/Tom_Craddick_0001.jpg\n",
            "LFW/lfw_align_112/James_Robertson_Jr/\n",
            "LFW/lfw_align_112/James_Robertson_Jr/James_Robertson_Jr_0001.jpg\n",
            "LFW/lfw_align_112/Eddie_Fenech_Adami/\n",
            "LFW/lfw_align_112/Eddie_Fenech_Adami/Eddie_Fenech_Adami_0001.jpg\n",
            "LFW/lfw_align_112/Brigitte_Boisselier/\n",
            "LFW/lfw_align_112/Brigitte_Boisselier/Brigitte_Boisselier_0002.jpg\n",
            "LFW/lfw_align_112/Brigitte_Boisselier/Brigitte_Boisselier_0001.jpg\n",
            "LFW/lfw_align_112/Carolina_Barco/\n",
            "LFW/lfw_align_112/Carolina_Barco/Carolina_Barco_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Khamenei/\n",
            "LFW/lfw_align_112/Ali_Khamenei/Ali_Khamenei_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Khamenei/Ali_Khamenei_0003.jpg\n",
            "LFW/lfw_align_112/Ali_Khamenei/Ali_Khamenei_0002.jpg\n",
            "LFW/lfw_align_112/Andy_Hebb/\n",
            "LFW/lfw_align_112/Andy_Hebb/Andy_Hebb_0001.jpg\n",
            "LFW/lfw_align_112/Andy_Hebb/Andy_Hebb_0002.jpg\n",
            "LFW/lfw_align_112/Jerry_Sexton/\n",
            "LFW/lfw_align_112/Jerry_Sexton/Jerry_Sexton_0001.jpg\n",
            "LFW/lfw_align_112/Sachin_Tendulkar/\n",
            "LFW/lfw_align_112/Sachin_Tendulkar/Sachin_Tendulkar_0001.jpg\n",
            "LFW/lfw_align_112/Claude_Jorda/\n",
            "LFW/lfw_align_112/Claude_Jorda/Claude_Jorda_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Kocharian/\n",
            "LFW/lfw_align_112/Robert_Kocharian/Robert_Kocharian_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Kocharian/Robert_Kocharian_0003.jpg\n",
            "LFW/lfw_align_112/Robert_Kocharian/Robert_Kocharian_0002.jpg\n",
            "LFW/lfw_align_112/Robert_Kocharian/Robert_Kocharian_0004.jpg\n",
            "LFW/lfw_align_112/Robert_Kocharian/Robert_Kocharian_0005.jpg\n",
            "LFW/lfw_align_112/Donatella_Versace/\n",
            "LFW/lfw_align_112/Donatella_Versace/Donatella_Versace_0002.jpg\n",
            "LFW/lfw_align_112/Donatella_Versace/Donatella_Versace_0003.jpg\n",
            "LFW/lfw_align_112/Donatella_Versace/Donatella_Versace_0001.jpg\n",
            "LFW/lfw_align_112/Giulio_Andreotti/\n",
            "LFW/lfw_align_112/Giulio_Andreotti/Giulio_Andreotti_0001.jpg\n",
            "LFW/lfw_align_112/Nestor_Santillan/\n",
            "LFW/lfw_align_112/Nestor_Santillan/Nestor_Santillan_0001.jpg\n",
            "LFW/lfw_align_112/Ashraf_Alasmar/\n",
            "LFW/lfw_align_112/Ashraf_Alasmar/Ashraf_Alasmar_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0003.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0005.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0002.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0011.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0013.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0009.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0010.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0006.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0012.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0008.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0004.jpg\n",
            "LFW/lfw_align_112/Charles_Moose/Charles_Moose_0007.jpg\n",
            "LFW/lfw_align_112/Ray_Young/\n",
            "LFW/lfw_align_112/Ray_Young/Ray_Young_0001.jpg\n",
            "LFW/lfw_align_112/Wilma_McNabb/\n",
            "LFW/lfw_align_112/Wilma_McNabb/Wilma_McNabb_0001.jpg\n",
            "LFW/lfw_align_112/Camilla_Parker_Bowles/\n",
            "LFW/lfw_align_112/Camilla_Parker_Bowles/Camilla_Parker_Bowles_0001.jpg\n",
            "LFW/lfw_align_112/Camilla_Parker_Bowles/Camilla_Parker_Bowles_0002.jpg\n",
            "LFW/lfw_align_112/Roger_Mahony/\n",
            "LFW/lfw_align_112/Roger_Mahony/Roger_Mahony_0001.jpg\n",
            "LFW/lfw_align_112/Joaquim_Levy/\n",
            "LFW/lfw_align_112/Joaquim_Levy/Joaquim_Levy_0001.jpg\n",
            "LFW/lfw_align_112/Rob_Morrow/\n",
            "LFW/lfw_align_112/Rob_Morrow/Rob_Morrow_0001.jpg\n",
            "LFW/lfw_align_112/Alex_King/\n",
            "LFW/lfw_align_112/Alex_King/Alex_King_0001.jpg\n",
            "LFW/lfw_align_112/John_Herrington/\n",
            "LFW/lfw_align_112/John_Herrington/John_Herrington_0001.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0006.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0005.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0010.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0004.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0001.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0003.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0007.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0009.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0002.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0008.jpg\n",
            "LFW/lfw_align_112/Ann_Veneman/Ann_Veneman_0011.jpg\n",
            "LFW/lfw_align_112/Mira_Sorvino/\n",
            "LFW/lfw_align_112/Mira_Sorvino/Mira_Sorvino_0001.jpg\n",
            "LFW/lfw_align_112/Kurt_Budke/\n",
            "LFW/lfw_align_112/Kurt_Budke/Kurt_Budke_0001.jpg\n",
            "LFW/lfw_align_112/Sally_Kirkland/\n",
            "LFW/lfw_align_112/Sally_Kirkland/Sally_Kirkland_0001.jpg\n",
            "LFW/lfw_align_112/Sally_Kirkland/Sally_Kirkland_0003.jpg\n",
            "LFW/lfw_align_112/Sally_Kirkland/Sally_Kirkland_0004.jpg\n",
            "LFW/lfw_align_112/Sally_Kirkland/Sally_Kirkland_0002.jpg\n",
            "LFW/lfw_align_112/Meirion_Evans/\n",
            "LFW/lfw_align_112/Meirion_Evans/Meirion_Evans_0001.jpg\n",
            "LFW/lfw_align_112/Sandra_Banning/\n",
            "LFW/lfw_align_112/Sandra_Banning/Sandra_Banning_0001.jpg\n",
            "LFW/lfw_align_112/Philippe_Gagnon/\n",
            "LFW/lfw_align_112/Philippe_Gagnon/Philippe_Gagnon_0001.jpg\n",
            "LFW/lfw_align_112/Ilan_Ramon/\n",
            "LFW/lfw_align_112/Ilan_Ramon/Ilan_Ramon_0001.jpg\n",
            "LFW/lfw_align_112/Ilan_Ramon/Ilan_Ramon_0002.jpg\n",
            "LFW/lfw_align_112/Ilan_Ramon/Ilan_Ramon_0003.jpg\n",
            "LFW/lfw_align_112/Darryl_Stingley/\n",
            "LFW/lfw_align_112/Darryl_Stingley/Darryl_Stingley_0001.jpg\n",
            "LFW/lfw_align_112/Newt_Gingrich/\n",
            "LFW/lfw_align_112/Newt_Gingrich/Newt_Gingrich_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Daley/\n",
            "LFW/lfw_align_112/Richard_Daley/Richard_Daley_0001.jpg\n",
            "LFW/lfw_align_112/Robin_Tunney/\n",
            "LFW/lfw_align_112/Robin_Tunney/Robin_Tunney_0001.jpg\n",
            "LFW/lfw_align_112/Robbie_Coltrane/\n",
            "LFW/lfw_align_112/Robbie_Coltrane/Robbie_Coltrane_0001.jpg\n",
            "LFW/lfw_align_112/Sohail_Abbas/\n",
            "LFW/lfw_align_112/Sohail_Abbas/Sohail_Abbas_0001.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0007.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0003.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0002.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0006.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0004.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0001.jpg\n",
            "LFW/lfw_align_112/Vojislav_Kostunica/Vojislav_Kostunica_0005.jpg\n",
            "LFW/lfw_align_112/Kevin_Crane/\n",
            "LFW/lfw_align_112/Kevin_Crane/Kevin_Crane_0001.jpg\n",
            "LFW/lfw_align_112/Conrad_Black/\n",
            "LFW/lfw_align_112/Conrad_Black/Conrad_Black_0001.jpg\n",
            "LFW/lfw_align_112/Michael_DeMinico/\n",
            "LFW/lfw_align_112/Michael_DeMinico/Michael_DeMinico_0001.jpg\n",
            "LFW/lfw_align_112/Kathy_Baker/\n",
            "LFW/lfw_align_112/Kathy_Baker/Kathy_Baker_0001.jpg\n",
            "LFW/lfw_align_112/Julian_Fantino/\n",
            "LFW/lfw_align_112/Julian_Fantino/Julian_Fantino_0001.jpg\n",
            "LFW/lfw_align_112/Kathleen_Kennedy_Townsend/\n",
            "LFW/lfw_align_112/Kathleen_Kennedy_Townsend/Kathleen_Kennedy_Townsend_0001.jpg\n",
            "LFW/lfw_align_112/Kathleen_Kennedy_Townsend/Kathleen_Kennedy_Townsend_0002.jpg\n",
            "LFW/lfw_align_112/Kathleen_Kennedy_Townsend/Kathleen_Kennedy_Townsend_0004.jpg\n",
            "LFW/lfw_align_112/Kathleen_Kennedy_Townsend/Kathleen_Kennedy_Townsend_0003.jpg\n",
            "LFW/lfw_align_112/Teresa_Williams/\n",
            "LFW/lfw_align_112/Teresa_Williams/Teresa_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Roger_King/\n",
            "LFW/lfw_align_112/Roger_King/Roger_King_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0003.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0007.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0006.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0008.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0004.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0005.jpg\n",
            "LFW/lfw_align_112/Ali_Naimi/Ali_Naimi_0002.jpg\n",
            "LFW/lfw_align_112/Hrithik_Roshan/\n",
            "LFW/lfw_align_112/Hrithik_Roshan/Hrithik_Roshan_0001.jpg\n",
            "LFW/lfw_align_112/Kenneth_Brill/\n",
            "LFW/lfw_align_112/Kenneth_Brill/Kenneth_Brill_0001.jpg\n",
            "LFW/lfw_align_112/Dorothy_Wilson/\n",
            "LFW/lfw_align_112/Dorothy_Wilson/Dorothy_Wilson_0001.jpg\n",
            "LFW/lfw_align_112/Vince_Gill/\n",
            "LFW/lfw_align_112/Vince_Gill/Vince_Gill_0002.jpg\n",
            "LFW/lfw_align_112/Vince_Gill/Vince_Gill_0001.jpg\n",
            "LFW/lfw_align_112/Pierre_Png/\n",
            "LFW/lfw_align_112/Pierre_Png/Pierre_Png_0001.jpg\n",
            "LFW/lfw_align_112/Francois_Ozon/\n",
            "LFW/lfw_align_112/Francois_Ozon/Francois_Ozon_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Paul_Evans/\n",
            "LFW/lfw_align_112/Richard_Paul_Evans/Richard_Paul_Evans_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Schnackenberg/\n",
            "LFW/lfw_align_112/Tom_Schnackenberg/Tom_Schnackenberg_0001.jpg\n",
            "LFW/lfw_align_112/Jules_Asner/\n",
            "LFW/lfw_align_112/Jules_Asner/Jules_Asner_0001.jpg\n",
            "LFW/lfw_align_112/Dennis_Miller/\n",
            "LFW/lfw_align_112/Dennis_Miller/Dennis_Miller_0001.jpg\n",
            "LFW/lfw_align_112/George_Karl/\n",
            "LFW/lfw_align_112/George_Karl/George_Karl_0001.jpg\n",
            "LFW/lfw_align_112/George_Karl/George_Karl_0002.jpg\n",
            "LFW/lfw_align_112/Terrell_Suggs/\n",
            "LFW/lfw_align_112/Terrell_Suggs/Terrell_Suggs_0001.jpg\n",
            "LFW/lfw_align_112/Terrell_Suggs/Terrell_Suggs_0002.jpg\n",
            "LFW/lfw_align_112/Monica_Lewinsky/\n",
            "LFW/lfw_align_112/Monica_Lewinsky/Monica_Lewinsky_0002.jpg\n",
            "LFW/lfw_align_112/Monica_Lewinsky/Monica_Lewinsky_0001.jpg\n",
            "LFW/lfw_align_112/Monica_Lewinsky/Monica_Lewinsky_0003.jpg\n",
            "LFW/lfw_align_112/John_Henry/\n",
            "LFW/lfw_align_112/John_Henry/John_Henry_0001.jpg\n",
            "LFW/lfw_align_112/Jaime_Orti/\n",
            "LFW/lfw_align_112/Jaime_Orti/Jaime_Orti_0001.jpg\n",
            "LFW/lfw_align_112/Leigh_Winchell/\n",
            "LFW/lfw_align_112/Leigh_Winchell/Leigh_Winchell_0001.jpg\n",
            "LFW/lfw_align_112/Ellen_Barkin/\n",
            "LFW/lfw_align_112/Ellen_Barkin/Ellen_Barkin_0001.jpg\n",
            "LFW/lfw_align_112/Norodom_Chakrapong/\n",
            "LFW/lfw_align_112/Norodom_Chakrapong/Norodom_Chakrapong_0001.jpg\n",
            "LFW/lfw_align_112/Andrew_Wetzler/\n",
            "LFW/lfw_align_112/Andrew_Wetzler/Andrew_Wetzler_0001.jpg\n",
            "LFW/lfw_align_112/Gina_Torres/\n",
            "LFW/lfw_align_112/Gina_Torres/Gina_Torres_0001.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0026.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0010.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0005.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0002.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0024.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0027.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0032.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0028.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0023.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0025.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0015.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0007.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0004.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0022.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0003.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0017.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0009.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0019.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0011.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0031.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0035.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0021.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0020.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0033.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0001.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0016.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0012.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0014.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0030.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0013.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0018.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0006.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0008.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0034.jpg\n",
            "LFW/lfw_align_112/Alvaro_Uribe/Alvaro_Uribe_0029.jpg\n",
            "LFW/lfw_align_112/Marie_Haghal/\n",
            "LFW/lfw_align_112/Marie_Haghal/Marie_Haghal_0001.jpg\n",
            "LFW/lfw_align_112/Jakob_Kellenberger/\n",
            "LFW/lfw_align_112/Jakob_Kellenberger/Jakob_Kellenberger_0001.jpg\n",
            "LFW/lfw_align_112/Sean_Patrick_Thomas/\n",
            "LFW/lfw_align_112/Sean_Patrick_Thomas/Sean_Patrick_Thomas_0001.jpg\n",
            "LFW/lfw_align_112/Dule_Hill/\n",
            "LFW/lfw_align_112/Dule_Hill/Dule_Hill_0001.jpg\n",
            "LFW/lfw_align_112/Theresa_May/\n",
            "LFW/lfw_align_112/Theresa_May/Theresa_May_0003.jpg\n",
            "LFW/lfw_align_112/Theresa_May/Theresa_May_0002.jpg\n",
            "LFW/lfw_align_112/Theresa_May/Theresa_May_0001.jpg\n",
            "LFW/lfw_align_112/Mercedes_Amor/\n",
            "LFW/lfw_align_112/Mercedes_Amor/Mercedes_Amor_0001.jpg\n",
            "LFW/lfw_align_112/Eric_Bana/\n",
            "LFW/lfw_align_112/Eric_Bana/Eric_Bana_0001.jpg\n",
            "LFW/lfw_align_112/Martin_McCauley/\n",
            "LFW/lfw_align_112/Martin_McCauley/Martin_McCauley_0001.jpg\n",
            "LFW/lfw_align_112/Martin_McCauley/Martin_McCauley_0002.jpg\n",
            "LFW/lfw_align_112/Dino_Risi/\n",
            "LFW/lfw_align_112/Dino_Risi/Dino_Risi_0001.jpg\n",
            "LFW/lfw_align_112/Greg_Ostertag/\n",
            "LFW/lfw_align_112/Greg_Ostertag/Greg_Ostertag_0002.jpg\n",
            "LFW/lfw_align_112/Greg_Ostertag/Greg_Ostertag_0001.jpg\n",
            "LFW/lfw_align_112/Dirk_Kempthorne/\n",
            "LFW/lfw_align_112/Dirk_Kempthorne/Dirk_Kempthorne_0001.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0001.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0006.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0003.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0005.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0002.jpg\n",
            "LFW/lfw_align_112/Valery_Giscard_dEstaing/Valery_Giscard_dEstaing_0004.jpg\n",
            "LFW/lfw_align_112/John_F_Kennedy_Jr/\n",
            "LFW/lfw_align_112/John_F_Kennedy_Jr/John_F_Kennedy_Jr_0001.jpg\n",
            "LFW/lfw_align_112/John_F_Kennedy_Jr/John_F_Kennedy_Jr_0002.jpg\n",
            "LFW/lfw_align_112/Jan_De_Bont/\n",
            "LFW/lfw_align_112/Jan_De_Bont/Jan_De_Bont_0001.jpg\n",
            "LFW/lfw_align_112/Danielle_Spencer/\n",
            "LFW/lfw_align_112/Danielle_Spencer/Danielle_Spencer_0001.jpg\n",
            "LFW/lfw_align_112/Milt_Heflin/\n",
            "LFW/lfw_align_112/Milt_Heflin/Milt_Heflin_0001.jpg\n",
            "LFW/lfw_align_112/Travis_Rudolph/\n",
            "LFW/lfw_align_112/Travis_Rudolph/Travis_Rudolph_0001.jpg\n",
            "LFW/lfw_align_112/Sonia_Gandhi/\n",
            "LFW/lfw_align_112/Sonia_Gandhi/Sonia_Gandhi_0004.jpg\n",
            "LFW/lfw_align_112/Sonia_Gandhi/Sonia_Gandhi_0003.jpg\n",
            "LFW/lfw_align_112/Sonia_Gandhi/Sonia_Gandhi_0001.jpg\n",
            "LFW/lfw_align_112/Sonia_Gandhi/Sonia_Gandhi_0002.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Braquet/\n",
            "LFW/lfw_align_112/Jean-Claude_Braquet/Jean-Claude_Braquet_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Braquet/Jean-Claude_Braquet_0002.jpg\n",
            "LFW/lfw_align_112/Mary_Steenburgen/\n",
            "LFW/lfw_align_112/Mary_Steenburgen/Mary_Steenburgen_0003.jpg\n",
            "LFW/lfw_align_112/Mary_Steenburgen/Mary_Steenburgen_0001.jpg\n",
            "LFW/lfw_align_112/Mary_Steenburgen/Mary_Steenburgen_0002.jpg\n",
            "LFW/lfw_align_112/Marc_Racicot/\n",
            "LFW/lfw_align_112/Marc_Racicot/Marc_Racicot_0001.jpg\n",
            "LFW/lfw_align_112/Charla_Moye/\n",
            "LFW/lfw_align_112/Charla_Moye/Charla_Moye_0001.jpg\n",
            "LFW/lfw_align_112/Avril_Lavigne/\n",
            "LFW/lfw_align_112/Avril_Lavigne/Avril_Lavigne_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Jong-Il/\n",
            "LFW/lfw_align_112/Kim_Jong-Il/Kim_Jong-Il_0003.jpg\n",
            "LFW/lfw_align_112/Kim_Jong-Il/Kim_Jong-Il_0002.jpg\n",
            "LFW/lfw_align_112/Kim_Jong-Il/Kim_Jong-Il_0004.jpg\n",
            "LFW/lfw_align_112/Kim_Jong-Il/Kim_Jong-Il_0001.jpg\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/Maria_Soledad_Alvear_Valenzuela_0002.jpg\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/Maria_Soledad_Alvear_Valenzuela_0003.jpg\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/Maria_Soledad_Alvear_Valenzuela_0004.jpg\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/Maria_Soledad_Alvear_Valenzuela_0005.jpg\n",
            "LFW/lfw_align_112/Maria_Soledad_Alvear_Valenzuela/Maria_Soledad_Alvear_Valenzuela_0001.jpg\n",
            "LFW/lfw_align_112/Lina_Krasnoroutskaya/\n",
            "LFW/lfw_align_112/Lina_Krasnoroutskaya/Lina_Krasnoroutskaya_0001.jpg\n",
            "LFW/lfw_align_112/Lina_Krasnoroutskaya/Lina_Krasnoroutskaya_0002.jpg\n",
            "LFW/lfw_align_112/Ronnie_Musgrove/\n",
            "LFW/lfw_align_112/Ronnie_Musgrove/Ronnie_Musgrove_0001.jpg\n",
            "LFW/lfw_align_112/Lou_Ye/\n",
            "LFW/lfw_align_112/Lou_Ye/Lou_Ye_0001.jpg\n",
            "LFW/lfw_align_112/Ingrid_Betancourt/\n",
            "LFW/lfw_align_112/Ingrid_Betancourt/Ingrid_Betancourt_0001.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0002.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0006.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0005.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0003.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0004.jpg\n",
            "LFW/lfw_align_112/Gwyneth_Paltrow/Gwyneth_Paltrow_0001.jpg\n",
            "LFW/lfw_align_112/Mona_Ayoub/\n",
            "LFW/lfw_align_112/Mona_Ayoub/Mona_Ayoub_0001.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0005.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0006.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0003.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0001.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0007.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0004.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0008.jpg\n",
            "LFW/lfw_align_112/Kevin_Costner/Kevin_Costner_0002.jpg\n",
            "LFW/lfw_align_112/Rob_Moore/\n",
            "LFW/lfw_align_112/Rob_Moore/Rob_Moore_0001.jpg\n",
            "LFW/lfw_align_112/Kellie_Greene/\n",
            "LFW/lfw_align_112/Kellie_Greene/Kellie_Greene_0001.jpg\n",
            "LFW/lfw_align_112/Cindy_Margolis/\n",
            "LFW/lfw_align_112/Cindy_Margolis/Cindy_Margolis_0002.jpg\n",
            "LFW/lfw_align_112/Cindy_Margolis/Cindy_Margolis_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Levin/\n",
            "LFW/lfw_align_112/Richard_Levin/Richard_Levin_0001.jpg\n",
            "LFW/lfw_align_112/Jack_Welch/\n",
            "LFW/lfw_align_112/Jack_Welch/Jack_Welch_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Ebberharter/\n",
            "LFW/lfw_align_112/Stephen_Ebberharter/Stephen_Ebberharter_0001.jpg\n",
            "LFW/lfw_align_112/Joaquin_Sanchez/\n",
            "LFW/lfw_align_112/Joaquin_Sanchez/Joaquin_Sanchez_0001.jpg\n",
            "LFW/lfw_align_112/Yusuf_Misbac/\n",
            "LFW/lfw_align_112/Yusuf_Misbac/Yusuf_Misbac_0001.jpg\n",
            "LFW/lfw_align_112/Pa_Kou_Hang/\n",
            "LFW/lfw_align_112/Pa_Kou_Hang/Pa_Kou_Hang_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Sisk/\n",
            "LFW/lfw_align_112/Mark_Sisk/Mark_Sisk_0001.jpg\n",
            "LFW/lfw_align_112/Tina_Andrews/\n",
            "LFW/lfw_align_112/Tina_Andrews/Tina_Andrews_0001.jpg\n",
            "LFW/lfw_align_112/Vince_Vaughan/\n",
            "LFW/lfw_align_112/Vince_Vaughan/Vince_Vaughan_0001.jpg\n",
            "LFW/lfw_align_112/Timothy_Rigas/\n",
            "LFW/lfw_align_112/Timothy_Rigas/Timothy_Rigas_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Carl/\n",
            "LFW/lfw_align_112/Richard_Carl/Richard_Carl_0001.jpg\n",
            "LFW/lfw_align_112/Sheila_Fraser/\n",
            "LFW/lfw_align_112/Sheila_Fraser/Sheila_Fraser_0002.jpg\n",
            "LFW/lfw_align_112/Sheila_Fraser/Sheila_Fraser_0001.jpg\n",
            "LFW/lfw_align_112/Alberto_Gonzales/\n",
            "LFW/lfw_align_112/Alberto_Gonzales/Alberto_Gonzales_0001.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0002.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0005.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0003.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0004.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0001.jpg\n",
            "LFW/lfw_align_112/Christine_Todd_Whitman/Christine_Todd_Whitman_0006.jpg\n",
            "LFW/lfw_align_112/Tia_Mowry/\n",
            "LFW/lfw_align_112/Tia_Mowry/Tia_Mowry_0001.jpg\n",
            "LFW/lfw_align_112/Freda_Black/\n",
            "LFW/lfw_align_112/Freda_Black/Freda_Black_0001.jpg\n",
            "LFW/lfw_align_112/Emma_Nicholson/\n",
            "LFW/lfw_align_112/Emma_Nicholson/Emma_Nicholson_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Paxson/\n",
            "LFW/lfw_align_112/Jim_Paxson/Jim_Paxson_0001.jpg\n",
            "LFW/lfw_align_112/Buzz_Hargrove/\n",
            "LFW/lfw_align_112/Buzz_Hargrove/Buzz_Hargrove_0001.jpg\n",
            "LFW/lfw_align_112/Helena_Schneider/\n",
            "LFW/lfw_align_112/Helena_Schneider/Helena_Schneider_0001.jpg\n",
            "LFW/lfw_align_112/Cristina_Torrens_Valero/\n",
            "LFW/lfw_align_112/Cristina_Torrens_Valero/Cristina_Torrens_Valero_0001.jpg\n",
            "LFW/lfw_align_112/Rachel_Wadsworth/\n",
            "LFW/lfw_align_112/Rachel_Wadsworth/Rachel_Wadsworth_0001.jpg\n",
            "LFW/lfw_align_112/Sergio_Garcia/\n",
            "LFW/lfw_align_112/Sergio_Garcia/Sergio_Garcia_0002.jpg\n",
            "LFW/lfw_align_112/Sergio_Garcia/Sergio_Garcia_0001.jpg\n",
            "LFW/lfw_align_112/Kelsey_Grammer/\n",
            "LFW/lfw_align_112/Kelsey_Grammer/Kelsey_Grammer_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Pleau/\n",
            "LFW/lfw_align_112/Larry_Pleau/Larry_Pleau_0001.jpg\n",
            "LFW/lfw_align_112/Eddie_Jordan/\n",
            "LFW/lfw_align_112/Eddie_Jordan/Eddie_Jordan_0001.jpg\n",
            "LFW/lfw_align_112/Mireille_Jospin-Dandieu/\n",
            "LFW/lfw_align_112/Mireille_Jospin-Dandieu/Mireille_Jospin-Dandieu_0001.jpg\n",
            "LFW/lfw_align_112/Filippo_Inzaghi/\n",
            "LFW/lfw_align_112/Filippo_Inzaghi/Filippo_Inzaghi_0001.jpg\n",
            "LFW/lfw_align_112/Filippo_Inzaghi/Filippo_Inzaghi_0003.jpg\n",
            "LFW/lfw_align_112/Filippo_Inzaghi/Filippo_Inzaghi_0002.jpg\n",
            "LFW/lfw_align_112/Dan_LaCoutre/\n",
            "LFW/lfw_align_112/Dan_LaCoutre/Dan_LaCoutre_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Bollman/\n",
            "LFW/lfw_align_112/Jim_Bollman/Jim_Bollman_0001.jpg\n",
            "LFW/lfw_align_112/Felix_Doh/\n",
            "LFW/lfw_align_112/Felix_Doh/Felix_Doh_0001.jpg\n",
            "LFW/lfw_align_112/Juan_Fernandez/\n",
            "LFW/lfw_align_112/Juan_Fernandez/Juan_Fernandez_0001.jpg\n",
            "LFW/lfw_align_112/Boris_Jordan/\n",
            "LFW/lfw_align_112/Boris_Jordan/Boris_Jordan_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Daldry/\n",
            "LFW/lfw_align_112/Stephen_Daldry/Stephen_Daldry_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Daldry/Stephen_Daldry_0002.jpg\n",
            "LFW/lfw_align_112/Kalpana_Chawla/\n",
            "LFW/lfw_align_112/Kalpana_Chawla/Kalpana_Chawla_0002.jpg\n",
            "LFW/lfw_align_112/Kalpana_Chawla/Kalpana_Chawla_0001.jpg\n",
            "LFW/lfw_align_112/Kalpana_Chawla/Kalpana_Chawla_0004.jpg\n",
            "LFW/lfw_align_112/Kalpana_Chawla/Kalpana_Chawla_0003.jpg\n",
            "LFW/lfw_align_112/Kalpana_Chawla/Kalpana_Chawla_0005.jpg\n",
            "LFW/lfw_align_112/Dan_Bartlett/\n",
            "LFW/lfw_align_112/Dan_Bartlett/Dan_Bartlett_0001.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0006.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0016.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0031.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0024.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0020.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0019.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0002.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0029.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0010.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0013.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0014.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0030.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0004.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0026.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0027.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0023.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0025.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0032.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0003.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0011.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0018.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0007.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0012.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0028.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0009.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0005.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0022.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0017.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0001.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0008.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0021.jpg\n",
            "LFW/lfw_align_112/Roh_Moo-hyun/Roh_Moo-hyun_0015.jpg\n",
            "LFW/lfw_align_112/Bernadette_Peters/\n",
            "LFW/lfw_align_112/Bernadette_Peters/Bernadette_Peters_0001.jpg\n",
            "LFW/lfw_align_112/Al_Gore/\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0003.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0002.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0004.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0001.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0008.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0006.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0007.jpg\n",
            "LFW/lfw_align_112/Al_Gore/Al_Gore_0005.jpg\n",
            "LFW/lfw_align_112/Mike_OConnell/\n",
            "LFW/lfw_align_112/Mike_OConnell/Mike_OConnell_0001.jpg\n",
            "LFW/lfw_align_112/Nick_Rimando/\n",
            "LFW/lfw_align_112/Nick_Rimando/Nick_Rimando_0001.jpg\n",
            "LFW/lfw_align_112/Bobo_Balde/\n",
            "LFW/lfw_align_112/Bobo_Balde/Bobo_Balde_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Cattrall/\n",
            "LFW/lfw_align_112/Kim_Cattrall/Kim_Cattrall_0001.jpg\n",
            "LFW/lfw_align_112/Estella_Warren/\n",
            "LFW/lfw_align_112/Estella_Warren/Estella_Warren_0001.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0003.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0005.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0002.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0004.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0001.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0006.jpg\n",
            "LFW/lfw_align_112/Goldie_Hawn/Goldie_Hawn_0007.jpg\n",
            "LFW/lfw_align_112/Angel_Lockward/\n",
            "LFW/lfw_align_112/Angel_Lockward/Angel_Lockward_0001.jpg\n",
            "LFW/lfw_align_112/Pedro_Malan/\n",
            "LFW/lfw_align_112/Pedro_Malan/Pedro_Malan_0001.jpg\n",
            "LFW/lfw_align_112/Pedro_Malan/Pedro_Malan_0005.jpg\n",
            "LFW/lfw_align_112/Pedro_Malan/Pedro_Malan_0003.jpg\n",
            "LFW/lfw_align_112/Pedro_Malan/Pedro_Malan_0002.jpg\n",
            "LFW/lfw_align_112/Pedro_Malan/Pedro_Malan_0004.jpg\n",
            "LFW/lfw_align_112/Oswald_Gruebel/\n",
            "LFW/lfw_align_112/Oswald_Gruebel/Oswald_Gruebel_0001.jpg\n",
            "LFW/lfw_align_112/Kristin_Davis/\n",
            "LFW/lfw_align_112/Kristin_Davis/Kristin_Davis_0003.jpg\n",
            "LFW/lfw_align_112/Kristin_Davis/Kristin_Davis_0001.jpg\n",
            "LFW/lfw_align_112/Kristin_Davis/Kristin_Davis_0002.jpg\n",
            "LFW/lfw_align_112/Jerome_Golmard/\n",
            "LFW/lfw_align_112/Jerome_Golmard/Jerome_Golmard_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Falwell/\n",
            "LFW/lfw_align_112/Jerry_Falwell/Jerry_Falwell_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Falwell/Jerry_Falwell_0002.jpg\n",
            "LFW/lfw_align_112/Marcelo_Rios/\n",
            "LFW/lfw_align_112/Marcelo_Rios/Marcelo_Rios_0005.jpg\n",
            "LFW/lfw_align_112/Marcelo_Rios/Marcelo_Rios_0003.jpg\n",
            "LFW/lfw_align_112/Marcelo_Rios/Marcelo_Rios_0002.jpg\n",
            "LFW/lfw_align_112/Marcelo_Rios/Marcelo_Rios_0001.jpg\n",
            "LFW/lfw_align_112/Marcelo_Rios/Marcelo_Rios_0004.jpg\n",
            "LFW/lfw_align_112/Randy_Johnson/\n",
            "LFW/lfw_align_112/Randy_Johnson/Randy_Johnson_0001.jpg\n",
            "LFW/lfw_align_112/Assad_Ahmadi/\n",
            "LFW/lfw_align_112/Assad_Ahmadi/Assad_Ahmadi_0001.jpg\n",
            "LFW/lfw_align_112/Yale_Kamisar/\n",
            "LFW/lfw_align_112/Yale_Kamisar/Yale_Kamisar_0001.jpg\n",
            "LFW/lfw_align_112/Claire_Danes/\n",
            "LFW/lfw_align_112/Claire_Danes/Claire_Danes_0002.jpg\n",
            "LFW/lfw_align_112/Claire_Danes/Claire_Danes_0003.jpg\n",
            "LFW/lfw_align_112/Claire_Danes/Claire_Danes_0001.jpg\n",
            "LFW/lfw_align_112/Aiysha_Smith/\n",
            "LFW/lfw_align_112/Aiysha_Smith/Aiysha_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Idi_Amin/\n",
            "LFW/lfw_align_112/Idi_Amin/Idi_Amin_0001.jpg\n",
            "LFW/lfw_align_112/Robby_Ginepri/\n",
            "LFW/lfw_align_112/Robby_Ginepri/Robby_Ginepri_0002.jpg\n",
            "LFW/lfw_align_112/Robby_Ginepri/Robby_Ginepri_0001.jpg\n",
            "LFW/lfw_align_112/Beth_Jones/\n",
            "LFW/lfw_align_112/Beth_Jones/Beth_Jones_0001.jpg\n",
            "LFW/lfw_align_112/Beth_Jones/Beth_Jones_0002.jpg\n",
            "LFW/lfw_align_112/John_Brady/\n",
            "LFW/lfw_align_112/John_Brady/John_Brady_0002.jpg\n",
            "LFW/lfw_align_112/John_Brady/John_Brady_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Sebastien_Giguere/\n",
            "LFW/lfw_align_112/Jean-Sebastien_Giguere/Jean-Sebastien_Giguere_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Sebastien_Giguere/Jean-Sebastien_Giguere_0002.jpg\n",
            "LFW/lfw_align_112/Linn_Thornton/\n",
            "LFW/lfw_align_112/Linn_Thornton/Linn_Thornton_0001.jpg\n",
            "LFW/lfw_align_112/Christian_Patino/\n",
            "LFW/lfw_align_112/Christian_Patino/Christian_Patino_0001.jpg\n",
            "LFW/lfw_align_112/Ivana_Trump/\n",
            "LFW/lfw_align_112/Ivana_Trump/Ivana_Trump_0001.jpg\n",
            "LFW/lfw_align_112/Jean_Carnahan/\n",
            "LFW/lfw_align_112/Jean_Carnahan/Jean_Carnahan_0001.jpg\n",
            "LFW/lfw_align_112/Jean_Carnahan/Jean_Carnahan_0002.jpg\n",
            "LFW/lfw_align_112/Tim_Blake_Nelson/\n",
            "LFW/lfw_align_112/Tim_Blake_Nelson/Tim_Blake_Nelson_0001.jpg\n",
            "LFW/lfw_align_112/Hamza_Atiya_Muhsen/\n",
            "LFW/lfw_align_112/Hamza_Atiya_Muhsen/Hamza_Atiya_Muhsen_0001.jpg\n",
            "LFW/lfw_align_112/Goran_Zivkovic/\n",
            "LFW/lfw_align_112/Goran_Zivkovic/Goran_Zivkovic_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Eskew/\n",
            "LFW/lfw_align_112/Mike_Eskew/Mike_Eskew_0001.jpg\n",
            "LFW/lfw_align_112/Loretta_Lynn_Harper/\n",
            "LFW/lfw_align_112/Loretta_Lynn_Harper/Loretta_Lynn_Harper_0001.jpg\n",
            "LFW/lfw_align_112/Ximena_Bohorquez/\n",
            "LFW/lfw_align_112/Ximena_Bohorquez/Ximena_Bohorquez_0001.jpg\n",
            "LFW/lfw_align_112/Billy_Bob_Thornton/\n",
            "LFW/lfw_align_112/Billy_Bob_Thornton/Billy_Bob_Thornton_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Lopez-Alegria/\n",
            "LFW/lfw_align_112/Michael_Lopez-Alegria/Michael_Lopez-Alegria_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0008.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0005.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0009.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0011.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0003.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0002.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0004.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0006.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0010.jpg\n",
            "LFW/lfw_align_112/Mike_Weir/Mike_Weir_0007.jpg\n",
            "LFW/lfw_align_112/Patsy_Kensit/\n",
            "LFW/lfw_align_112/Patsy_Kensit/Patsy_Kensit_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Holmberg/\n",
            "LFW/lfw_align_112/Peter_Holmberg/Peter_Holmberg_0001.jpg\n",
            "LFW/lfw_align_112/Pedro_Duque/\n",
            "LFW/lfw_align_112/Pedro_Duque/Pedro_Duque_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Crede/\n",
            "LFW/lfw_align_112/Joe_Crede/Joe_Crede_0001.jpg\n",
            "LFW/lfw_align_112/Andy_Lau/\n",
            "LFW/lfw_align_112/Andy_Lau/Andy_Lau_0001.jpg\n",
            "LFW/lfw_align_112/Justin_Gatlin/\n",
            "LFW/lfw_align_112/Justin_Gatlin/Justin_Gatlin_0001.jpg\n",
            "LFW/lfw_align_112/Justin_Gatlin/Justin_Gatlin_0002.jpg\n",
            "LFW/lfw_align_112/Chris_Thomas/\n",
            "LFW/lfw_align_112/Chris_Thomas/Chris_Thomas_0001.jpg\n",
            "LFW/lfw_align_112/Herb_Brooks/\n",
            "LFW/lfw_align_112/Herb_Brooks/Herb_Brooks_0001.jpg\n",
            "LFW/lfw_align_112/Raghad_Saddam_Hussein/\n",
            "LFW/lfw_align_112/Raghad_Saddam_Hussein/Raghad_Saddam_Hussein_0001.jpg\n",
            "LFW/lfw_align_112/Raghad_Saddam_Hussein/Raghad_Saddam_Hussein_0002.jpg\n",
            "LFW/lfw_align_112/Naomi_Bronstein/\n",
            "LFW/lfw_align_112/Naomi_Bronstein/Naomi_Bronstein_0001.jpg\n",
            "LFW/lfw_align_112/Oxana_Fedorova/\n",
            "LFW/lfw_align_112/Oxana_Fedorova/Oxana_Fedorova_0003.jpg\n",
            "LFW/lfw_align_112/Oxana_Fedorova/Oxana_Fedorova_0002.jpg\n",
            "LFW/lfw_align_112/Oxana_Fedorova/Oxana_Fedorova_0004.jpg\n",
            "LFW/lfw_align_112/Oxana_Fedorova/Oxana_Fedorova_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Ellison/\n",
            "LFW/lfw_align_112/Larry_Ellison/Larry_Ellison_0003.jpg\n",
            "LFW/lfw_align_112/Larry_Ellison/Larry_Ellison_0002.jpg\n",
            "LFW/lfw_align_112/Larry_Ellison/Larry_Ellison_0001.jpg\n",
            "LFW/lfw_align_112/Luis_Pujols/\n",
            "LFW/lfw_align_112/Luis_Pujols/Luis_Pujols_0001.jpg\n",
            "LFW/lfw_align_112/Nadine_Vinzens/\n",
            "LFW/lfw_align_112/Nadine_Vinzens/Nadine_Vinzens_0002.jpg\n",
            "LFW/lfw_align_112/Nadine_Vinzens/Nadine_Vinzens_0001.jpg\n",
            "LFW/lfw_align_112/Janela_Jara/\n",
            "LFW/lfw_align_112/Janela_Jara/Janela_Jara_0001.jpg\n",
            "LFW/lfw_align_112/Collis_Temple_III/\n",
            "LFW/lfw_align_112/Collis_Temple_III/Collis_Temple_III_0001.jpg\n",
            "LFW/lfw_align_112/Tammy_Lynn_Michaels/\n",
            "LFW/lfw_align_112/Tammy_Lynn_Michaels/Tammy_Lynn_Michaels_0002.jpg\n",
            "LFW/lfw_align_112/Tammy_Lynn_Michaels/Tammy_Lynn_Michaels_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Pennington/\n",
            "LFW/lfw_align_112/Richard_Pennington/Richard_Pennington_0001.jpg\n",
            "LFW/lfw_align_112/Terrence_Trammell/\n",
            "LFW/lfw_align_112/Terrence_Trammell/Terrence_Trammell_0001.jpg\n",
            "LFW/lfw_align_112/Dick_Armey/\n",
            "LFW/lfw_align_112/Dick_Armey/Dick_Armey_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Nesbitt/\n",
            "LFW/lfw_align_112/Steve_Nesbitt/Steve_Nesbitt_0001.jpg\n",
            "LFW/lfw_align_112/Kifah_Ajouri/\n",
            "LFW/lfw_align_112/Kifah_Ajouri/Kifah_Ajouri_0002.jpg\n",
            "LFW/lfw_align_112/Kifah_Ajouri/Kifah_Ajouri_0001.jpg\n",
            "LFW/lfw_align_112/Hubie_Brown/\n",
            "LFW/lfw_align_112/Hubie_Brown/Hubie_Brown_0001.jpg\n",
            "LFW/lfw_align_112/Mohammad_Hasanein/\n",
            "LFW/lfw_align_112/Mohammad_Hasanein/Mohammad_Hasanein_0001.jpg\n",
            "LFW/lfw_align_112/John_Walsh/\n",
            "LFW/lfw_align_112/John_Walsh/John_Walsh_0001.jpg\n",
            "LFW/lfw_align_112/John_Walsh/John_Walsh_0002.jpg\n",
            "LFW/lfw_align_112/Nursultan_Nazarbayev/\n",
            "LFW/lfw_align_112/Nursultan_Nazarbayev/Nursultan_Nazarbayev_0001.jpg\n",
            "LFW/lfw_align_112/Nursultan_Nazarbayev/Nursultan_Nazarbayev_0002.jpg\n",
            "LFW/lfw_align_112/Chan_Choi/\n",
            "LFW/lfw_align_112/Chan_Choi/Chan_Choi_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Rasch/\n",
            "LFW/lfw_align_112/Peter_Rasch/Peter_Rasch_0001.jpg\n",
            "LFW/lfw_align_112/Kent_Robinson/\n",
            "LFW/lfw_align_112/Kent_Robinson/Kent_Robinson_0001.jpg\n",
            "LFW/lfw_align_112/Steny_Hoyer/\n",
            "LFW/lfw_align_112/Steny_Hoyer/Steny_Hoyer_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Cravens/\n",
            "LFW/lfw_align_112/Joe_Cravens/Joe_Cravens_0001.jpg\n",
            "LFW/lfw_align_112/Takashi_Yamamoto/\n",
            "LFW/lfw_align_112/Takashi_Yamamoto/Takashi_Yamamoto_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Dreyfuss/\n",
            "LFW/lfw_align_112/Richard_Dreyfuss/Richard_Dreyfuss_0001.jpg\n",
            "LFW/lfw_align_112/Dan_Morales/\n",
            "LFW/lfw_align_112/Dan_Morales/Dan_Morales_0002.jpg\n",
            "LFW/lfw_align_112/Dan_Morales/Dan_Morales_0003.jpg\n",
            "LFW/lfw_align_112/Dan_Morales/Dan_Morales_0001.jpg\n",
            "LFW/lfw_align_112/Ramon_Delgado/\n",
            "LFW/lfw_align_112/Ramon_Delgado/Ramon_Delgado_0001.jpg\n",
            "LFW/lfw_align_112/Keith_Foulke/\n",
            "LFW/lfw_align_112/Keith_Foulke/Keith_Foulke_0001.jpg\n",
            "LFW/lfw_align_112/Mehmet_Ali_Sahin/\n",
            "LFW/lfw_align_112/Mehmet_Ali_Sahin/Mehmet_Ali_Sahin_0001.jpg\n",
            "LFW/lfw_align_112/Theo_Epstein/\n",
            "LFW/lfw_align_112/Theo_Epstein/Theo_Epstein_0001.jpg\n",
            "LFW/lfw_align_112/Theo_Epstein/Theo_Epstein_0002.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0004.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0009.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0008.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0003.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0007.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0011.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0002.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0014.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0001.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0013.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0015.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0010.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0012.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0006.jpg\n",
            "LFW/lfw_align_112/Pierce_Brosnan/Pierce_Brosnan_0005.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0004.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0011.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0007.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0010.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0008.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0009.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0003.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0006.jpg\n",
            "LFW/lfw_align_112/Richard_Gephardt/Richard_Gephardt_0005.jpg\n",
            "LFW/lfw_align_112/Walter_Woods/\n",
            "LFW/lfw_align_112/Walter_Woods/Walter_Woods_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Moss/\n",
            "LFW/lfw_align_112/Tom_Moss/Tom_Moss_0001.jpg\n",
            "LFW/lfw_align_112/Glafcos_Clerides/\n",
            "LFW/lfw_align_112/Glafcos_Clerides/Glafcos_Clerides_0001.jpg\n",
            "LFW/lfw_align_112/Glafcos_Clerides/Glafcos_Clerides_0004.jpg\n",
            "LFW/lfw_align_112/Glafcos_Clerides/Glafcos_Clerides_0003.jpg\n",
            "LFW/lfw_align_112/Glafcos_Clerides/Glafcos_Clerides_0002.jpg\n",
            "LFW/lfw_align_112/Kathy_Bates/\n",
            "LFW/lfw_align_112/Kathy_Bates/Kathy_Bates_0001.jpg\n",
            "LFW/lfw_align_112/Roberto_Canessa/\n",
            "LFW/lfw_align_112/Roberto_Canessa/Roberto_Canessa_0001.jpg\n",
            "LFW/lfw_align_112/Cathy_Chisholm/\n",
            "LFW/lfw_align_112/Cathy_Chisholm/Cathy_Chisholm_0001.jpg\n",
            "LFW/lfw_align_112/Rohman_al-Ghozi/\n",
            "LFW/lfw_align_112/Rohman_al-Ghozi/Rohman_al-Ghozi_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Lucchino/\n",
            "LFW/lfw_align_112/Larry_Lucchino/Larry_Lucchino_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Lucchino/Larry_Lucchino_0002.jpg\n",
            "LFW/lfw_align_112/Augusto_Roa_Bastos/\n",
            "LFW/lfw_align_112/Augusto_Roa_Bastos/Augusto_Roa_Bastos_0001.jpg\n",
            "LFW/lfw_align_112/Augusto_Roa_Bastos/Augusto_Roa_Bastos_0002.jpg\n",
            "LFW/lfw_align_112/Craig_Doblin/\n",
            "LFW/lfw_align_112/Craig_Doblin/Craig_Doblin_0001.jpg\n",
            "LFW/lfw_align_112/John_Belushi/\n",
            "LFW/lfw_align_112/John_Belushi/John_Belushi_0001.jpg\n",
            "LFW/lfw_align_112/Janis_Ruth_Coulter/\n",
            "LFW/lfw_align_112/Janis_Ruth_Coulter/Janis_Ruth_Coulter_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Patton/\n",
            "LFW/lfw_align_112/Paul_Patton/Paul_Patton_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Patton/Paul_Patton_0002.jpg\n",
            "LFW/lfw_align_112/Jeff_Bzdelik/\n",
            "LFW/lfw_align_112/Jeff_Bzdelik/Jeff_Bzdelik_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Curry/\n",
            "LFW/lfw_align_112/Bill_Curry/Bill_Curry_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Williams/\n",
            "LFW/lfw_align_112/Gary_Williams/Gary_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Williams/Gary_Williams_0002.jpg\n",
            "LFW/lfw_align_112/Tim_Salmon/\n",
            "LFW/lfw_align_112/Tim_Salmon/Tim_Salmon_0001.jpg\n",
            "LFW/lfw_align_112/Pablo_Latras/\n",
            "LFW/lfw_align_112/Pablo_Latras/Pablo_Latras_0001.jpg\n",
            "LFW/lfw_align_112/Brandon_Larson/\n",
            "LFW/lfw_align_112/Brandon_Larson/Brandon_Larson_0001.jpg\n",
            "LFW/lfw_align_112/Jon_Stewart/\n",
            "LFW/lfw_align_112/Jon_Stewart/Jon_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Butler/\n",
            "LFW/lfw_align_112/Bill_Butler/Bill_Butler_0001.jpg\n",
            "LFW/lfw_align_112/Mickey_Sherman/\n",
            "LFW/lfw_align_112/Mickey_Sherman/Mickey_Sherman_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Greenwood/\n",
            "LFW/lfw_align_112/Jim_Greenwood/Jim_Greenwood_0001.jpg\n",
            "LFW/lfw_align_112/Valerie_Harper/\n",
            "LFW/lfw_align_112/Valerie_Harper/Valerie_Harper_0001.jpg\n",
            "LFW/lfw_align_112/Valerie_Harper/Valerie_Harper_0002.jpg\n",
            "LFW/lfw_align_112/Brandon_Hammond/\n",
            "LFW/lfw_align_112/Brandon_Hammond/Brandon_Hammond_0001.jpg\n",
            "LFW/lfw_align_112/Rudolph_Holton/\n",
            "LFW/lfw_align_112/Rudolph_Holton/Rudolph_Holton_0001.jpg\n",
            "LFW/lfw_align_112/Zara_Akhmadova/\n",
            "LFW/lfw_align_112/Zara_Akhmadova/Zara_Akhmadova_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Polansky/\n",
            "LFW/lfw_align_112/Mark_Polansky/Mark_Polansky_0001.jpg\n",
            "LFW/lfw_align_112/Amy_Yasbeck/\n",
            "LFW/lfw_align_112/Amy_Yasbeck/Amy_Yasbeck_0001.jpg\n",
            "LFW/lfw_align_112/Naomi_Campbell/\n",
            "LFW/lfw_align_112/Naomi_Campbell/Naomi_Campbell_0001.jpg\n",
            "LFW/lfw_align_112/Naomi_Campbell/Naomi_Campbell_0002.jpg\n",
            "LFW/lfw_align_112/Emma_Watson/\n",
            "LFW/lfw_align_112/Emma_Watson/Emma_Watson_0001.jpg\n",
            "LFW/lfw_align_112/Emma_Watson/Emma_Watson_0005.jpg\n",
            "LFW/lfw_align_112/Emma_Watson/Emma_Watson_0004.jpg\n",
            "LFW/lfw_align_112/Emma_Watson/Emma_Watson_0003.jpg\n",
            "LFW/lfw_align_112/Emma_Watson/Emma_Watson_0002.jpg\n",
            "LFW/lfw_align_112/Abid_Hamid_Mahmud_Al-Tikriti/\n",
            "LFW/lfw_align_112/Abid_Hamid_Mahmud_Al-Tikriti/Abid_Hamid_Mahmud_Al-Tikriti_0002.jpg\n",
            "LFW/lfw_align_112/Abid_Hamid_Mahmud_Al-Tikriti/Abid_Hamid_Mahmud_Al-Tikriti_0003.jpg\n",
            "LFW/lfw_align_112/Abid_Hamid_Mahmud_Al-Tikriti/Abid_Hamid_Mahmud_Al-Tikriti_0001.jpg\n",
            "LFW/lfw_align_112/Daniel_Darnell/\n",
            "LFW/lfw_align_112/Daniel_Darnell/Daniel_Darnell_0001.jpg\n",
            "LFW/lfw_align_112/Marricia_Tate/\n",
            "LFW/lfw_align_112/Marricia_Tate/Marricia_Tate_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Greene/\n",
            "LFW/lfw_align_112/Larry_Greene/Larry_Greene_0001.jpg\n",
            "LFW/lfw_align_112/Naoto_Kan/\n",
            "LFW/lfw_align_112/Naoto_Kan/Naoto_Kan_0004.jpg\n",
            "LFW/lfw_align_112/Naoto_Kan/Naoto_Kan_0002.jpg\n",
            "LFW/lfw_align_112/Naoto_Kan/Naoto_Kan_0003.jpg\n",
            "LFW/lfw_align_112/Naoto_Kan/Naoto_Kan_0001.jpg\n",
            "LFW/lfw_align_112/Anne_Cavers/\n",
            "LFW/lfw_align_112/Anne_Cavers/Anne_Cavers_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Stein/\n",
            "LFW/lfw_align_112/Bill_Stein/Bill_Stein_0001.jpg\n",
            "LFW/lfw_align_112/Michele_Placido/\n",
            "LFW/lfw_align_112/Michele_Placido/Michele_Placido_0001.jpg\n",
            "LFW/lfw_align_112/Chan_Ho_Park/\n",
            "LFW/lfw_align_112/Chan_Ho_Park/Chan_Ho_Park_0001.jpg\n",
            "LFW/lfw_align_112/Qian_Qichen/\n",
            "LFW/lfw_align_112/Qian_Qichen/Qian_Qichen_0001.jpg\n",
            "LFW/lfw_align_112/Joseph_Hoy/\n",
            "LFW/lfw_align_112/Joseph_Hoy/Joseph_Hoy_0001.jpg\n",
            "LFW/lfw_align_112/Lana_Clarkson/\n",
            "LFW/lfw_align_112/Lana_Clarkson/Lana_Clarkson_0002.jpg\n",
            "LFW/lfw_align_112/Lana_Clarkson/Lana_Clarkson_0001.jpg\n",
            "LFW/lfw_align_112/Lauren_Hutton/\n",
            "LFW/lfw_align_112/Lauren_Hutton/Lauren_Hutton_0002.jpg\n",
            "LFW/lfw_align_112/Lauren_Hutton/Lauren_Hutton_0001.jpg\n",
            "LFW/lfw_align_112/Seydou_Diarra/\n",
            "LFW/lfw_align_112/Seydou_Diarra/Seydou_Diarra_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Gero/\n",
            "LFW/lfw_align_112/Gary_Gero/Gary_Gero_0001.jpg\n",
            "LFW/lfw_align_112/Nancy_Greenlaw/\n",
            "LFW/lfw_align_112/Nancy_Greenlaw/Nancy_Greenlaw_0001.jpg\n",
            "LFW/lfw_align_112/Tatsuya_Fuji/\n",
            "LFW/lfw_align_112/Tatsuya_Fuji/Tatsuya_Fuji_0001.jpg\n",
            "LFW/lfw_align_112/Matt_Braker/\n",
            "LFW/lfw_align_112/Matt_Braker/Matt_Braker_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Wilmore/\n",
            "LFW/lfw_align_112/Larry_Wilmore/Larry_Wilmore_0001.jpg\n",
            "LFW/lfw_align_112/Agnes_Bruckner/\n",
            "LFW/lfw_align_112/Agnes_Bruckner/Agnes_Bruckner_0001.jpg\n",
            "LFW/lfw_align_112/Hipolito_Mejia/\n",
            "LFW/lfw_align_112/Hipolito_Mejia/Hipolito_Mejia_0002.jpg\n",
            "LFW/lfw_align_112/Hipolito_Mejia/Hipolito_Mejia_0001.jpg\n",
            "LFW/lfw_align_112/Hipolito_Mejia/Hipolito_Mejia_0003.jpg\n",
            "LFW/lfw_align_112/Hipolito_Mejia/Hipolito_Mejia_0004.jpg\n",
            "LFW/lfw_align_112/Kyra_Sedgwick/\n",
            "LFW/lfw_align_112/Kyra_Sedgwick/Kyra_Sedgwick_0001.jpg\n",
            "LFW/lfw_align_112/Naomi_Hayashi/\n",
            "LFW/lfw_align_112/Naomi_Hayashi/Naomi_Hayashi_0001.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0004.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0003.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0005.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0007.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0002.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0001.jpg\n",
            "LFW/lfw_align_112/Holly_Hunter/Holly_Hunter_0006.jpg\n",
            "LFW/lfw_align_112/Shireen_Amir_Begum/\n",
            "LFW/lfw_align_112/Shireen_Amir_Begum/Shireen_Amir_Begum_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Nillson/\n",
            "LFW/lfw_align_112/Robert_Nillson/Robert_Nillson_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Hoi-chang/\n",
            "LFW/lfw_align_112/Lee_Hoi-chang/Lee_Hoi-chang_0003.jpg\n",
            "LFW/lfw_align_112/Lee_Hoi-chang/Lee_Hoi-chang_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Hoi-chang/Lee_Hoi-chang_0004.jpg\n",
            "LFW/lfw_align_112/Lee_Hoi-chang/Lee_Hoi-chang_0002.jpg\n",
            "LFW/lfw_align_112/Junichi_Inamoto/\n",
            "LFW/lfw_align_112/Junichi_Inamoto/Junichi_Inamoto_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Bouchard/\n",
            "LFW/lfw_align_112/Michael_Bouchard/Michael_Bouchard_0001.jpg\n",
            "LFW/lfw_align_112/Laura_Flessel/\n",
            "LFW/lfw_align_112/Laura_Flessel/Laura_Flessel_0001.jpg\n",
            "LFW/lfw_align_112/Natalie_Stewart/\n",
            "LFW/lfw_align_112/Natalie_Stewart/Natalie_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Brendan_Stai/\n",
            "LFW/lfw_align_112/Brendan_Stai/Brendan_Stai_0001.jpg\n",
            "LFW/lfw_align_112/Gabriel_Jorge_Ferreia/\n",
            "LFW/lfw_align_112/Gabriel_Jorge_Ferreia/Gabriel_Jorge_Ferreia_0001.jpg\n",
            "LFW/lfw_align_112/David_Caruso/\n",
            "LFW/lfw_align_112/David_Caruso/David_Caruso_0003.jpg\n",
            "LFW/lfw_align_112/David_Caruso/David_Caruso_0002.jpg\n",
            "LFW/lfw_align_112/David_Caruso/David_Caruso_0001.jpg\n",
            "LFW/lfw_align_112/Sila_Calderon/\n",
            "LFW/lfw_align_112/Sila_Calderon/Sila_Calderon_0004.jpg\n",
            "LFW/lfw_align_112/Sila_Calderon/Sila_Calderon_0001.jpg\n",
            "LFW/lfw_align_112/Sila_Calderon/Sila_Calderon_0003.jpg\n",
            "LFW/lfw_align_112/Sila_Calderon/Sila_Calderon_0002.jpg\n",
            "LFW/lfw_align_112/Mae_Jemison/\n",
            "LFW/lfw_align_112/Mae_Jemison/Mae_Jemison_0001.jpg\n",
            "LFW/lfw_align_112/Tony_Bennett/\n",
            "LFW/lfw_align_112/Tony_Bennett/Tony_Bennett_0003.jpg\n",
            "LFW/lfw_align_112/Tony_Bennett/Tony_Bennett_0001.jpg\n",
            "LFW/lfw_align_112/Tony_Bennett/Tony_Bennett_0002.jpg\n",
            "LFW/lfw_align_112/Tony_Bennett/Tony_Bennett_0004.jpg\n",
            "LFW/lfw_align_112/Alex_Wallau/\n",
            "LFW/lfw_align_112/Alex_Wallau/Alex_Wallau_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Case/\n",
            "LFW/lfw_align_112/Steve_Case/Steve_Case_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Hollingworth/\n",
            "LFW/lfw_align_112/Peter_Hollingworth/Peter_Hollingworth_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Floyd/\n",
            "LFW/lfw_align_112/Tim_Floyd/Tim_Floyd_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Floyd/Tim_Floyd_0002.jpg\n",
            "LFW/lfw_align_112/Bruce_Lunsford/\n",
            "LFW/lfw_align_112/Bruce_Lunsford/Bruce_Lunsford_0001.jpg\n",
            "LFW/lfw_align_112/Glenn_Rivers/\n",
            "LFW/lfw_align_112/Glenn_Rivers/Glenn_Rivers_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Hsieh/\n",
            "LFW/lfw_align_112/Frank_Hsieh/Frank_Hsieh_0001.jpg\n",
            "LFW/lfw_align_112/Iran_Brown/\n",
            "LFW/lfw_align_112/Iran_Brown/Iran_Brown_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Fisher/\n",
            "LFW/lfw_align_112/Peter_Fisher/Peter_Fisher_0001.jpg\n",
            "LFW/lfw_align_112/Francesco_Totti/\n",
            "LFW/lfw_align_112/Francesco_Totti/Francesco_Totti_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Tubb/\n",
            "LFW/lfw_align_112/Richard_Tubb/Richard_Tubb_0001.jpg\n",
            "LFW/lfw_align_112/Perri_Shaw/\n",
            "LFW/lfw_align_112/Perri_Shaw/Perri_Shaw_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Coogan/\n",
            "LFW/lfw_align_112/Steve_Coogan/Steve_Coogan_0001.jpg\n",
            "LFW/lfw_align_112/Michelle_Bachelet/\n",
            "LFW/lfw_align_112/Michelle_Bachelet/Michelle_Bachelet_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Wollnough/\n",
            "LFW/lfw_align_112/Paul_Wollnough/Paul_Wollnough_0001.jpg\n",
            "LFW/lfw_align_112/Eminem/\n",
            "LFW/lfw_align_112/Eminem/Eminem_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Hong-ki/\n",
            "LFW/lfw_align_112/Lee_Hong-ki/Lee_Hong-ki_0001.jpg\n",
            "LFW/lfw_align_112/Ed_Rendell/\n",
            "LFW/lfw_align_112/Ed_Rendell/Ed_Rendell_0001.jpg\n",
            "LFW/lfw_align_112/Pat_Burns/\n",
            "LFW/lfw_align_112/Pat_Burns/Pat_Burns_0001.jpg\n",
            "LFW/lfw_align_112/Pat_Burns/Pat_Burns_0002.jpg\n",
            "LFW/lfw_align_112/Saadi_Gadhafi/\n",
            "LFW/lfw_align_112/Saadi_Gadhafi/Saadi_Gadhafi_0001.jpg\n",
            "LFW/lfw_align_112/Holly_Robinson_Peete/\n",
            "LFW/lfw_align_112/Holly_Robinson_Peete/Holly_Robinson_Peete_0001.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0066.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0034.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0010.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0025.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0021.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0033.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0047.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0020.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0002.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0001.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0056.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0012.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0004.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0017.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0039.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0024.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0049.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0005.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0022.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0067.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0062.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0011.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0061.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0070.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0052.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0040.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0058.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0013.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0009.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0003.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0059.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0060.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0053.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0030.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0019.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0027.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0007.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0068.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0043.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0018.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0048.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0014.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0044.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0071.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0046.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0006.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0028.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0031.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0015.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0008.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0035.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0016.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0041.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0057.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0045.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0055.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0036.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0063.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0023.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0054.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0051.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0064.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0050.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0042.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0038.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0065.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0029.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0069.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0026.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0037.jpg\n",
            "LFW/lfw_align_112/Hugo_Chavez/Hugo_Chavez_0032.jpg\n",
            "LFW/lfw_align_112/Ann_Godbehere/\n",
            "LFW/lfw_align_112/Ann_Godbehere/Ann_Godbehere_0001.jpg\n",
            "LFW/lfw_align_112/Margie_Puente/\n",
            "LFW/lfw_align_112/Margie_Puente/Margie_Puente_0001.jpg\n",
            "LFW/lfw_align_112/Hermann_Maier/\n",
            "LFW/lfw_align_112/Hermann_Maier/Hermann_Maier_0001.jpg\n",
            "LFW/lfw_align_112/Hermann_Maier/Hermann_Maier_0002.jpg\n",
            "LFW/lfw_align_112/Joseph_Deiss/\n",
            "LFW/lfw_align_112/Joseph_Deiss/Joseph_Deiss_0003.jpg\n",
            "LFW/lfw_align_112/Joseph_Deiss/Joseph_Deiss_0001.jpg\n",
            "LFW/lfw_align_112/Joseph_Deiss/Joseph_Deiss_0002.jpg\n",
            "LFW/lfw_align_112/Tom_Osborne/\n",
            "LFW/lfw_align_112/Tom_Osborne/Tom_Osborne_0001.jpg\n",
            "LFW/lfw_align_112/Eva_Herzigova/\n",
            "LFW/lfw_align_112/Eva_Herzigova/Eva_Herzigova_0001.jpg\n",
            "LFW/lfw_align_112/Maureen_Kanka/\n",
            "LFW/lfw_align_112/Maureen_Kanka/Maureen_Kanka_0001.jpg\n",
            "LFW/lfw_align_112/Rogelio_Montemayor/\n",
            "LFW/lfw_align_112/Rogelio_Montemayor/Rogelio_Montemayor_0001.jpg\n",
            "LFW/lfw_align_112/Orlando_Bloom/\n",
            "LFW/lfw_align_112/Orlando_Bloom/Orlando_Bloom_0002.jpg\n",
            "LFW/lfw_align_112/Orlando_Bloom/Orlando_Bloom_0003.jpg\n",
            "LFW/lfw_align_112/Orlando_Bloom/Orlando_Bloom_0001.jpg\n",
            "LFW/lfw_align_112/Janet_Chandler/\n",
            "LFW/lfw_align_112/Janet_Chandler/Janet_Chandler_0001.jpg\n",
            "LFW/lfw_align_112/Isabel_Orellana/\n",
            "LFW/lfw_align_112/Isabel_Orellana/Isabel_Orellana_0001.jpg\n",
            "LFW/lfw_align_112/Jennie_Finch/\n",
            "LFW/lfw_align_112/Jennie_Finch/Jennie_Finch_0001.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0002.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0001.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0005.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0004.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0006.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0003.jpg\n",
            "LFW/lfw_align_112/Jon_Gruden/Jon_Gruden_0007.jpg\n",
            "LFW/lfw_align_112/Ozzie_Smith/\n",
            "LFW/lfw_align_112/Ozzie_Smith/Ozzie_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Crippen/\n",
            "LFW/lfw_align_112/Bob_Crippen/Bob_Crippen_0001.jpg\n",
            "LFW/lfw_align_112/Lucio_Angulo/\n",
            "LFW/lfw_align_112/Lucio_Angulo/Lucio_Angulo_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Tracy/\n",
            "LFW/lfw_align_112/Paul_Tracy/Paul_Tracy_0001.jpg\n",
            "LFW/lfw_align_112/Orrin_Hatch/\n",
            "LFW/lfw_align_112/Orrin_Hatch/Orrin_Hatch_0002.jpg\n",
            "LFW/lfw_align_112/Orrin_Hatch/Orrin_Hatch_0001.jpg\n",
            "LFW/lfw_align_112/Gwendal_Peizerat/\n",
            "LFW/lfw_align_112/Gwendal_Peizerat/Gwendal_Peizerat_0001.jpg\n",
            "LFW/lfw_align_112/Gwendal_Peizerat/Gwendal_Peizerat_0003.jpg\n",
            "LFW/lfw_align_112/Gwendal_Peizerat/Gwendal_Peizerat_0002.jpg\n",
            "LFW/lfw_align_112/Scott_Ritter/\n",
            "LFW/lfw_align_112/Scott_Ritter/Scott_Ritter_0002.jpg\n",
            "LFW/lfw_align_112/Scott_Ritter/Scott_Ritter_0001.jpg\n",
            "LFW/lfw_align_112/Georgia_Giddings/\n",
            "LFW/lfw_align_112/Georgia_Giddings/Georgia_Giddings_0001.jpg\n",
            "LFW/lfw_align_112/Suzanne_Fox/\n",
            "LFW/lfw_align_112/Suzanne_Fox/Suzanne_Fox_0001.jpg\n",
            "LFW/lfw_align_112/Kjell_Magne_Bondevik/\n",
            "LFW/lfw_align_112/Kjell_Magne_Bondevik/Kjell_Magne_Bondevik_0001.jpg\n",
            "LFW/lfw_align_112/Kjell_Magne_Bondevik/Kjell_Magne_Bondevik_0003.jpg\n",
            "LFW/lfw_align_112/Kjell_Magne_Bondevik/Kjell_Magne_Bondevik_0002.jpg\n",
            "LFW/lfw_align_112/Jesse_James/\n",
            "LFW/lfw_align_112/Jesse_James/Jesse_James_0001.jpg\n",
            "LFW/lfw_align_112/Bruce_Weber/\n",
            "LFW/lfw_align_112/Bruce_Weber/Bruce_Weber_0002.jpg\n",
            "LFW/lfw_align_112/Bruce_Weber/Bruce_Weber_0001.jpg\n",
            "LFW/lfw_align_112/Martha_Beatriz_Roque/\n",
            "LFW/lfw_align_112/Martha_Beatriz_Roque/Martha_Beatriz_Roque_0001.jpg\n",
            "LFW/lfw_align_112/Martha_Beatriz_Roque/Martha_Beatriz_Roque_0002.jpg\n",
            "LFW/lfw_align_112/Donna_Shalala/\n",
            "LFW/lfw_align_112/Donna_Shalala/Donna_Shalala_0002.jpg\n",
            "LFW/lfw_align_112/Donna_Shalala/Donna_Shalala_0001.jpg\n",
            "LFW/lfw_align_112/Lisa_Girman/\n",
            "LFW/lfw_align_112/Lisa_Girman/Lisa_Girman_0001.jpg\n",
            "LFW/lfw_align_112/Mario_Kreutzberger/\n",
            "LFW/lfw_align_112/Mario_Kreutzberger/Mario_Kreutzberger_0001.jpg\n",
            "LFW/lfw_align_112/Mario_Kreutzberger/Mario_Kreutzberger_0002.jpg\n",
            "LFW/lfw_align_112/Mario_Lemieux/\n",
            "LFW/lfw_align_112/Mario_Lemieux/Mario_Lemieux_0001.jpg\n",
            "LFW/lfw_align_112/Alex_Gonzalez/\n",
            "LFW/lfw_align_112/Alex_Gonzalez/Alex_Gonzalez_0001.jpg\n",
            "LFW/lfw_align_112/Mikhail_Kasyanov/\n",
            "LFW/lfw_align_112/Mikhail_Kasyanov/Mikhail_Kasyanov_0002.jpg\n",
            "LFW/lfw_align_112/Mikhail_Kasyanov/Mikhail_Kasyanov_0003.jpg\n",
            "LFW/lfw_align_112/Mikhail_Kasyanov/Mikhail_Kasyanov_0001.jpg\n",
            "LFW/lfw_align_112/Mikhail_Kasyanov/Mikhail_Kasyanov_0004.jpg\n",
            "LFW/lfw_align_112/Keira_Knightley/\n",
            "LFW/lfw_align_112/Keira_Knightley/Keira_Knightley_0001.jpg\n",
            "LFW/lfw_align_112/Keira_Knightley/Keira_Knightley_0002.jpg\n",
            "LFW/lfw_align_112/Alicia_Silverstone/\n",
            "LFW/lfw_align_112/Alicia_Silverstone/Alicia_Silverstone_0001.jpg\n",
            "LFW/lfw_align_112/Alicia_Silverstone/Alicia_Silverstone_0002.jpg\n",
            "LFW/lfw_align_112/Tom_Coughlin/\n",
            "LFW/lfw_align_112/Tom_Coughlin/Tom_Coughlin_0001.jpg\n",
            "LFW/lfw_align_112/Francis_George/\n",
            "LFW/lfw_align_112/Francis_George/Francis_George_0002.jpg\n",
            "LFW/lfw_align_112/Francis_George/Francis_George_0001.jpg\n",
            "LFW/lfw_align_112/Chang_Dae-whan/\n",
            "LFW/lfw_align_112/Chang_Dae-whan/Chang_Dae-whan_0002.jpg\n",
            "LFW/lfw_align_112/Chang_Dae-whan/Chang_Dae-whan_0001.jpg\n",
            "LFW/lfw_align_112/Toby_Keith/\n",
            "LFW/lfw_align_112/Toby_Keith/Toby_Keith_0001.jpg\n",
            "LFW/lfw_align_112/Kevin_Spacey/\n",
            "LFW/lfw_align_112/Kevin_Spacey/Kevin_Spacey_0005.jpg\n",
            "LFW/lfw_align_112/Kevin_Spacey/Kevin_Spacey_0003.jpg\n",
            "LFW/lfw_align_112/Kevin_Spacey/Kevin_Spacey_0001.jpg\n",
            "LFW/lfw_align_112/Kevin_Spacey/Kevin_Spacey_0002.jpg\n",
            "LFW/lfw_align_112/Kevin_Spacey/Kevin_Spacey_0004.jpg\n",
            "LFW/lfw_align_112/Olene_Walker/\n",
            "LFW/lfw_align_112/Olene_Walker/Olene_Walker_0001.jpg\n",
            "LFW/lfw_align_112/Tracee_Treadwell/\n",
            "LFW/lfw_align_112/Tracee_Treadwell/Tracee_Treadwell_0001.jpg\n",
            "LFW/lfw_align_112/Pierre_Lacroix/\n",
            "LFW/lfw_align_112/Pierre_Lacroix/Pierre_Lacroix_0001.jpg\n",
            "LFW/lfw_align_112/Al_Davis/\n",
            "LFW/lfw_align_112/Al_Davis/Al_Davis_0001.jpg\n",
            "LFW/lfw_align_112/Al_Davis/Al_Davis_0002.jpg\n",
            "LFW/lfw_align_112/Brooke_Gordon/\n",
            "LFW/lfw_align_112/Brooke_Gordon/Brooke_Gordon_0001.jpg\n",
            "LFW/lfw_align_112/Tony_Clement/\n",
            "LFW/lfw_align_112/Tony_Clement/Tony_Clement_0001.jpg\n",
            "LFW/lfw_align_112/Kwame_Kilpatrick/\n",
            "LFW/lfw_align_112/Kwame_Kilpatrick/Kwame_Kilpatrick_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Killeen/\n",
            "LFW/lfw_align_112/Michael_Killeen/Michael_Killeen_0001.jpg\n",
            "LFW/lfw_align_112/Stephanie_Cohen_Aloro/\n",
            "LFW/lfw_align_112/Stephanie_Cohen_Aloro/Stephanie_Cohen_Aloro_0001.jpg\n",
            "LFW/lfw_align_112/Wally_Szczerbiak/\n",
            "LFW/lfw_align_112/Wally_Szczerbiak/Wally_Szczerbiak_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Mueller/\n",
            "LFW/lfw_align_112/Robert_Mueller/Robert_Mueller_0002.jpg\n",
            "LFW/lfw_align_112/Robert_Mueller/Robert_Mueller_0003.jpg\n",
            "LFW/lfw_align_112/Robert_Mueller/Robert_Mueller_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Mueller/Robert_Mueller_0004.jpg\n",
            "LFW/lfw_align_112/Robert_Mueller/Robert_Mueller_0005.jpg\n",
            "LFW/lfw_align_112/Mohammed_Salmane/\n",
            "LFW/lfw_align_112/Mohammed_Salmane/Mohammed_Salmane_0001.jpg\n",
            "LFW/lfw_align_112/Ralph_Firman/\n",
            "LFW/lfw_align_112/Ralph_Firman/Ralph_Firman_0001.jpg\n",
            "LFW/lfw_align_112/Ralph_Firman/Ralph_Firman_0002.jpg\n",
            "LFW/lfw_align_112/Frank_Solich/\n",
            "LFW/lfw_align_112/Frank_Solich/Frank_Solich_0002.jpg\n",
            "LFW/lfw_align_112/Frank_Solich/Frank_Solich_0004.jpg\n",
            "LFW/lfw_align_112/Frank_Solich/Frank_Solich_0003.jpg\n",
            "LFW/lfw_align_112/Frank_Solich/Frank_Solich_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Solich/Frank_Solich_0005.jpg\n",
            "LFW/lfw_align_112/Colleen_Ryan/\n",
            "LFW/lfw_align_112/Colleen_Ryan/Colleen_Ryan_0001.jpg\n",
            "LFW/lfw_align_112/Ian_McKellen/\n",
            "LFW/lfw_align_112/Ian_McKellen/Ian_McKellen_0001.jpg\n",
            "LFW/lfw_align_112/Ian_McKellen/Ian_McKellen_0003.jpg\n",
            "LFW/lfw_align_112/Ian_McKellen/Ian_McKellen_0002.jpg\n",
            "LFW/lfw_align_112/Doug_Christie/\n",
            "LFW/lfw_align_112/Doug_Christie/Doug_Christie_0001.jpg\n",
            "LFW/lfw_align_112/LeBron_James/\n",
            "LFW/lfw_align_112/LeBron_James/LeBron_James_0005.jpg\n",
            "LFW/lfw_align_112/LeBron_James/LeBron_James_0001.jpg\n",
            "LFW/lfw_align_112/LeBron_James/LeBron_James_0002.jpg\n",
            "LFW/lfw_align_112/LeBron_James/LeBron_James_0003.jpg\n",
            "LFW/lfw_align_112/LeBron_James/LeBron_James_0004.jpg\n",
            "LFW/lfw_align_112/Edward_Albee/\n",
            "LFW/lfw_align_112/Edward_Albee/Edward_Albee_0001.jpg\n",
            "LFW/lfw_align_112/Cuba_Gooding_Jr/\n",
            "LFW/lfw_align_112/Cuba_Gooding_Jr/Cuba_Gooding_Jr_0001.jpg\n",
            "LFW/lfw_align_112/Dave_Tucker/\n",
            "LFW/lfw_align_112/Dave_Tucker/Dave_Tucker_0001.jpg\n",
            "LFW/lfw_align_112/Gloria_Allred/\n",
            "LFW/lfw_align_112/Gloria_Allred/Gloria_Allred_0002.jpg\n",
            "LFW/lfw_align_112/Gloria_Allred/Gloria_Allred_0001.jpg\n",
            "LFW/lfw_align_112/Cherie_Blair/\n",
            "LFW/lfw_align_112/Cherie_Blair/Cherie_Blair_0001.jpg\n",
            "LFW/lfw_align_112/Cherie_Blair/Cherie_Blair_0003.jpg\n",
            "LFW/lfw_align_112/Cherie_Blair/Cherie_Blair_0002.jpg\n",
            "LFW/lfw_align_112/Cherie_Blair/Cherie_Blair_0004.jpg\n",
            "LFW/lfw_align_112/Robert_Horan/\n",
            "LFW/lfw_align_112/Robert_Horan/Robert_Horan_0002.jpg\n",
            "LFW/lfw_align_112/Robert_Horan/Robert_Horan_0001.jpg\n",
            "LFW/lfw_align_112/James_Hallock/\n",
            "LFW/lfw_align_112/James_Hallock/James_Hallock_0001.jpg\n",
            "LFW/lfw_align_112/Meg_Wakeman/\n",
            "LFW/lfw_align_112/Meg_Wakeman/Meg_Wakeman_0001.jpg\n",
            "LFW/lfw_align_112/Shane_Hmiel/\n",
            "LFW/lfw_align_112/Shane_Hmiel/Shane_Hmiel_0001.jpg\n",
            "LFW/lfw_align_112/Marion_Barry/\n",
            "LFW/lfw_align_112/Marion_Barry/Marion_Barry_0001.jpg\n",
            "LFW/lfw_align_112/Howard_Ross/\n",
            "LFW/lfw_align_112/Howard_Ross/Howard_Ross_0001.jpg\n",
            "LFW/lfw_align_112/Richie_Adubato/\n",
            "LFW/lfw_align_112/Richie_Adubato/Richie_Adubato_0002.jpg\n",
            "LFW/lfw_align_112/Richie_Adubato/Richie_Adubato_0001.jpg\n",
            "LFW/lfw_align_112/Halbert_Fillinger/\n",
            "LFW/lfw_align_112/Halbert_Fillinger/Halbert_Fillinger_0001.jpg\n",
            "LFW/lfw_align_112/Roger_Cook/\n",
            "LFW/lfw_align_112/Roger_Cook/Roger_Cook_0001.jpg\n",
            "LFW/lfw_align_112/Lily_Tomlin/\n",
            "LFW/lfw_align_112/Lily_Tomlin/Lily_Tomlin_0001.jpg\n",
            "LFW/lfw_align_112/Lily_Tomlin/Lily_Tomlin_0002.jpg\n",
            "LFW/lfw_align_112/Chip_Burrus/\n",
            "LFW/lfw_align_112/Chip_Burrus/Chip_Burrus_0001.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0007.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0003.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0005.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0001.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0013.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0008.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0002.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0011.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0010.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0004.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0009.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0006.jpg\n",
            "LFW/lfw_align_112/Salma_Hayek/Salma_Hayek_0012.jpg\n",
            "LFW/lfw_align_112/Mathilda_Karel_Spak/\n",
            "LFW/lfw_align_112/Mathilda_Karel_Spak/Mathilda_Karel_Spak_0001.jpg\n",
            "LFW/lfw_align_112/Carla_Tricoli/\n",
            "LFW/lfw_align_112/Carla_Tricoli/Carla_Tricoli_0001.jpg\n",
            "LFW/lfw_align_112/John_McEnroe/\n",
            "LFW/lfw_align_112/John_McEnroe/John_McEnroe_0002.jpg\n",
            "LFW/lfw_align_112/John_McEnroe/John_McEnroe_0001.jpg\n",
            "LFW/lfw_align_112/Paulina_Rodriguez_Davila/\n",
            "LFW/lfw_align_112/Paulina_Rodriguez_Davila/Paulina_Rodriguez_Davila_0001.jpg\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/Eduard_Shevardnadze_0003.jpg\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/Eduard_Shevardnadze_0004.jpg\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/Eduard_Shevardnadze_0005.jpg\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/Eduard_Shevardnadze_0001.jpg\n",
            "LFW/lfw_align_112/Eduard_Shevardnadze/Eduard_Shevardnadze_0002.jpg\n",
            "LFW/lfw_align_112/Lin_Yi-fu/\n",
            "LFW/lfw_align_112/Lin_Yi-fu/Lin_Yi-fu_0001.jpg\n",
            "LFW/lfw_align_112/Mario_Vasquez_Rana/\n",
            "LFW/lfw_align_112/Mario_Vasquez_Rana/Mario_Vasquez_Rana_0001.jpg\n",
            "LFW/lfw_align_112/Koji_Uehara/\n",
            "LFW/lfw_align_112/Koji_Uehara/Koji_Uehara_0001.jpg\n",
            "LFW/lfw_align_112/Cristina_Saralegui/\n",
            "LFW/lfw_align_112/Cristina_Saralegui/Cristina_Saralegui_0002.jpg\n",
            "LFW/lfw_align_112/Cristina_Saralegui/Cristina_Saralegui_0001.jpg\n",
            "LFW/lfw_align_112/Don_Lake/\n",
            "LFW/lfw_align_112/Don_Lake/Don_Lake_0001.jpg\n",
            "LFW/lfw_align_112/Zurab_Tsereteli/\n",
            "LFW/lfw_align_112/Zurab_Tsereteli/Zurab_Tsereteli_0001.jpg\n",
            "LFW/lfw_align_112/Clifford_Etienne/\n",
            "LFW/lfw_align_112/Clifford_Etienne/Clifford_Etienne_0001.jpg\n",
            "LFW/lfw_align_112/Brad_Alexander_Smith/\n",
            "LFW/lfw_align_112/Brad_Alexander_Smith/Brad_Alexander_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Marc-Andre_Fleury/\n",
            "LFW/lfw_align_112/Marc-Andre_Fleury/Marc-Andre_Fleury_0001.jpg\n",
            "LFW/lfw_align_112/Marc-Andre_Fleury/Marc-Andre_Fleury_0002.jpg\n",
            "LFW/lfw_align_112/Anne_ONeil/\n",
            "LFW/lfw_align_112/Anne_ONeil/Anne_ONeil_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Wilson/\n",
            "LFW/lfw_align_112/Paul_Wilson/Paul_Wilson_0001.jpg\n",
            "LFW/lfw_align_112/Craig_OClair/\n",
            "LFW/lfw_align_112/Craig_OClair/Craig_OClair_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Peterson/\n",
            "LFW/lfw_align_112/Scott_Peterson/Scott_Peterson_0005.jpg\n",
            "LFW/lfw_align_112/Scott_Peterson/Scott_Peterson_0004.jpg\n",
            "LFW/lfw_align_112/Scott_Peterson/Scott_Peterson_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Peterson/Scott_Peterson_0003.jpg\n",
            "LFW/lfw_align_112/Scott_Peterson/Scott_Peterson_0002.jpg\n",
            "LFW/lfw_align_112/Kristin_Chenoweth/\n",
            "LFW/lfw_align_112/Kristin_Chenoweth/Kristin_Chenoweth_0001.jpg\n",
            "LFW/lfw_align_112/Roman_Abramovich/\n",
            "LFW/lfw_align_112/Roman_Abramovich/Roman_Abramovich_0001.jpg\n",
            "LFW/lfw_align_112/Thomas_Ferguson/\n",
            "LFW/lfw_align_112/Thomas_Ferguson/Thomas_Ferguson_0001.jpg\n",
            "LFW/lfw_align_112/Gordon_Lightfoot/\n",
            "LFW/lfw_align_112/Gordon_Lightfoot/Gordon_Lightfoot_0001.jpg\n",
            "LFW/lfw_align_112/Todd_Haynes/\n",
            "LFW/lfw_align_112/Todd_Haynes/Todd_Haynes_0001.jpg\n",
            "LFW/lfw_align_112/Todd_Haynes/Todd_Haynes_0002.jpg\n",
            "LFW/lfw_align_112/Todd_Haynes/Todd_Haynes_0003.jpg\n",
            "LFW/lfw_align_112/Todd_Haynes/Todd_Haynes_0004.jpg\n",
            "LFW/lfw_align_112/Andre_Smith/\n",
            "LFW/lfw_align_112/Andre_Smith/Andre_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Alanna_Ubach/\n",
            "LFW/lfw_align_112/Alanna_Ubach/Alanna_Ubach_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Stack/\n",
            "LFW/lfw_align_112/Robert_Stack/Robert_Stack_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Stack/Robert_Stack_0002.jpg\n",
            "LFW/lfw_align_112/Chakib_Khelil/\n",
            "LFW/lfw_align_112/Chakib_Khelil/Chakib_Khelil_0002.jpg\n",
            "LFW/lfw_align_112/Chakib_Khelil/Chakib_Khelil_0001.jpg\n",
            "LFW/lfw_align_112/Tyron_Garner/\n",
            "LFW/lfw_align_112/Tyron_Garner/Tyron_Garner_0002.jpg\n",
            "LFW/lfw_align_112/Tyron_Garner/Tyron_Garner_0001.jpg\n",
            "LFW/lfw_align_112/Nick_Reilly/\n",
            "LFW/lfw_align_112/Nick_Reilly/Nick_Reilly_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Petrino/\n",
            "LFW/lfw_align_112/Bob_Petrino/Bob_Petrino_0001.jpg\n",
            "LFW/lfw_align_112/Lisa_Marie_Presley/\n",
            "LFW/lfw_align_112/Lisa_Marie_Presley/Lisa_Marie_Presley_0003.jpg\n",
            "LFW/lfw_align_112/Lisa_Marie_Presley/Lisa_Marie_Presley_0002.jpg\n",
            "LFW/lfw_align_112/Lisa_Marie_Presley/Lisa_Marie_Presley_0001.jpg\n",
            "LFW/lfw_align_112/Lisa_Marie_Presley/Lisa_Marie_Presley_0004.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0019.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0010.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0009.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0018.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0017.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0001.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0005.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0013.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0002.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0004.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0003.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0015.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0006.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0011.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0012.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0007.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0021.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0020.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0014.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0016.jpg\n",
            "LFW/lfw_align_112/Amelie_Mauresmo/Amelie_Mauresmo_0008.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Trichet/\n",
            "LFW/lfw_align_112/Jean-Claude_Trichet/Jean-Claude_Trichet_0002.jpg\n",
            "LFW/lfw_align_112/Jean-Claude_Trichet/Jean-Claude_Trichet_0001.jpg\n",
            "LFW/lfw_align_112/Ilan_Goldfajn/\n",
            "LFW/lfw_align_112/Ilan_Goldfajn/Ilan_Goldfajn_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Duncan/\n",
            "LFW/lfw_align_112/Tim_Duncan/Tim_Duncan_0002.jpg\n",
            "LFW/lfw_align_112/Tim_Duncan/Tim_Duncan_0004.jpg\n",
            "LFW/lfw_align_112/Tim_Duncan/Tim_Duncan_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Duncan/Tim_Duncan_0003.jpg\n",
            "LFW/lfw_align_112/Norio_Ohga/\n",
            "LFW/lfw_align_112/Norio_Ohga/Norio_Ohga_0001.jpg\n",
            "LFW/lfw_align_112/Lou_Lang/\n",
            "LFW/lfw_align_112/Lou_Lang/Lou_Lang_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Blake/\n",
            "LFW/lfw_align_112/Steve_Blake/Steve_Blake_0001.jpg\n",
            "LFW/lfw_align_112/Annette_Bening/\n",
            "LFW/lfw_align_112/Annette_Bening/Annette_Bening_0002.jpg\n",
            "LFW/lfw_align_112/Annette_Bening/Annette_Bening_0001.jpg\n",
            "LFW/lfw_align_112/Colin_Prescot/\n",
            "LFW/lfw_align_112/Colin_Prescot/Colin_Prescot_0001.jpg\n",
            "LFW/lfw_align_112/Gerard_de_Cortanze/\n",
            "LFW/lfw_align_112/Gerard_de_Cortanze/Gerard_de_Cortanze_0001.jpg\n",
            "LFW/lfw_align_112/Chuck_Eidson/\n",
            "LFW/lfw_align_112/Chuck_Eidson/Chuck_Eidson_0001.jpg\n",
            "LFW/lfw_align_112/Liane_Janda/\n",
            "LFW/lfw_align_112/Liane_Janda/Liane_Janda_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Kirby/\n",
            "LFW/lfw_align_112/Michael_Kirby/Michael_Kirby_0001.jpg\n",
            "LFW/lfw_align_112/Audrey_Sauret/\n",
            "LFW/lfw_align_112/Audrey_Sauret/Audrey_Sauret_0001.jpg\n",
            "LFW/lfw_align_112/Carla_Gugino/\n",
            "LFW/lfw_align_112/Carla_Gugino/Carla_Gugino_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Boyce/\n",
            "LFW/lfw_align_112/Michael_Boyce/Michael_Boyce_0001.jpg\n",
            "LFW/lfw_align_112/Patricia_Heaton/\n",
            "LFW/lfw_align_112/Patricia_Heaton/Patricia_Heaton_0002.jpg\n",
            "LFW/lfw_align_112/Patricia_Heaton/Patricia_Heaton_0001.jpg\n",
            "LFW/lfw_align_112/Baz_Luhrmann/\n",
            "LFW/lfw_align_112/Baz_Luhrmann/Baz_Luhrmann_0001.jpg\n",
            "LFW/lfw_align_112/Gabrielle_Union/\n",
            "LFW/lfw_align_112/Gabrielle_Union/Gabrielle_Union_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Marshall/\n",
            "LFW/lfw_align_112/Gary_Marshall/Gary_Marshall_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Beattie/\n",
            "LFW/lfw_align_112/Jim_Beattie/Jim_Beattie_0001.jpg\n",
            "LFW/lfw_align_112/Dean_Jacek/\n",
            "LFW/lfw_align_112/Dean_Jacek/Dean_Jacek_0001.jpg\n",
            "LFW/lfw_align_112/Samantha_Daniels/\n",
            "LFW/lfw_align_112/Samantha_Daniels/Samantha_Daniels_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Iger/\n",
            "LFW/lfw_align_112/Bob_Iger/Bob_Iger_0001.jpg\n",
            "LFW/lfw_align_112/Jane_Russell/\n",
            "LFW/lfw_align_112/Jane_Russell/Jane_Russell_0001.jpg\n",
            "LFW/lfw_align_112/Linda_Ham/\n",
            "LFW/lfw_align_112/Linda_Ham/Linda_Ham_0001.jpg\n",
            "LFW/lfw_align_112/Thor_Pedersen/\n",
            "LFW/lfw_align_112/Thor_Pedersen/Thor_Pedersen_0001.jpg\n",
            "LFW/lfw_align_112/Janice_Abreu/\n",
            "LFW/lfw_align_112/Janice_Abreu/Janice_Abreu_0001.jpg\n",
            "LFW/lfw_align_112/Patricia_Russo/\n",
            "LFW/lfw_align_112/Patricia_Russo/Patricia_Russo_0001.jpg\n",
            "LFW/lfw_align_112/Davey_Johnson/\n",
            "LFW/lfw_align_112/Davey_Johnson/Davey_Johnson_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0010.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0006.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0003.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0009.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0005.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0008.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0004.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0002.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0007.jpg\n",
            "LFW/lfw_align_112/Kim_Ryong-sung/Kim_Ryong-sung_0011.jpg\n",
            "LFW/lfw_align_112/Dana_Vollmer/\n",
            "LFW/lfw_align_112/Dana_Vollmer/Dana_Vollmer_0001.jpg\n",
            "LFW/lfw_align_112/Amy_Cotton/\n",
            "LFW/lfw_align_112/Amy_Cotton/Amy_Cotton_0001.jpg\n",
            "LFW/lfw_align_112/Iain_Duncan_Smith/\n",
            "LFW/lfw_align_112/Iain_Duncan_Smith/Iain_Duncan_Smith_0004.jpg\n",
            "LFW/lfw_align_112/Iain_Duncan_Smith/Iain_Duncan_Smith_0003.jpg\n",
            "LFW/lfw_align_112/Iain_Duncan_Smith/Iain_Duncan_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Iain_Duncan_Smith/Iain_Duncan_Smith_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Jefferson/\n",
            "LFW/lfw_align_112/Richard_Jefferson/Richard_Jefferson_0001.jpg\n",
            "LFW/lfw_align_112/Daniell_Sunjata/\n",
            "LFW/lfw_align_112/Daniell_Sunjata/Daniell_Sunjata_0001.jpg\n",
            "LFW/lfw_align_112/Marina_Kuptsova/\n",
            "LFW/lfw_align_112/Marina_Kuptsova/Marina_Kuptsova_0001.jpg\n",
            "LFW/lfw_align_112/Emmy_Rossum/\n",
            "LFW/lfw_align_112/Emmy_Rossum/Emmy_Rossum_0001.jpg\n",
            "LFW/lfw_align_112/Corliss_Williamson/\n",
            "LFW/lfw_align_112/Corliss_Williamson/Corliss_Williamson_0001.jpg\n",
            "LFW/lfw_align_112/James_W_Kennedy/\n",
            "LFW/lfw_align_112/James_W_Kennedy/James_W_Kennedy_0001.jpg\n",
            "LFW/lfw_align_112/Habib_Rizieq/\n",
            "LFW/lfw_align_112/Habib_Rizieq/Habib_Rizieq_0001.jpg\n",
            "LFW/lfw_align_112/Habib_Rizieq/Habib_Rizieq_0005.jpg\n",
            "LFW/lfw_align_112/Habib_Rizieq/Habib_Rizieq_0003.jpg\n",
            "LFW/lfw_align_112/Habib_Rizieq/Habib_Rizieq_0004.jpg\n",
            "LFW/lfw_align_112/Habib_Rizieq/Habib_Rizieq_0002.jpg\n",
            "LFW/lfw_align_112/Roger_Etchegaray/\n",
            "LFW/lfw_align_112/Roger_Etchegaray/Roger_Etchegaray_0001.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0011.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0010.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0006.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0005.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0003.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0007.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0002.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0001.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0008.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0009.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0004.jpg\n",
            "LFW/lfw_align_112/Gonzalo_Sanchez_de_Lozada/Gonzalo_Sanchez_de_Lozada_0012.jpg\n",
            "LFW/lfw_align_112/Desiree_Lemosi/\n",
            "LFW/lfw_align_112/Desiree_Lemosi/Desiree_Lemosi_0002.jpg\n",
            "LFW/lfw_align_112/Desiree_Lemosi/Desiree_Lemosi_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Chapman/\n",
            "LFW/lfw_align_112/Tim_Chapman/Tim_Chapman_0002.jpg\n",
            "LFW/lfw_align_112/Tim_Chapman/Tim_Chapman_0001.jpg\n",
            "LFW/lfw_align_112/Christian_Wulff/\n",
            "LFW/lfw_align_112/Christian_Wulff/Christian_Wulff_0002.jpg\n",
            "LFW/lfw_align_112/Christian_Wulff/Christian_Wulff_0001.jpg\n",
            "LFW/lfw_align_112/Derek_Jeter/\n",
            "LFW/lfw_align_112/Derek_Jeter/Derek_Jeter_0002.jpg\n",
            "LFW/lfw_align_112/Derek_Jeter/Derek_Jeter_0003.jpg\n",
            "LFW/lfw_align_112/Derek_Jeter/Derek_Jeter_0004.jpg\n",
            "LFW/lfw_align_112/Derek_Jeter/Derek_Jeter_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Chandler_IV/\n",
            "LFW/lfw_align_112/Charles_Chandler_IV/Charles_Chandler_IV_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Maher/\n",
            "LFW/lfw_align_112/Bill_Maher/Bill_Maher_0001.jpg\n",
            "LFW/lfw_align_112/Xanana_Gusmao/\n",
            "LFW/lfw_align_112/Xanana_Gusmao/Xanana_Gusmao_0005.jpg\n",
            "LFW/lfw_align_112/Xanana_Gusmao/Xanana_Gusmao_0001.jpg\n",
            "LFW/lfw_align_112/Xanana_Gusmao/Xanana_Gusmao_0002.jpg\n",
            "LFW/lfw_align_112/Xanana_Gusmao/Xanana_Gusmao_0003.jpg\n",
            "LFW/lfw_align_112/Xanana_Gusmao/Xanana_Gusmao_0004.jpg\n",
            "LFW/lfw_align_112/Hayley_Tullett/\n",
            "LFW/lfw_align_112/Hayley_Tullett/Hayley_Tullett_0001.jpg\n",
            "LFW/lfw_align_112/Hayley_Tullett/Hayley_Tullett_0002.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0004.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0001.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0003.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0002.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0006.jpg\n",
            "LFW/lfw_align_112/Albert_Costa/Albert_Costa_0005.jpg\n",
            "LFW/lfw_align_112/John_McCain/\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0007.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0003.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0006.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0002.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0004.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0005.jpg\n",
            "LFW/lfw_align_112/John_McCain/John_McCain_0001.jpg\n",
            "LFW/lfw_align_112/Erick_Barkley/\n",
            "LFW/lfw_align_112/Erick_Barkley/Erick_Barkley_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Coverdale/\n",
            "LFW/lfw_align_112/Tom_Coverdale/Tom_Coverdale_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Coverdale/Tom_Coverdale_0002.jpg\n",
            "LFW/lfw_align_112/Judy_Locy/\n",
            "LFW/lfw_align_112/Judy_Locy/Judy_Locy_0001.jpg\n",
            "LFW/lfw_align_112/Jennifer_Renee_Short/\n",
            "LFW/lfw_align_112/Jennifer_Renee_Short/Jennifer_Renee_Short_0001.jpg\n",
            "LFW/lfw_align_112/Mehdi_Baala/\n",
            "LFW/lfw_align_112/Mehdi_Baala/Mehdi_Baala_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0005.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0003.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0006.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0002.jpg\n",
            "LFW/lfw_align_112/Jim_Furyk/Jim_Furyk_0004.jpg\n",
            "LFW/lfw_align_112/Hank_McKinnell/\n",
            "LFW/lfw_align_112/Hank_McKinnell/Hank_McKinnell_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Fonda/\n",
            "LFW/lfw_align_112/Peter_Fonda/Peter_Fonda_0001.jpg\n",
            "LFW/lfw_align_112/Giannina_Facio/\n",
            "LFW/lfw_align_112/Giannina_Facio/Giannina_Facio_0001.jpg\n",
            "LFW/lfw_align_112/Nicola_Wells/\n",
            "LFW/lfw_align_112/Nicola_Wells/Nicola_Wells_0001.jpg\n",
            "LFW/lfw_align_112/Patricia_Garone/\n",
            "LFW/lfw_align_112/Patricia_Garone/Patricia_Garone_0001.jpg\n",
            "LFW/lfw_align_112/Thad_Matta/\n",
            "LFW/lfw_align_112/Thad_Matta/Thad_Matta_0001.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0007.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0005.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0010.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0009.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0002.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0006.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0004.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0001.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0003.jpg\n",
            "LFW/lfw_align_112/Javier_Solana/Javier_Solana_0008.jpg\n",
            "LFW/lfw_align_112/Tian_Liang/\n",
            "LFW/lfw_align_112/Tian_Liang/Tian_Liang_0001.jpg\n",
            "LFW/lfw_align_112/Yu_Shyi-kun/\n",
            "LFW/lfw_align_112/Yu_Shyi-kun/Yu_Shyi-kun_0004.jpg\n",
            "LFW/lfw_align_112/Yu_Shyi-kun/Yu_Shyi-kun_0003.jpg\n",
            "LFW/lfw_align_112/Yu_Shyi-kun/Yu_Shyi-kun_0002.jpg\n",
            "LFW/lfw_align_112/Yu_Shyi-kun/Yu_Shyi-kun_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Samp/\n",
            "LFW/lfw_align_112/Mike_Samp/Mike_Samp_0001.jpg\n",
            "LFW/lfw_align_112/Kimi_Raikkonen/\n",
            "LFW/lfw_align_112/Kimi_Raikkonen/Kimi_Raikkonen_0002.jpg\n",
            "LFW/lfw_align_112/Kimi_Raikkonen/Kimi_Raikkonen_0003.jpg\n",
            "LFW/lfw_align_112/Kimi_Raikkonen/Kimi_Raikkonen_0001.jpg\n",
            "LFW/lfw_align_112/Albert_Pujols/\n",
            "LFW/lfw_align_112/Albert_Pujols/Albert_Pujols_0001.jpg\n",
            "LFW/lfw_align_112/Elena_Bovina/\n",
            "LFW/lfw_align_112/Elena_Bovina/Elena_Bovina_0001.jpg\n",
            "LFW/lfw_align_112/Elena_Bovina/Elena_Bovina_0002.jpg\n",
            "LFW/lfw_align_112/Elena_Bovina/Elena_Bovina_0003.jpg\n",
            "LFW/lfw_align_112/Diego_Armando_Maradona/\n",
            "LFW/lfw_align_112/Diego_Armando_Maradona/Diego_Armando_Maradona_0001.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0006.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0003.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0004.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0007.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0005.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0002.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0008.jpg\n",
            "LFW/lfw_align_112/Kim_Dae-jung/Kim_Dae-jung_0001.jpg\n",
            "LFW/lfw_align_112/Barbara_Bodine/\n",
            "LFW/lfw_align_112/Barbara_Bodine/Barbara_Bodine_0001.jpg\n",
            "LFW/lfw_align_112/Zoe_Ball/\n",
            "LFW/lfw_align_112/Zoe_Ball/Zoe_Ball_0001.jpg\n",
            "LFW/lfw_align_112/Hatsui_Hasuike/\n",
            "LFW/lfw_align_112/Hatsui_Hasuike/Hatsui_Hasuike_0001.jpg\n",
            "LFW/lfw_align_112/Gregorio_Honasan/\n",
            "LFW/lfw_align_112/Gregorio_Honasan/Gregorio_Honasan_0001.jpg\n",
            "LFW/lfw_align_112/Derrick_Taylor/\n",
            "LFW/lfw_align_112/Derrick_Taylor/Derrick_Taylor_0001.jpg\n",
            "LFW/lfw_align_112/Jan_Peter_Balkenende/\n",
            "LFW/lfw_align_112/Jan_Peter_Balkenende/Jan_Peter_Balkenende_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0006.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0007.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0004.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0011.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0010.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0003.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0008.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0002.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0009.jpg\n",
            "LFW/lfw_align_112/Paul_Burrell/Paul_Burrell_0005.jpg\n",
            "LFW/lfw_align_112/Anastasia_Myskina/\n",
            "LFW/lfw_align_112/Anastasia_Myskina/Anastasia_Myskina_0001.jpg\n",
            "LFW/lfw_align_112/Anastasia_Myskina/Anastasia_Myskina_0002.jpg\n",
            "LFW/lfw_align_112/Anastasia_Myskina/Anastasia_Myskina_0003.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0012.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0005.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0025.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0011.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0003.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0009.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0022.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0008.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0007.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0023.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0014.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0021.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0020.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0004.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0024.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0016.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0018.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0019.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0010.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0006.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0017.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0002.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0015.jpg\n",
            "LFW/lfw_align_112/Tom_Daschle/Tom_Daschle_0013.jpg\n",
            "LFW/lfw_align_112/Dion_Glover/\n",
            "LFW/lfw_align_112/Dion_Glover/Dion_Glover_0001.jpg\n",
            "LFW/lfw_align_112/Myung_Yang/\n",
            "LFW/lfw_align_112/Myung_Yang/Myung_Yang_0001.jpg\n",
            "LFW/lfw_align_112/Daniel_Rouse/\n",
            "LFW/lfw_align_112/Daniel_Rouse/Daniel_Rouse_0001.jpg\n",
            "LFW/lfw_align_112/Aram_Adler/\n",
            "LFW/lfw_align_112/Aram_Adler/Aram_Adler_0001.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0009.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0005.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0013.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0011.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0008.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0010.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0012.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0003.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0002.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0006.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0001.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0007.jpg\n",
            "LFW/lfw_align_112/Edmund_Stoiber/Edmund_Stoiber_0004.jpg\n",
            "LFW/lfw_align_112/Vincent_Sombrotto/\n",
            "LFW/lfw_align_112/Vincent_Sombrotto/Vincent_Sombrotto_0001.jpg\n",
            "LFW/lfw_align_112/Pierre_Van_Hooijdonk/\n",
            "LFW/lfw_align_112/Pierre_Van_Hooijdonk/Pierre_Van_Hooijdonk_0001.jpg\n",
            "LFW/lfw_align_112/Melvin_Talbert/\n",
            "LFW/lfw_align_112/Melvin_Talbert/Melvin_Talbert_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Joseph/\n",
            "LFW/lfw_align_112/Stephen_Joseph/Stephen_Joseph_0001.jpg\n",
            "LFW/lfw_align_112/Kellie_Coffey/\n",
            "LFW/lfw_align_112/Kellie_Coffey/Kellie_Coffey_0001.jpg\n",
            "LFW/lfw_align_112/Anders_Fogh_Rasmussen/\n",
            "LFW/lfw_align_112/Anders_Fogh_Rasmussen/Anders_Fogh_Rasmussen_0001.jpg\n",
            "LFW/lfw_align_112/Anders_Fogh_Rasmussen/Anders_Fogh_Rasmussen_0003.jpg\n",
            "LFW/lfw_align_112/Anders_Fogh_Rasmussen/Anders_Fogh_Rasmussen_0004.jpg\n",
            "LFW/lfw_align_112/Anders_Fogh_Rasmussen/Anders_Fogh_Rasmussen_0002.jpg\n",
            "LFW/lfw_align_112/George_Foreman/\n",
            "LFW/lfw_align_112/George_Foreman/George_Foreman_0002.jpg\n",
            "LFW/lfw_align_112/George_Foreman/George_Foreman_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Dole/\n",
            "LFW/lfw_align_112/Bob_Dole/Bob_Dole_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Dole/Bob_Dole_0002.jpg\n",
            "LFW/lfw_align_112/Bob_Dole/Bob_Dole_0003.jpg\n",
            "LFW/lfw_align_112/Dany_Heatley/\n",
            "LFW/lfw_align_112/Dany_Heatley/Dany_Heatley_0001.jpg\n",
            "LFW/lfw_align_112/Doug_Collins/\n",
            "LFW/lfw_align_112/Doug_Collins/Doug_Collins_0001.jpg\n",
            "LFW/lfw_align_112/Doug_Collins/Doug_Collins_0002.jpg\n",
            "LFW/lfw_align_112/Conan_OBrien/\n",
            "LFW/lfw_align_112/Conan_OBrien/Conan_OBrien_0004.jpg\n",
            "LFW/lfw_align_112/Conan_OBrien/Conan_OBrien_0003.jpg\n",
            "LFW/lfw_align_112/Conan_OBrien/Conan_OBrien_0001.jpg\n",
            "LFW/lfw_align_112/Conan_OBrien/Conan_OBrien_0002.jpg\n",
            "LFW/lfw_align_112/Stefano_Gabbana/\n",
            "LFW/lfw_align_112/Stefano_Gabbana/Stefano_Gabbana_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Finley/\n",
            "LFW/lfw_align_112/Joe_Finley/Joe_Finley_0001.jpg\n",
            "LFW/lfw_align_112/Mikhail_Shvydkoi/\n",
            "LFW/lfw_align_112/Mikhail_Shvydkoi/Mikhail_Shvydkoi_0001.jpg\n",
            "LFW/lfw_align_112/Larry_Ralston/\n",
            "LFW/lfw_align_112/Larry_Ralston/Larry_Ralston_0001.jpg\n",
            "LFW/lfw_align_112/Emilio_Botin/\n",
            "LFW/lfw_align_112/Emilio_Botin/Emilio_Botin_0001.jpg\n",
            "LFW/lfw_align_112/Gene_Robinson/\n",
            "LFW/lfw_align_112/Gene_Robinson/Gene_Robinson_0005.jpg\n",
            "LFW/lfw_align_112/Gene_Robinson/Gene_Robinson_0004.jpg\n",
            "LFW/lfw_align_112/Gene_Robinson/Gene_Robinson_0001.jpg\n",
            "LFW/lfw_align_112/Gene_Robinson/Gene_Robinson_0003.jpg\n",
            "LFW/lfw_align_112/Gene_Robinson/Gene_Robinson_0002.jpg\n",
            "LFW/lfw_align_112/Park_Jie-won/\n",
            "LFW/lfw_align_112/Park_Jie-won/Park_Jie-won_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Bronson/\n",
            "LFW/lfw_align_112/Charles_Bronson/Charles_Bronson_0002.jpg\n",
            "LFW/lfw_align_112/Charles_Bronson/Charles_Bronson_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Bronson/Charles_Bronson_0003.jpg\n",
            "LFW/lfw_align_112/John_Sununu/\n",
            "LFW/lfw_align_112/John_Sununu/John_Sununu_0001.jpg\n",
            "LFW/lfw_align_112/Thomas_Day/\n",
            "LFW/lfw_align_112/Thomas_Day/Thomas_Day_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0005.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0006.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0003.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0002.jpg\n",
            "LFW/lfw_align_112/Bob_Graham/Bob_Graham_0004.jpg\n",
            "LFW/lfw_align_112/Nathan_Smith/\n",
            "LFW/lfw_align_112/Nathan_Smith/Nathan_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Aly_Wagner/\n",
            "LFW/lfw_align_112/Aly_Wagner/Aly_Wagner_0001.jpg\n",
            "LFW/lfw_align_112/Melchor_Cob_Castro/\n",
            "LFW/lfw_align_112/Melchor_Cob_Castro/Melchor_Cob_Castro_0001.jpg\n",
            "LFW/lfw_align_112/Tippi_Hedren/\n",
            "LFW/lfw_align_112/Tippi_Hedren/Tippi_Hedren_0002.jpg\n",
            "LFW/lfw_align_112/Tippi_Hedren/Tippi_Hedren_0001.jpg\n",
            "LFW/lfw_align_112/Ion_Iliescu/\n",
            "LFW/lfw_align_112/Ion_Iliescu/Ion_Iliescu_0001.jpg\n",
            "LFW/lfw_align_112/Joan_Dangerfield/\n",
            "LFW/lfw_align_112/Joan_Dangerfield/Joan_Dangerfield_0001.jpg\n",
            "LFW/lfw_align_112/Shi_Guangsheng/\n",
            "LFW/lfw_align_112/Shi_Guangsheng/Shi_Guangsheng_0001.jpg\n",
            "LFW/lfw_align_112/Stella_McCartney/\n",
            "LFW/lfw_align_112/Stella_McCartney/Stella_McCartney_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Mohammed_Maher/\n",
            "LFW/lfw_align_112/Ali_Mohammed_Maher/Ali_Mohammed_Maher_0001.jpg\n",
            "LFW/lfw_align_112/Carlton_Dotson/\n",
            "LFW/lfw_align_112/Carlton_Dotson/Carlton_Dotson_0001.jpg\n",
            "LFW/lfw_align_112/Eric_Fehr/\n",
            "LFW/lfw_align_112/Eric_Fehr/Eric_Fehr_0001.jpg\n",
            "LFW/lfw_align_112/Riek_Blanjaar/\n",
            "LFW/lfw_align_112/Riek_Blanjaar/Riek_Blanjaar_0001.jpg\n",
            "LFW/lfw_align_112/Roger_Staubach/\n",
            "LFW/lfw_align_112/Roger_Staubach/Roger_Staubach_0001.jpg\n",
            "LFW/lfw_align_112/Allison_Janney/\n",
            "LFW/lfw_align_112/Allison_Janney/Allison_Janney_0001.jpg\n",
            "LFW/lfw_align_112/Allison_Janney/Allison_Janney_0002.jpg\n",
            "LFW/lfw_align_112/Azmi_Bishara/\n",
            "LFW/lfw_align_112/Azmi_Bishara/Azmi_Bishara_0001.jpg\n",
            "LFW/lfw_align_112/Gordon_McDonald/\n",
            "LFW/lfw_align_112/Gordon_McDonald/Gordon_McDonald_0001.jpg\n",
            "LFW/lfw_align_112/LK_Advani/\n",
            "LFW/lfw_align_112/LK_Advani/LK_Advani_0002.jpg\n",
            "LFW/lfw_align_112/LK_Advani/LK_Advani_0001.jpg\n",
            "LFW/lfw_align_112/LK_Advani/LK_Advani_0003.jpg\n",
            "LFW/lfw_align_112/Keith_Van_Horn/\n",
            "LFW/lfw_align_112/Keith_Van_Horn/Keith_Van_Horn_0001.jpg\n",
            "LFW/lfw_align_112/Reggie_Sanders/\n",
            "LFW/lfw_align_112/Reggie_Sanders/Reggie_Sanders_0001.jpg\n",
            "LFW/lfw_align_112/Maggie_Smith/\n",
            "LFW/lfw_align_112/Maggie_Smith/Maggie_Smith_0001.jpg\n",
            "LFW/lfw_align_112/Maggie_Smith/Maggie_Smith_0002.jpg\n",
            "LFW/lfw_align_112/Aaron_Pena/\n",
            "LFW/lfw_align_112/Aaron_Pena/Aaron_Pena_0001.jpg\n",
            "LFW/lfw_align_112/Guillaume_Cannet/\n",
            "LFW/lfw_align_112/Guillaume_Cannet/Guillaume_Cannet_0001.jpg\n",
            "LFW/lfw_align_112/Enrique_Oliu/\n",
            "LFW/lfw_align_112/Enrique_Oliu/Enrique_Oliu_0001.jpg\n",
            "LFW/lfw_align_112/Jong_Wook_Lee/\n",
            "LFW/lfw_align_112/Jong_Wook_Lee/Jong_Wook_Lee_0003.jpg\n",
            "LFW/lfw_align_112/Jong_Wook_Lee/Jong_Wook_Lee_0001.jpg\n",
            "LFW/lfw_align_112/Jong_Wook_Lee/Jong_Wook_Lee_0002.jpg\n",
            "LFW/lfw_align_112/Jong_Wook_Lee/Jong_Wook_Lee_0004.jpg\n",
            "LFW/lfw_align_112/Tsutomu_Takebe/\n",
            "LFW/lfw_align_112/Tsutomu_Takebe/Tsutomu_Takebe_0001.jpg\n",
            "LFW/lfw_align_112/Tsutomu_Takebe/Tsutomu_Takebe_0002.jpg\n",
            "LFW/lfw_align_112/Sonia_Lopez/\n",
            "LFW/lfw_align_112/Sonia_Lopez/Sonia_Lopez_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Byrd/\n",
            "LFW/lfw_align_112/Paul_Byrd/Paul_Byrd_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Byrd/Paul_Byrd_0002.jpg\n",
            "LFW/lfw_align_112/Nick_Cassavetes/\n",
            "LFW/lfw_align_112/Nick_Cassavetes/Nick_Cassavetes_0001.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0004.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0001.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0006.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0002.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0005.jpg\n",
            "LFW/lfw_align_112/Thaksin_Shinawatra/Thaksin_Shinawatra_0003.jpg\n",
            "LFW/lfw_align_112/Rahul_Dravid/\n",
            "LFW/lfw_align_112/Rahul_Dravid/Rahul_Dravid_0001.jpg\n",
            "LFW/lfw_align_112/Juliette_Binoche/\n",
            "LFW/lfw_align_112/Juliette_Binoche/Juliette_Binoche_0001.jpg\n",
            "LFW/lfw_align_112/Spike_Jonze/\n",
            "LFW/lfw_align_112/Spike_Jonze/Spike_Jonze_0001.jpg\n",
            "LFW/lfw_align_112/Ian_Moran/\n",
            "LFW/lfw_align_112/Ian_Moran/Ian_Moran_0001.jpg\n",
            "LFW/lfw_align_112/Brad_Brownell/\n",
            "LFW/lfw_align_112/Brad_Brownell/Brad_Brownell_0001.jpg\n",
            "LFW/lfw_align_112/Ernie_Eves/\n",
            "LFW/lfw_align_112/Ernie_Eves/Ernie_Eves_0002.jpg\n",
            "LFW/lfw_align_112/Ernie_Eves/Ernie_Eves_0001.jpg\n",
            "LFW/lfw_align_112/Edward_Norton/\n",
            "LFW/lfw_align_112/Edward_Norton/Edward_Norton_0002.jpg\n",
            "LFW/lfw_align_112/Edward_Norton/Edward_Norton_0001.jpg\n",
            "LFW/lfw_align_112/Helen_Alvare/\n",
            "LFW/lfw_align_112/Helen_Alvare/Helen_Alvare_0001.jpg\n",
            "LFW/lfw_align_112/Audrey_Lacroix/\n",
            "LFW/lfw_align_112/Audrey_Lacroix/Audrey_Lacroix_0001.jpg\n",
            "LFW/lfw_align_112/Chuck_Amato/\n",
            "LFW/lfw_align_112/Chuck_Amato/Chuck_Amato_0002.jpg\n",
            "LFW/lfw_align_112/Chuck_Amato/Chuck_Amato_0001.jpg\n",
            "LFW/lfw_align_112/Michelle_Chiklis/\n",
            "LFW/lfw_align_112/Michelle_Chiklis/Michelle_Chiklis_0001.jpg\n",
            "LFW/lfw_align_112/Walt_Harris/\n",
            "LFW/lfw_align_112/Walt_Harris/Walt_Harris_0001.jpg\n",
            "LFW/lfw_align_112/Julia_Ormond/\n",
            "LFW/lfw_align_112/Julia_Ormond/Julia_Ormond_0001.jpg\n",
            "LFW/lfw_align_112/Franklin_Brown/\n",
            "LFW/lfw_align_112/Franklin_Brown/Franklin_Brown_0001.jpg\n",
            "LFW/lfw_align_112/Adrian_Annus/\n",
            "LFW/lfw_align_112/Adrian_Annus/Adrian_Annus_0001.jpg\n",
            "LFW/lfw_align_112/Edward_Burns/\n",
            "LFW/lfw_align_112/Edward_Burns/Edward_Burns_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Cantalupo/\n",
            "LFW/lfw_align_112/Jim_Cantalupo/Jim_Cantalupo_0001.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0007.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0001.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0003.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0002.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0006.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0005.jpg\n",
            "LFW/lfw_align_112/Ana_Guevara/Ana_Guevara_0004.jpg\n",
            "LFW/lfw_align_112/Jim_Spinoza/\n",
            "LFW/lfw_align_112/Jim_Spinoza/Jim_Spinoza_0001.jpg\n",
            "LFW/lfw_align_112/Mikhail_Wehbe/\n",
            "LFW/lfw_align_112/Mikhail_Wehbe/Mikhail_Wehbe_0003.jpg\n",
            "LFW/lfw_align_112/Mikhail_Wehbe/Mikhail_Wehbe_0002.jpg\n",
            "LFW/lfw_align_112/Mikhail_Wehbe/Mikhail_Wehbe_0004.jpg\n",
            "LFW/lfw_align_112/Mikhail_Wehbe/Mikhail_Wehbe_0001.jpg\n",
            "LFW/lfw_align_112/Marsha_Thomason/\n",
            "LFW/lfw_align_112/Marsha_Thomason/Marsha_Thomason_0001.jpg\n",
            "LFW/lfw_align_112/David_Anderson/\n",
            "LFW/lfw_align_112/David_Anderson/David_Anderson_0001.jpg\n",
            "LFW/lfw_align_112/David_Anderson/David_Anderson_0005.jpg\n",
            "LFW/lfw_align_112/David_Anderson/David_Anderson_0002.jpg\n",
            "LFW/lfw_align_112/David_Anderson/David_Anderson_0003.jpg\n",
            "LFW/lfw_align_112/David_Anderson/David_Anderson_0004.jpg\n",
            "LFW/lfw_align_112/Silvia_Farina_Elia/\n",
            "LFW/lfw_align_112/Silvia_Farina_Elia/Silvia_Farina_Elia_0002.jpg\n",
            "LFW/lfw_align_112/Silvia_Farina_Elia/Silvia_Farina_Elia_0001.jpg\n",
            "LFW/lfw_align_112/Silvia_Farina_Elia/Silvia_Farina_Elia_0003.jpg\n",
            "LFW/lfw_align_112/Chung_Mong-hun/\n",
            "LFW/lfw_align_112/Chung_Mong-hun/Chung_Mong-hun_0001.jpg\n",
            "LFW/lfw_align_112/Chung_Mong-hun/Chung_Mong-hun_0002.jpg\n",
            "LFW/lfw_align_112/Nastia_Liukin/\n",
            "LFW/lfw_align_112/Nastia_Liukin/Nastia_Liukin_0001.jpg\n",
            "LFW/lfw_align_112/Ray_Allen/\n",
            "LFW/lfw_align_112/Ray_Allen/Ray_Allen_0002.jpg\n",
            "LFW/lfw_align_112/Ray_Allen/Ray_Allen_0003.jpg\n",
            "LFW/lfw_align_112/Ray_Allen/Ray_Allen_0001.jpg\n",
            "LFW/lfw_align_112/Russ_Ortiz/\n",
            "LFW/lfw_align_112/Russ_Ortiz/Russ_Ortiz_0001.jpg\n",
            "LFW/lfw_align_112/Zakia_Hakki/\n",
            "LFW/lfw_align_112/Zakia_Hakki/Zakia_Hakki_0001.jpg\n",
            "LFW/lfw_align_112/Florencia_Macri/\n",
            "LFW/lfw_align_112/Florencia_Macri/Florencia_Macri_0001.jpg\n",
            "LFW/lfw_align_112/Ratna_Sari_Dewi_Sukarno/\n",
            "LFW/lfw_align_112/Ratna_Sari_Dewi_Sukarno/Ratna_Sari_Dewi_Sukarno_0001.jpg\n",
            "LFW/lfw_align_112/Gloria_Trevi/\n",
            "LFW/lfw_align_112/Gloria_Trevi/Gloria_Trevi_0003.jpg\n",
            "LFW/lfw_align_112/Gloria_Trevi/Gloria_Trevi_0001.jpg\n",
            "LFW/lfw_align_112/Gloria_Trevi/Gloria_Trevi_0002.jpg\n",
            "LFW/lfw_align_112/Gloria_Trevi/Gloria_Trevi_0004.jpg\n",
            "LFW/lfw_align_112/Richard_Reid/\n",
            "LFW/lfw_align_112/Richard_Reid/Richard_Reid_0001.jpg\n",
            "LFW/lfw_align_112/Joey_Harrington/\n",
            "LFW/lfw_align_112/Joey_Harrington/Joey_Harrington_0001.jpg\n",
            "LFW/lfw_align_112/Debbie_Reynolds/\n",
            "LFW/lfw_align_112/Debbie_Reynolds/Debbie_Reynolds_0003.jpg\n",
            "LFW/lfw_align_112/Debbie_Reynolds/Debbie_Reynolds_0001.jpg\n",
            "LFW/lfw_align_112/Debbie_Reynolds/Debbie_Reynolds_0002.jpg\n",
            "LFW/lfw_align_112/Debbie_Reynolds/Debbie_Reynolds_0004.jpg\n",
            "LFW/lfw_align_112/Terence_Newman/\n",
            "LFW/lfw_align_112/Terence_Newman/Terence_Newman_0001.jpg\n",
            "LFW/lfw_align_112/Valerie_Thwaites/\n",
            "LFW/lfw_align_112/Valerie_Thwaites/Valerie_Thwaites_0001.jpg\n",
            "LFW/lfw_align_112/Pilar_Montenegro/\n",
            "LFW/lfw_align_112/Pilar_Montenegro/Pilar_Montenegro_0001.jpg\n",
            "LFW/lfw_align_112/Sean_OKeefe/\n",
            "LFW/lfw_align_112/Sean_OKeefe/Sean_OKeefe_0005.jpg\n",
            "LFW/lfw_align_112/Sean_OKeefe/Sean_OKeefe_0001.jpg\n",
            "LFW/lfw_align_112/Sean_OKeefe/Sean_OKeefe_0002.jpg\n",
            "LFW/lfw_align_112/Sean_OKeefe/Sean_OKeefe_0004.jpg\n",
            "LFW/lfw_align_112/Sean_OKeefe/Sean_OKeefe_0003.jpg\n",
            "LFW/lfw_align_112/Randy_Dryer/\n",
            "LFW/lfw_align_112/Randy_Dryer/Randy_Dryer_0001.jpg\n",
            "LFW/lfw_align_112/Darvis_Patton/\n",
            "LFW/lfw_align_112/Darvis_Patton/Darvis_Patton_0001.jpg\n",
            "LFW/lfw_align_112/Svetlana_Koroleva/\n",
            "LFW/lfw_align_112/Svetlana_Koroleva/Svetlana_Koroleva_0002.jpg\n",
            "LFW/lfw_align_112/Svetlana_Koroleva/Svetlana_Koroleva_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Chan/\n",
            "LFW/lfw_align_112/Peter_Chan/Peter_Chan_0001.jpg\n",
            "LFW/lfw_align_112/Jonathan_Woodgate/\n",
            "LFW/lfw_align_112/Jonathan_Woodgate/Jonathan_Woodgate_0001.jpg\n",
            "LFW/lfw_align_112/Gisele_Bundchen/\n",
            "LFW/lfw_align_112/Gisele_Bundchen/Gisele_Bundchen_0001.jpg\n",
            "LFW/lfw_align_112/Gisele_Bundchen/Gisele_Bundchen_0002.jpg\n",
            "LFW/lfw_align_112/Mark_Hurlbert/\n",
            "LFW/lfw_align_112/Mark_Hurlbert/Mark_Hurlbert_0004.jpg\n",
            "LFW/lfw_align_112/Mark_Hurlbert/Mark_Hurlbert_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Hurlbert/Mark_Hurlbert_0005.jpg\n",
            "LFW/lfw_align_112/Mark_Hurlbert/Mark_Hurlbert_0002.jpg\n",
            "LFW/lfw_align_112/Mark_Hurlbert/Mark_Hurlbert_0003.jpg\n",
            "LFW/lfw_align_112/Alex_Zanardi/\n",
            "LFW/lfw_align_112/Alex_Zanardi/Alex_Zanardi_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0005.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0002.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0004.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0003.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0008.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0007.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0009.jpg\n",
            "LFW/lfw_align_112/Richard_Armitage/Richard_Armitage_0006.jpg\n",
            "LFW/lfw_align_112/Gustavo_Terrazas/\n",
            "LFW/lfw_align_112/Gustavo_Terrazas/Gustavo_Terrazas_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Whitney/\n",
            "LFW/lfw_align_112/Chris_Whitney/Chris_Whitney_0001.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0004.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0009.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0016.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0012.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0007.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0013.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0018.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0011.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0010.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0005.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0003.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0015.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0014.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0008.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0019.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0017.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0006.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0002.jpg\n",
            "LFW/lfw_align_112/Carlos_Moya/Carlos_Moya_0001.jpg\n",
            "LFW/lfw_align_112/Abdel_Nasser_Assidi/\n",
            "LFW/lfw_align_112/Abdel_Nasser_Assidi/Abdel_Nasser_Assidi_0001.jpg\n",
            "LFW/lfw_align_112/Abdel_Nasser_Assidi/Abdel_Nasser_Assidi_0002.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0008.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0002.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0005.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0004.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0001.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0007.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0003.jpg\n",
            "LFW/lfw_align_112/Shimon_Peres/Shimon_Peres_0006.jpg\n",
            "LFW/lfw_align_112/Bill_Kollar/\n",
            "LFW/lfw_align_112/Bill_Kollar/Bill_Kollar_0001.jpg\n",
            "LFW/lfw_align_112/Luis_Rosario_Huertas/\n",
            "LFW/lfw_align_112/Luis_Rosario_Huertas/Luis_Rosario_Huertas_0001.jpg\n",
            "LFW/lfw_align_112/Federico_Fellini/\n",
            "LFW/lfw_align_112/Federico_Fellini/Federico_Fellini_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Carrey/\n",
            "LFW/lfw_align_112/Jim_Carrey/Jim_Carrey_0002.jpg\n",
            "LFW/lfw_align_112/Jim_Carrey/Jim_Carrey_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Carrey/Jim_Carrey_0003.jpg\n",
            "LFW/lfw_align_112/Kong_Quan/\n",
            "LFW/lfw_align_112/Kong_Quan/Kong_Quan_0001.jpg\n",
            "LFW/lfw_align_112/Pamela_Anderson/\n",
            "LFW/lfw_align_112/Pamela_Anderson/Pamela_Anderson_0004.jpg\n",
            "LFW/lfw_align_112/Pamela_Anderson/Pamela_Anderson_0002.jpg\n",
            "LFW/lfw_align_112/Pamela_Anderson/Pamela_Anderson_0005.jpg\n",
            "LFW/lfw_align_112/Pamela_Anderson/Pamela_Anderson_0001.jpg\n",
            "LFW/lfw_align_112/Pamela_Anderson/Pamela_Anderson_0003.jpg\n",
            "LFW/lfw_align_112/Dan_Guerrero/\n",
            "LFW/lfw_align_112/Dan_Guerrero/Dan_Guerrero_0001.jpg\n",
            "LFW/lfw_align_112/Ernie_Preate/\n",
            "LFW/lfw_align_112/Ernie_Preate/Ernie_Preate_0001.jpg\n",
            "LFW/lfw_align_112/Ray_Halbritter/\n",
            "LFW/lfw_align_112/Ray_Halbritter/Ray_Halbritter_0001.jpg\n",
            "LFW/lfw_align_112/Franco_Dragone/\n",
            "LFW/lfw_align_112/Franco_Dragone/Franco_Dragone_0002.jpg\n",
            "LFW/lfw_align_112/Franco_Dragone/Franco_Dragone_0001.jpg\n",
            "LFW/lfw_align_112/Fredric_Seaman/\n",
            "LFW/lfw_align_112/Fredric_Seaman/Fredric_Seaman_0001.jpg\n",
            "LFW/lfw_align_112/Ken_Wharfe/\n",
            "LFW/lfw_align_112/Ken_Wharfe/Ken_Wharfe_0001.jpg\n",
            "LFW/lfw_align_112/Hugh_Miller/\n",
            "LFW/lfw_align_112/Hugh_Miller/Hugh_Miller_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Martin/\n",
            "LFW/lfw_align_112/Jamie_Martin/Jamie_Martin_0001.jpg\n",
            "LFW/lfw_align_112/Adrianna_Zuzic/\n",
            "LFW/lfw_align_112/Adrianna_Zuzic/Adrianna_Zuzic_0001.jpg\n",
            "LFW/lfw_align_112/Jonathan_Fine/\n",
            "LFW/lfw_align_112/Jonathan_Fine/Jonathan_Fine_0001.jpg\n",
            "LFW/lfw_align_112/Carlos_Ghosn/\n",
            "LFW/lfw_align_112/Carlos_Ghosn/Carlos_Ghosn_0002.jpg\n",
            "LFW/lfw_align_112/Carlos_Ghosn/Carlos_Ghosn_0001.jpg\n",
            "LFW/lfw_align_112/Derek_King/\n",
            "LFW/lfw_align_112/Derek_King/Derek_King_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Funk/\n",
            "LFW/lfw_align_112/Stephen_Funk/Stephen_Funk_0001.jpg\n",
            "LFW/lfw_align_112/Ahmad_Masood/\n",
            "LFW/lfw_align_112/Ahmad_Masood/Ahmad_Masood_0001.jpg\n",
            "LFW/lfw_align_112/Ahmad_Masood/Ahmad_Masood_0002.jpg\n",
            "LFW/lfw_align_112/Debra_Shank/\n",
            "LFW/lfw_align_112/Debra_Shank/Debra_Shank_0001.jpg\n",
            "LFW/lfw_align_112/Iain_Richmond/\n",
            "LFW/lfw_align_112/Iain_Richmond/Iain_Richmond_0001.jpg\n",
            "LFW/lfw_align_112/Rosalyn_Carter/\n",
            "LFW/lfw_align_112/Rosalyn_Carter/Rosalyn_Carter_0001.jpg\n",
            "LFW/lfw_align_112/Noor_Mohammed/\n",
            "LFW/lfw_align_112/Noor_Mohammed/Noor_Mohammed_0001.jpg\n",
            "LFW/lfw_align_112/Robin_Wright_Penn/\n",
            "LFW/lfw_align_112/Robin_Wright_Penn/Robin_Wright_Penn_0001.jpg\n",
            "LFW/lfw_align_112/Candace_Sutton/\n",
            "LFW/lfw_align_112/Candace_Sutton/Candace_Sutton_0001.jpg\n",
            "LFW/lfw_align_112/Roberto_Robaina/\n",
            "LFW/lfw_align_112/Roberto_Robaina/Roberto_Robaina_0001.jpg\n",
            "LFW/lfw_align_112/Daniela_Cicarelli/\n",
            "LFW/lfw_align_112/Daniela_Cicarelli/Daniela_Cicarelli_0001.jpg\n",
            "LFW/lfw_align_112/Adam_Ant/\n",
            "LFW/lfw_align_112/Adam_Ant/Adam_Ant_0001.jpg\n",
            "LFW/lfw_align_112/Jason_Statham/\n",
            "LFW/lfw_align_112/Jason_Statham/Jason_Statham_0001.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0006.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0002.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0005.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0003.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0001.jpg\n",
            "LFW/lfw_align_112/Zinedine_Zidane/Zinedine_Zidane_0004.jpg\n",
            "LFW/lfw_align_112/Katrin_Susi/\n",
            "LFW/lfw_align_112/Katrin_Susi/Katrin_Susi_0001.jpg\n",
            "LFW/lfw_align_112/Lou_Piniella/\n",
            "LFW/lfw_align_112/Lou_Piniella/Lou_Piniella_0002.jpg\n",
            "LFW/lfw_align_112/Lou_Piniella/Lou_Piniella_0003.jpg\n",
            "LFW/lfw_align_112/Lou_Piniella/Lou_Piniella_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Bellhorn/\n",
            "LFW/lfw_align_112/Mark_Bellhorn/Mark_Bellhorn_0001.jpg\n",
            "LFW/lfw_align_112/Keith_Fotta/\n",
            "LFW/lfw_align_112/Keith_Fotta/Keith_Fotta_0001.jpg\n",
            "LFW/lfw_align_112/Mstislav_Rostropovich/\n",
            "LFW/lfw_align_112/Mstislav_Rostropovich/Mstislav_Rostropovich_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Hahn/\n",
            "LFW/lfw_align_112/Jim_Hahn/Jim_Hahn_0004.jpg\n",
            "LFW/lfw_align_112/Jim_Hahn/Jim_Hahn_0002.jpg\n",
            "LFW/lfw_align_112/Jim_Hahn/Jim_Hahn_0003.jpg\n",
            "LFW/lfw_align_112/Jim_Hahn/Jim_Hahn_0001.jpg\n",
            "LFW/lfw_align_112/Ian_Knop/\n",
            "LFW/lfw_align_112/Ian_Knop/Ian_Knop_0001.jpg\n",
            "LFW/lfw_align_112/Olivier_Boulay/\n",
            "LFW/lfw_align_112/Olivier_Boulay/Olivier_Boulay_0001.jpg\n",
            "LFW/lfw_align_112/Cruz_Bustamante/\n",
            "LFW/lfw_align_112/Cruz_Bustamante/Cruz_Bustamante_0004.jpg\n",
            "LFW/lfw_align_112/Cruz_Bustamante/Cruz_Bustamante_0005.jpg\n",
            "LFW/lfw_align_112/Cruz_Bustamante/Cruz_Bustamante_0002.jpg\n",
            "LFW/lfw_align_112/Cruz_Bustamante/Cruz_Bustamante_0001.jpg\n",
            "LFW/lfw_align_112/Cruz_Bustamante/Cruz_Bustamante_0003.jpg\n",
            "LFW/lfw_align_112/Anton_Balasingham/\n",
            "LFW/lfw_align_112/Anton_Balasingham/Anton_Balasingham_0001.jpg\n",
            "LFW/lfw_align_112/Tracee_Ellis_Ross/\n",
            "LFW/lfw_align_112/Tracee_Ellis_Ross/Tracee_Ellis_Ross_0001.jpg\n",
            "LFW/lfw_align_112/Tracee_Ellis_Ross/Tracee_Ellis_Ross_0002.jpg\n",
            "LFW/lfw_align_112/Victor_Hanescu/\n",
            "LFW/lfw_align_112/Victor_Hanescu/Victor_Hanescu_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Pollack/\n",
            "LFW/lfw_align_112/Robert_Pollack/Robert_Pollack_0001.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0010.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0018.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0002.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0005.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0016.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0006.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0004.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0015.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0001.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0009.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0014.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0012.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0013.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0008.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0011.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0017.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0007.jpg\n",
            "LFW/lfw_align_112/Fidel_Castro/Fidel_Castro_0003.jpg\n",
            "LFW/lfw_align_112/Roel_Campos/\n",
            "LFW/lfw_align_112/Roel_Campos/Roel_Campos_0001.jpg\n",
            "LFW/lfw_align_112/Janet_Crawford/\n",
            "LFW/lfw_align_112/Janet_Crawford/Janet_Crawford_0001.jpg\n",
            "LFW/lfw_align_112/Terry_Lynn_Barton/\n",
            "LFW/lfw_align_112/Terry_Lynn_Barton/Terry_Lynn_Barton_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Wolf/\n",
            "LFW/lfw_align_112/Scott_Wolf/Scott_Wolf_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Wolf/Scott_Wolf_0002.jpg\n",
            "LFW/lfw_align_112/Joe_Paterno/\n",
            "LFW/lfw_align_112/Joe_Paterno/Joe_Paterno_0001.jpg\n",
            "LFW/lfw_align_112/German_Khan/\n",
            "LFW/lfw_align_112/German_Khan/German_Khan_0001.jpg\n",
            "LFW/lfw_align_112/Philip_Murtaugh/\n",
            "LFW/lfw_align_112/Philip_Murtaugh/Philip_Murtaugh_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Blackwill/\n",
            "LFW/lfw_align_112/Robert_Blackwill/Robert_Blackwill_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Blackwill/Robert_Blackwill_0002.jpg\n",
            "LFW/lfw_align_112/Gabriel_Hughes/\n",
            "LFW/lfw_align_112/Gabriel_Hughes/Gabriel_Hughes_0001.jpg\n",
            "LFW/lfw_align_112/David_Westerfield/\n",
            "LFW/lfw_align_112/David_Westerfield/David_Westerfield_0001.jpg\n",
            "LFW/lfw_align_112/Petro_Symonenko/\n",
            "LFW/lfw_align_112/Petro_Symonenko/Petro_Symonenko_0002.jpg\n",
            "LFW/lfw_align_112/Petro_Symonenko/Petro_Symonenko_0001.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0011.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0052.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0012.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0039.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0045.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0020.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0018.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0017.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0037.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0019.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0010.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0031.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0024.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0040.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0049.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0041.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0023.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0038.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0001.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0033.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0044.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0026.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0042.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0030.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0029.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0007.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0051.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0014.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0022.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0003.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0036.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0046.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0035.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0004.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0025.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0028.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0021.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0047.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0016.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0006.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0048.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0043.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0008.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0002.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0013.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0050.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0005.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0034.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0015.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0027.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0032.jpg\n",
            "LFW/lfw_align_112/Jacques_Chirac/Jacques_Chirac_0009.jpg\n",
            "LFW/lfw_align_112/Edwina_Currie/\n",
            "LFW/lfw_align_112/Edwina_Currie/Edwina_Currie_0002.jpg\n",
            "LFW/lfw_align_112/Edwina_Currie/Edwina_Currie_0004.jpg\n",
            "LFW/lfw_align_112/Edwina_Currie/Edwina_Currie_0001.jpg\n",
            "LFW/lfw_align_112/Edwina_Currie/Edwina_Currie_0003.jpg\n",
            "LFW/lfw_align_112/Herman_Edwards/\n",
            "LFW/lfw_align_112/Herman_Edwards/Herman_Edwards_0001.jpg\n",
            "LFW/lfw_align_112/Marisol_Martinez_Sambran/\n",
            "LFW/lfw_align_112/Marisol_Martinez_Sambran/Marisol_Martinez_Sambran_0001.jpg\n",
            "LFW/lfw_align_112/Art_Lopez/\n",
            "LFW/lfw_align_112/Art_Lopez/Art_Lopez_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0009.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0003.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0006.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0007.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0005.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0008.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0011.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0004.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0002.jpg\n",
            "LFW/lfw_align_112/Mark_Philippoussis/Mark_Philippoussis_0010.jpg\n",
            "LFW/lfw_align_112/Janez_Drnovsek/\n",
            "LFW/lfw_align_112/Janez_Drnovsek/Janez_Drnovsek_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Simon/\n",
            "LFW/lfw_align_112/Chris_Simon/Chris_Simon_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Chamberlain/\n",
            "LFW/lfw_align_112/Richard_Chamberlain/Richard_Chamberlain_0001.jpg\n",
            "LFW/lfw_align_112/Jack_Knowlton/\n",
            "LFW/lfw_align_112/Jack_Knowlton/Jack_Knowlton_0001.jpg\n",
            "LFW/lfw_align_112/Abba_Eban/\n",
            "LFW/lfw_align_112/Abba_Eban/Abba_Eban_0001.jpg\n",
            "LFW/lfw_align_112/Evo_Morales/\n",
            "LFW/lfw_align_112/Evo_Morales/Evo_Morales_0001.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0014.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0001.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0003.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0013.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0002.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0015.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0011.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0008.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0005.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0016.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0010.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0007.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0004.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0009.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0017.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0012.jpg\n",
            "LFW/lfw_align_112/Renee_Zellweger/Renee_Zellweger_0006.jpg\n",
            "LFW/lfw_align_112/Marina_Hands/\n",
            "LFW/lfw_align_112/Marina_Hands/Marina_Hands_0001.jpg\n",
            "LFW/lfw_align_112/Guillermo_Canas/\n",
            "LFW/lfw_align_112/Guillermo_Canas/Guillermo_Canas_0003.jpg\n",
            "LFW/lfw_align_112/Guillermo_Canas/Guillermo_Canas_0004.jpg\n",
            "LFW/lfw_align_112/Guillermo_Canas/Guillermo_Canas_0001.jpg\n",
            "LFW/lfw_align_112/Guillermo_Canas/Guillermo_Canas_0002.jpg\n",
            "LFW/lfw_align_112/Mark_Mariscal/\n",
            "LFW/lfw_align_112/Mark_Mariscal/Mark_Mariscal_0001.jpg\n",
            "LFW/lfw_align_112/Daniele_Bergamin/\n",
            "LFW/lfw_align_112/Daniele_Bergamin/Daniele_Bergamin_0001.jpg\n",
            "LFW/lfw_align_112/Bela_Karolyi/\n",
            "LFW/lfw_align_112/Bela_Karolyi/Bela_Karolyi_0001.jpg\n",
            "LFW/lfw_align_112/Lawrence_MacAulay/\n",
            "LFW/lfw_align_112/Lawrence_MacAulay/Lawrence_MacAulay_0002.jpg\n",
            "LFW/lfw_align_112/Lawrence_MacAulay/Lawrence_MacAulay_0001.jpg\n",
            "LFW/lfw_align_112/Hal_Sutton/\n",
            "LFW/lfw_align_112/Hal_Sutton/Hal_Sutton_0002.jpg\n",
            "LFW/lfw_align_112/Hal_Sutton/Hal_Sutton_0001.jpg\n",
            "LFW/lfw_align_112/Nina_Jacobson/\n",
            "LFW/lfw_align_112/Nina_Jacobson/Nina_Jacobson_0001.jpg\n",
            "LFW/lfw_align_112/Jean-Luc_Bideau/\n",
            "LFW/lfw_align_112/Jean-Luc_Bideau/Jean-Luc_Bideau_0001.jpg\n",
            "LFW/lfw_align_112/Fred_Wilpon/\n",
            "LFW/lfw_align_112/Fred_Wilpon/Fred_Wilpon_0001.jpg\n",
            "LFW/lfw_align_112/Missy_Crider/\n",
            "LFW/lfw_align_112/Missy_Crider/Missy_Crider_0001.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0002.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0005.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0006.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0007.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0004.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0001.jpg\n",
            "LFW/lfw_align_112/Oscar_De_La_Hoya/Oscar_De_La_Hoya_0003.jpg\n",
            "LFW/lfw_align_112/Percy_Gibson/\n",
            "LFW/lfw_align_112/Percy_Gibson/Percy_Gibson_0001.jpg\n",
            "LFW/lfw_align_112/Mitch_Kupchak/\n",
            "LFW/lfw_align_112/Mitch_Kupchak/Mitch_Kupchak_0001.jpg\n",
            "LFW/lfw_align_112/Donna_Brazile/\n",
            "LFW/lfw_align_112/Donna_Brazile/Donna_Brazile_0001.jpg\n",
            "LFW/lfw_align_112/Askar_Akayev/\n",
            "LFW/lfw_align_112/Askar_Akayev/Askar_Akayev_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Broad/\n",
            "LFW/lfw_align_112/Michael_Broad/Michael_Broad_0001.jpg\n",
            "LFW/lfw_align_112/Kaio_Almeida/\n",
            "LFW/lfw_align_112/Kaio_Almeida/Kaio_Almeida_0001.jpg\n",
            "LFW/lfw_align_112/Carol_Burnett/\n",
            "LFW/lfw_align_112/Carol_Burnett/Carol_Burnett_0002.jpg\n",
            "LFW/lfw_align_112/Carol_Burnett/Carol_Burnett_0001.jpg\n",
            "LFW/lfw_align_112/Bill_Doba/\n",
            "LFW/lfw_align_112/Bill_Doba/Bill_Doba_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Flaherty/\n",
            "LFW/lfw_align_112/Jim_Flaherty/Jim_Flaherty_0001.jpg\n",
            "LFW/lfw_align_112/John_Sweeney/\n",
            "LFW/lfw_align_112/John_Sweeney/John_Sweeney_0001.jpg\n",
            "LFW/lfw_align_112/Evie_Lazarou/\n",
            "LFW/lfw_align_112/Evie_Lazarou/Evie_Lazarou_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0002.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0007.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0006.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0005.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0003.jpg\n",
            "LFW/lfw_align_112/Martin_Scorsese/Martin_Scorsese_0004.jpg\n",
            "LFW/lfw_align_112/Jorge_Batlle/\n",
            "LFW/lfw_align_112/Jorge_Batlle/Jorge_Batlle_0003.jpg\n",
            "LFW/lfw_align_112/Jorge_Batlle/Jorge_Batlle_0002.jpg\n",
            "LFW/lfw_align_112/Jorge_Batlle/Jorge_Batlle_0001.jpg\n",
            "LFW/lfw_align_112/Christopher_Walken/\n",
            "LFW/lfw_align_112/Christopher_Walken/Christopher_Walken_0001.jpg\n",
            "LFW/lfw_align_112/Christopher_Walken/Christopher_Walken_0004.jpg\n",
            "LFW/lfw_align_112/Christopher_Walken/Christopher_Walken_0002.jpg\n",
            "LFW/lfw_align_112/Christopher_Walken/Christopher_Walken_0003.jpg\n",
            "LFW/lfw_align_112/James_Parker/\n",
            "LFW/lfw_align_112/James_Parker/James_Parker_0001.jpg\n",
            "LFW/lfw_align_112/James_Parker/James_Parker_0002.jpg\n",
            "LFW/lfw_align_112/Allan_Kemakeza/\n",
            "LFW/lfw_align_112/Allan_Kemakeza/Allan_Kemakeza_0001.jpg\n",
            "LFW/lfw_align_112/Yoshiyuki_Kamei/\n",
            "LFW/lfw_align_112/Yoshiyuki_Kamei/Yoshiyuki_Kamei_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Olis/\n",
            "LFW/lfw_align_112/Jamie_Olis/Jamie_Olis_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Kagame/\n",
            "LFW/lfw_align_112/Paul_Kagame/Paul_Kagame_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Kagame/Paul_Kagame_0002.jpg\n",
            "LFW/lfw_align_112/Charlie_Zaa/\n",
            "LFW/lfw_align_112/Charlie_Zaa/Charlie_Zaa_0002.jpg\n",
            "LFW/lfw_align_112/Charlie_Zaa/Charlie_Zaa_0001.jpg\n",
            "LFW/lfw_align_112/Sybille_Schmid/\n",
            "LFW/lfw_align_112/Sybille_Schmid/Sybille_Schmid_0001.jpg\n",
            "LFW/lfw_align_112/Glen_DaSilva/\n",
            "LFW/lfw_align_112/Glen_DaSilva/Glen_DaSilva_0001.jpg\n",
            "LFW/lfw_align_112/Horace_Donovan_Reid/\n",
            "LFW/lfw_align_112/Horace_Donovan_Reid/Horace_Donovan_Reid_0001.jpg\n",
            "LFW/lfw_align_112/Federico_Castelan_Sayre/\n",
            "LFW/lfw_align_112/Federico_Castelan_Sayre/Federico_Castelan_Sayre_0001.jpg\n",
            "LFW/lfw_align_112/Shanna_Zolman/\n",
            "LFW/lfw_align_112/Shanna_Zolman/Shanna_Zolman_0001.jpg\n",
            "LFW/lfw_align_112/Ilham_Aliev/\n",
            "LFW/lfw_align_112/Ilham_Aliev/Ilham_Aliev_0001.jpg\n",
            "LFW/lfw_align_112/Marc_Gold/\n",
            "LFW/lfw_align_112/Marc_Gold/Marc_Gold_0001.jpg\n",
            "LFW/lfw_align_112/Jefferson_Perez/\n",
            "LFW/lfw_align_112/Jefferson_Perez/Jefferson_Perez_0001.jpg\n",
            "LFW/lfw_align_112/Jefferson_Perez/Jefferson_Perez_0002.jpg\n",
            "LFW/lfw_align_112/Samuel_Waksal/\n",
            "LFW/lfw_align_112/Samuel_Waksal/Samuel_Waksal_0002.jpg\n",
            "LFW/lfw_align_112/Samuel_Waksal/Samuel_Waksal_0003.jpg\n",
            "LFW/lfw_align_112/Samuel_Waksal/Samuel_Waksal_0004.jpg\n",
            "LFW/lfw_align_112/Samuel_Waksal/Samuel_Waksal_0001.jpg\n",
            "LFW/lfw_align_112/Jesse_Harris/\n",
            "LFW/lfw_align_112/Jesse_Harris/Jesse_Harris_0003.jpg\n",
            "LFW/lfw_align_112/Jesse_Harris/Jesse_Harris_0002.jpg\n",
            "LFW/lfw_align_112/Jesse_Harris/Jesse_Harris_0001.jpg\n",
            "LFW/lfw_align_112/Luca_Cordero_di_Montezemolo/\n",
            "LFW/lfw_align_112/Luca_Cordero_di_Montezemolo/Luca_Cordero_di_Montezemolo_0001.jpg\n",
            "LFW/lfw_align_112/Vadim_Strogalev/\n",
            "LFW/lfw_align_112/Vadim_Strogalev/Vadim_Strogalev_0001.jpg\n",
            "LFW/lfw_align_112/Tim_Robbins/\n",
            "LFW/lfw_align_112/Tim_Robbins/Tim_Robbins_0002.jpg\n",
            "LFW/lfw_align_112/Tim_Robbins/Tim_Robbins_0003.jpg\n",
            "LFW/lfw_align_112/Tim_Robbins/Tim_Robbins_0004.jpg\n",
            "LFW/lfw_align_112/Tim_Robbins/Tim_Robbins_0005.jpg\n",
            "LFW/lfw_align_112/Tim_Robbins/Tim_Robbins_0001.jpg\n",
            "LFW/lfw_align_112/Stephen_Thompson/\n",
            "LFW/lfw_align_112/Stephen_Thompson/Stephen_Thompson_0001.jpg\n",
            "LFW/lfw_align_112/Harry_Kalas/\n",
            "LFW/lfw_align_112/Harry_Kalas/Harry_Kalas_0001.jpg\n",
            "LFW/lfw_align_112/Harry_Kalas/Harry_Kalas_0002.jpg\n",
            "LFW/lfw_align_112/Einars_Repse/\n",
            "LFW/lfw_align_112/Einars_Repse/Einars_Repse_0001.jpg\n",
            "LFW/lfw_align_112/Malcolm_Glazer/\n",
            "LFW/lfw_align_112/Malcolm_Glazer/Malcolm_Glazer_0001.jpg\n",
            "LFW/lfw_align_112/Dan_Duquette/\n",
            "LFW/lfw_align_112/Dan_Duquette/Dan_Duquette_0001.jpg\n",
            "LFW/lfw_align_112/Vernon_Forrest/\n",
            "LFW/lfw_align_112/Vernon_Forrest/Vernon_Forrest_0001.jpg\n",
            "LFW/lfw_align_112/Henry_Hilow/\n",
            "LFW/lfw_align_112/Henry_Hilow/Henry_Hilow_0001.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0003.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0008.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0010.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0002.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0011.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0004.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0006.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0013.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0012.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0001.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0007.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0005.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0009.jpg\n",
            "LFW/lfw_align_112/Dick_Cheney/Dick_Cheney_0014.jpg\n",
            "LFW/lfw_align_112/Vadim_Devyatovskiy/\n",
            "LFW/lfw_align_112/Vadim_Devyatovskiy/Vadim_Devyatovskiy_0001.jpg\n",
            "LFW/lfw_align_112/Tabare_Vazquez/\n",
            "LFW/lfw_align_112/Tabare_Vazquez/Tabare_Vazquez_0001.jpg\n",
            "LFW/lfw_align_112/Claudette_Robinson/\n",
            "LFW/lfw_align_112/Claudette_Robinson/Claudette_Robinson_0001.jpg\n",
            "LFW/lfw_align_112/William_Delahunt/\n",
            "LFW/lfw_align_112/William_Delahunt/William_Delahunt_0001.jpg\n",
            "LFW/lfw_align_112/Thomas_Stewart/\n",
            "LFW/lfw_align_112/Thomas_Stewart/Thomas_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Luis_Berrondo/\n",
            "LFW/lfw_align_112/Luis_Berrondo/Luis_Berrondo_0001.jpg\n",
            "LFW/lfw_align_112/Ali_Bin_Hussein/\n",
            "LFW/lfw_align_112/Ali_Bin_Hussein/Ali_Bin_Hussein_0001.jpg\n",
            "LFW/lfw_align_112/Thierry_Mariani/\n",
            "LFW/lfw_align_112/Thierry_Mariani/Thierry_Mariani_0001.jpg\n",
            "LFW/lfw_align_112/Patricia_Phillips/\n",
            "LFW/lfw_align_112/Patricia_Phillips/Patricia_Phillips_0001.jpg\n",
            "LFW/lfw_align_112/James_Sensenbrenner/\n",
            "LFW/lfw_align_112/James_Sensenbrenner/James_Sensenbrenner_0001.jpg\n",
            "LFW/lfw_align_112/Ryan_Newman/\n",
            "LFW/lfw_align_112/Ryan_Newman/Ryan_Newman_0001.jpg\n",
            "LFW/lfw_align_112/David_Myers/\n",
            "LFW/lfw_align_112/David_Myers/David_Myers_0002.jpg\n",
            "LFW/lfw_align_112/David_Myers/David_Myers_0001.jpg\n",
            "LFW/lfw_align_112/Judith_Nathan/\n",
            "LFW/lfw_align_112/Judith_Nathan/Judith_Nathan_0001.jpg\n",
            "LFW/lfw_align_112/Heidi_Fleiss/\n",
            "LFW/lfw_align_112/Heidi_Fleiss/Heidi_Fleiss_0002.jpg\n",
            "LFW/lfw_align_112/Heidi_Fleiss/Heidi_Fleiss_0001.jpg\n",
            "LFW/lfw_align_112/Heidi_Fleiss/Heidi_Fleiss_0003.jpg\n",
            "LFW/lfw_align_112/Heidi_Fleiss/Heidi_Fleiss_0004.jpg\n",
            "LFW/lfw_align_112/Adriana_Perez_Navarro/\n",
            "LFW/lfw_align_112/Adriana_Perez_Navarro/Adriana_Perez_Navarro_0001.jpg\n",
            "LFW/lfw_align_112/Jose_Canseco_Sr/\n",
            "LFW/lfw_align_112/Jose_Canseco_Sr/Jose_Canseco_Sr_0001.jpg\n",
            "LFW/lfw_align_112/Guus_Hiddink/\n",
            "LFW/lfw_align_112/Guus_Hiddink/Guus_Hiddink_0001.jpg\n",
            "LFW/lfw_align_112/Austin_Kearns/\n",
            "LFW/lfw_align_112/Austin_Kearns/Austin_Kearns_0001.jpg\n",
            "LFW/lfw_align_112/Mary_Jo_Myers/\n",
            "LFW/lfw_align_112/Mary_Jo_Myers/Mary_Jo_Myers_0001.jpg\n",
            "LFW/lfw_align_112/Steven_Seagal/\n",
            "LFW/lfw_align_112/Steven_Seagal/Steven_Seagal_0002.jpg\n",
            "LFW/lfw_align_112/Steven_Seagal/Steven_Seagal_0001.jpg\n",
            "LFW/lfw_align_112/Lisa_Raymond/\n",
            "LFW/lfw_align_112/Lisa_Raymond/Lisa_Raymond_0001.jpg\n",
            "LFW/lfw_align_112/Lisa_Raymond/Lisa_Raymond_0002.jpg\n",
            "LFW/lfw_align_112/Blas_Ople/\n",
            "LFW/lfw_align_112/Blas_Ople/Blas_Ople_0001.jpg\n",
            "LFW/lfw_align_112/Kajsa_Bergqvist/\n",
            "LFW/lfw_align_112/Kajsa_Bergqvist/Kajsa_Bergqvist_0001.jpg\n",
            "LFW/lfw_align_112/Soon_Yi/\n",
            "LFW/lfw_align_112/Soon_Yi/Soon_Yi_0001.jpg\n",
            "LFW/lfw_align_112/John_McCormack/\n",
            "LFW/lfw_align_112/John_McCormack/John_McCormack_0002.jpg\n",
            "LFW/lfw_align_112/John_McCormack/John_McCormack_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Eskridge/\n",
            "LFW/lfw_align_112/Bob_Eskridge/Bob_Eskridge_0001.jpg\n",
            "LFW/lfw_align_112/Nasser_al-Kidwa/\n",
            "LFW/lfw_align_112/Nasser_al-Kidwa/Nasser_al-Kidwa_0002.jpg\n",
            "LFW/lfw_align_112/Nasser_al-Kidwa/Nasser_al-Kidwa_0001.jpg\n",
            "LFW/lfw_align_112/Patrick_Bourrat/\n",
            "LFW/lfw_align_112/Patrick_Bourrat/Patrick_Bourrat_0001.jpg\n",
            "LFW/lfw_align_112/Tono_Suratman/\n",
            "LFW/lfw_align_112/Tono_Suratman/Tono_Suratman_0001.jpg\n",
            "LFW/lfw_align_112/Jay_Leno/\n",
            "LFW/lfw_align_112/Jay_Leno/Jay_Leno_0002.jpg\n",
            "LFW/lfw_align_112/Jay_Leno/Jay_Leno_0001.jpg\n",
            "LFW/lfw_align_112/Jay_Leno/Jay_Leno_0003.jpg\n",
            "LFW/lfw_align_112/Angie_Martinez/\n",
            "LFW/lfw_align_112/Angie_Martinez/Angie_Martinez_0001.jpg\n",
            "LFW/lfw_align_112/Todd_Wike/\n",
            "LFW/lfw_align_112/Todd_Wike/Todd_Wike_0001.jpg\n",
            "LFW/lfw_align_112/Heinrich_Wolfgang/\n",
            "LFW/lfw_align_112/Heinrich_Wolfgang/Heinrich_Wolfgang_0001.jpg\n",
            "LFW/lfw_align_112/Darrell_Porter/\n",
            "LFW/lfw_align_112/Darrell_Porter/Darrell_Porter_0002.jpg\n",
            "LFW/lfw_align_112/Darrell_Porter/Darrell_Porter_0001.jpg\n",
            "LFW/lfw_align_112/Vanessa_Williams/\n",
            "LFW/lfw_align_112/Vanessa_Williams/Vanessa_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Vanessa_Williams/Vanessa_Williams_0002.jpg\n",
            "LFW/lfw_align_112/Vanessa_Williams/Vanessa_Williams_0003.jpg\n",
            "LFW/lfw_align_112/Stefan_Koubek/\n",
            "LFW/lfw_align_112/Stefan_Koubek/Stefan_Koubek_0001.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0002.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0001.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0009.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0004.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0010.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0011.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0014.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0012.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0013.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0006.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0007.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0005.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0008.jpg\n",
            "LFW/lfw_align_112/Yoriko_Kawaguchi/Yoriko_Kawaguchi_0003.jpg\n",
            "LFW/lfw_align_112/Melanie_Griffith/\n",
            "LFW/lfw_align_112/Melanie_Griffith/Melanie_Griffith_0001.jpg\n",
            "LFW/lfw_align_112/Melanie_Griffith/Melanie_Griffith_0003.jpg\n",
            "LFW/lfw_align_112/Melanie_Griffith/Melanie_Griffith_0002.jpg\n",
            "LFW/lfw_align_112/Kenny_Brack/\n",
            "LFW/lfw_align_112/Kenny_Brack/Kenny_Brack_0001.jpg\n",
            "LFW/lfw_align_112/Elena_de_Chavez/\n",
            "LFW/lfw_align_112/Elena_de_Chavez/Elena_de_Chavez_0001.jpg\n",
            "LFW/lfw_align_112/James_Wattana/\n",
            "LFW/lfw_align_112/James_Wattana/James_Wattana_0001.jpg\n",
            "LFW/lfw_align_112/Ramiro_Goben_Reducindo/\n",
            "LFW/lfw_align_112/Ramiro_Goben_Reducindo/Ramiro_Goben_Reducindo_0001.jpg\n",
            "LFW/lfw_align_112/Dexter_Jackson/\n",
            "LFW/lfw_align_112/Dexter_Jackson/Dexter_Jackson_0001.jpg\n",
            "LFW/lfw_align_112/Dexter_Jackson/Dexter_Jackson_0002.jpg\n",
            "LFW/lfw_align_112/Barry_Nakell/\n",
            "LFW/lfw_align_112/Barry_Nakell/Barry_Nakell_0001.jpg\n",
            "LFW/lfw_align_112/Pascal_Affi_Nguessan/\n",
            "LFW/lfw_align_112/Pascal_Affi_Nguessan/Pascal_Affi_Nguessan_0001.jpg\n",
            "LFW/lfw_align_112/Heath_Ledger/\n",
            "LFW/lfw_align_112/Heath_Ledger/Heath_Ledger_0003.jpg\n",
            "LFW/lfw_align_112/Heath_Ledger/Heath_Ledger_0001.jpg\n",
            "LFW/lfw_align_112/Heath_Ledger/Heath_Ledger_0004.jpg\n",
            "LFW/lfw_align_112/Heath_Ledger/Heath_Ledger_0002.jpg\n",
            "LFW/lfw_align_112/Dan_Prinster/\n",
            "LFW/lfw_align_112/Dan_Prinster/Dan_Prinster_0001.jpg\n",
            "LFW/lfw_align_112/Lincoln_Chafee/\n",
            "LFW/lfw_align_112/Lincoln_Chafee/Lincoln_Chafee_0001.jpg\n",
            "LFW/lfw_align_112/Taoufik_Mathlouthi/\n",
            "LFW/lfw_align_112/Taoufik_Mathlouthi/Taoufik_Mathlouthi_0001.jpg\n",
            "LFW/lfw_align_112/Martin_Kristof/\n",
            "LFW/lfw_align_112/Martin_Kristof/Martin_Kristof_0001.jpg\n",
            "LFW/lfw_align_112/Rob_Niedermayer/\n",
            "LFW/lfw_align_112/Rob_Niedermayer/Rob_Niedermayer_0001.jpg\n",
            "LFW/lfw_align_112/Pat_Rochester/\n",
            "LFW/lfw_align_112/Pat_Rochester/Pat_Rochester_0001.jpg\n",
            "LFW/lfw_align_112/Douglas_Meester/\n",
            "LFW/lfw_align_112/Douglas_Meester/Douglas_Meester_0001.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0001.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0011.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0002.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0008.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0009.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0005.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0010.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0004.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0007.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0006.jpg\n",
            "LFW/lfw_align_112/Condoleezza_Rice/Condoleezza_Rice_0003.jpg\n",
            "LFW/lfw_align_112/John_Barnett/\n",
            "LFW/lfw_align_112/John_Barnett/John_Barnett_0001.jpg\n",
            "LFW/lfw_align_112/Nicolas_Cage/\n",
            "LFW/lfw_align_112/Nicolas_Cage/Nicolas_Cage_0003.jpg\n",
            "LFW/lfw_align_112/Nicolas_Cage/Nicolas_Cage_0001.jpg\n",
            "LFW/lfw_align_112/Nicolas_Cage/Nicolas_Cage_0002.jpg\n",
            "LFW/lfw_align_112/Nicolas_Cage/Nicolas_Cage_0004.jpg\n",
            "LFW/lfw_align_112/Sergei_Yushenkov/\n",
            "LFW/lfw_align_112/Sergei_Yushenkov/Sergei_Yushenkov_0001.jpg\n",
            "LFW/lfw_align_112/John_Rosa/\n",
            "LFW/lfw_align_112/John_Rosa/John_Rosa_0003.jpg\n",
            "LFW/lfw_align_112/John_Rosa/John_Rosa_0002.jpg\n",
            "LFW/lfw_align_112/John_Rosa/John_Rosa_0001.jpg\n",
            "LFW/lfw_align_112/Milan_Kucan/\n",
            "LFW/lfw_align_112/Milan_Kucan/Milan_Kucan_0001.jpg\n",
            "LFW/lfw_align_112/Mary_Lou_Markakis/\n",
            "LFW/lfw_align_112/Mary_Lou_Markakis/Mary_Lou_Markakis_0001.jpg\n",
            "LFW/lfw_align_112/Gordon_Campbell/\n",
            "LFW/lfw_align_112/Gordon_Campbell/Gordon_Campbell_0002.jpg\n",
            "LFW/lfw_align_112/Gordon_Campbell/Gordon_Campbell_0001.jpg\n",
            "LFW/lfw_align_112/Jose_Miguel_Aleman/\n",
            "LFW/lfw_align_112/Jose_Miguel_Aleman/Jose_Miguel_Aleman_0001.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0006.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0002.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0003.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0001.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0004.jpg\n",
            "LFW/lfw_align_112/Rick_Perry/Rick_Perry_0005.jpg\n",
            "LFW/lfw_align_112/Gregory_Hines/\n",
            "LFW/lfw_align_112/Gregory_Hines/Gregory_Hines_0002.jpg\n",
            "LFW/lfw_align_112/Gregory_Hines/Gregory_Hines_0001.jpg\n",
            "LFW/lfw_align_112/Sharon_Osbourne/\n",
            "LFW/lfw_align_112/Sharon_Osbourne/Sharon_Osbourne_0001.jpg\n",
            "LFW/lfw_align_112/Sharon_Osbourne/Sharon_Osbourne_0002.jpg\n",
            "LFW/lfw_align_112/Sharon_Osbourne/Sharon_Osbourne_0003.jpg\n",
            "LFW/lfw_align_112/Elvis_Stojko/\n",
            "LFW/lfw_align_112/Elvis_Stojko/Elvis_Stojko_0001.jpg\n",
            "LFW/lfw_align_112/Emily_Robison/\n",
            "LFW/lfw_align_112/Emily_Robison/Emily_Robison_0002.jpg\n",
            "LFW/lfw_align_112/Emily_Robison/Emily_Robison_0001.jpg\n",
            "LFW/lfw_align_112/Veronica_Lake/\n",
            "LFW/lfw_align_112/Veronica_Lake/Veronica_Lake_0001.jpg\n",
            "LFW/lfw_align_112/Leslie_Wiser_Jr/\n",
            "LFW/lfw_align_112/Leslie_Wiser_Jr/Leslie_Wiser_Jr_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Calzaghe/\n",
            "LFW/lfw_align_112/Joe_Calzaghe/Joe_Calzaghe_0002.jpg\n",
            "LFW/lfw_align_112/Joe_Calzaghe/Joe_Calzaghe_0001.jpg\n",
            "LFW/lfw_align_112/Santiago_Botero/\n",
            "LFW/lfw_align_112/Santiago_Botero/Santiago_Botero_0001.jpg\n",
            "LFW/lfw_align_112/Wen_Ho_Lee/\n",
            "LFW/lfw_align_112/Wen_Ho_Lee/Wen_Ho_Lee_0001.jpg\n",
            "LFW/lfw_align_112/Ken_Watanabe/\n",
            "LFW/lfw_align_112/Ken_Watanabe/Ken_Watanabe_0002.jpg\n",
            "LFW/lfw_align_112/Ken_Watanabe/Ken_Watanabe_0001.jpg\n",
            "LFW/lfw_align_112/Jenna_Elfman/\n",
            "LFW/lfw_align_112/Jenna_Elfman/Jenna_Elfman_0001.jpg\n",
            "LFW/lfw_align_112/Pharrell_Williams/\n",
            "LFW/lfw_align_112/Pharrell_Williams/Pharrell_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Hugh_Campbell/\n",
            "LFW/lfw_align_112/Hugh_Campbell/Hugh_Campbell_0001.jpg\n",
            "LFW/lfw_align_112/Eric_Robert_Rudolph/\n",
            "LFW/lfw_align_112/Eric_Robert_Rudolph/Eric_Robert_Rudolph_0003.jpg\n",
            "LFW/lfw_align_112/Eric_Robert_Rudolph/Eric_Robert_Rudolph_0001.jpg\n",
            "LFW/lfw_align_112/Eric_Robert_Rudolph/Eric_Robert_Rudolph_0002.jpg\n",
            "LFW/lfw_align_112/Dave_Campo/\n",
            "LFW/lfw_align_112/Dave_Campo/Dave_Campo_0003.jpg\n",
            "LFW/lfw_align_112/Dave_Campo/Dave_Campo_0001.jpg\n",
            "LFW/lfw_align_112/Dave_Campo/Dave_Campo_0002.jpg\n",
            "LFW/lfw_align_112/Micky_Arison/\n",
            "LFW/lfw_align_112/Micky_Arison/Micky_Arison_0001.jpg\n",
            "LFW/lfw_align_112/Rod_Blagojevich/\n",
            "LFW/lfw_align_112/Rod_Blagojevich/Rod_Blagojevich_0001.jpg\n",
            "LFW/lfw_align_112/Rod_Blagojevich/Rod_Blagojevich_0002.jpg\n",
            "LFW/lfw_align_112/Robert_Korzeniowski/\n",
            "LFW/lfw_align_112/Robert_Korzeniowski/Robert_Korzeniowski_0001.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0007.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0001.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0008.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0004.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0003.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0006.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0002.jpg\n",
            "LFW/lfw_align_112/Jonathan_Edwards/Jonathan_Edwards_0005.jpg\n",
            "LFW/lfw_align_112/Martina_McBride/\n",
            "LFW/lfw_align_112/Martina_McBride/Martina_McBride_0002.jpg\n",
            "LFW/lfw_align_112/Martina_McBride/Martina_McBride_0004.jpg\n",
            "LFW/lfw_align_112/Martina_McBride/Martina_McBride_0001.jpg\n",
            "LFW/lfw_align_112/Martina_McBride/Martina_McBride_0005.jpg\n",
            "LFW/lfw_align_112/Martina_McBride/Martina_McBride_0003.jpg\n",
            "LFW/lfw_align_112/Elton_John/\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0004.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0003.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0005.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0002.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0007.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0006.jpg\n",
            "LFW/lfw_align_112/Elton_John/Elton_John_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Valentine/\n",
            "LFW/lfw_align_112/Steve_Valentine/Steve_Valentine_0001.jpg\n",
            "LFW/lfw_align_112/Anthony_Hazen/\n",
            "LFW/lfw_align_112/Anthony_Hazen/Anthony_Hazen_0001.jpg\n",
            "LFW/lfw_align_112/Alex_Cejka/\n",
            "LFW/lfw_align_112/Alex_Cejka/Alex_Cejka_0001.jpg\n",
            "LFW/lfw_align_112/Elisha_Cuthbert/\n",
            "LFW/lfw_align_112/Elisha_Cuthbert/Elisha_Cuthbert_0001.jpg\n",
            "LFW/lfw_align_112/Maribel_Dominguez/\n",
            "LFW/lfw_align_112/Maribel_Dominguez/Maribel_Dominguez_0001.jpg\n",
            "LFW/lfw_align_112/Ibrahim_Haddad/\n",
            "LFW/lfw_align_112/Ibrahim_Haddad/Ibrahim_Haddad_0001.jpg\n",
            "LFW/lfw_align_112/Joseph_Blatter/\n",
            "LFW/lfw_align_112/Joseph_Blatter/Joseph_Blatter_0002.jpg\n",
            "LFW/lfw_align_112/Joseph_Blatter/Joseph_Blatter_0001.jpg\n",
            "LFW/lfw_align_112/Asif_Ali_Zardari/\n",
            "LFW/lfw_align_112/Asif_Ali_Zardari/Asif_Ali_Zardari_0001.jpg\n",
            "LFW/lfw_align_112/Kate_Lee/\n",
            "LFW/lfw_align_112/Kate_Lee/Kate_Lee_0001.jpg\n",
            "LFW/lfw_align_112/Francis_Ricciardone/\n",
            "LFW/lfw_align_112/Francis_Ricciardone/Francis_Ricciardone_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Mariucci/\n",
            "LFW/lfw_align_112/Steve_Mariucci/Steve_Mariucci_0001.jpg\n",
            "LFW/lfw_align_112/Steve_Mariucci/Steve_Mariucci_0003.jpg\n",
            "LFW/lfw_align_112/Steve_Mariucci/Steve_Mariucci_0002.jpg\n",
            "LFW/lfw_align_112/Luo_Linquan/\n",
            "LFW/lfw_align_112/Luo_Linquan/Luo_Linquan_0001.jpg\n",
            "LFW/lfw_align_112/Wei_Wu/\n",
            "LFW/lfw_align_112/Wei_Wu/Wei_Wu_0001.jpg\n",
            "LFW/lfw_align_112/Daniel_Bruehl/\n",
            "LFW/lfw_align_112/Daniel_Bruehl/Daniel_Bruehl_0001.jpg\n",
            "LFW/lfw_align_112/Nora_Ephron/\n",
            "LFW/lfw_align_112/Nora_Ephron/Nora_Ephron_0001.jpg\n",
            "LFW/lfw_align_112/Janet_Ecker/\n",
            "LFW/lfw_align_112/Janet_Ecker/Janet_Ecker_0001.jpg\n",
            "LFW/lfw_align_112/Wayne_Brady/\n",
            "LFW/lfw_align_112/Wayne_Brady/Wayne_Brady_0001.jpg\n",
            "LFW/lfw_align_112/Woodrow_Stanley/\n",
            "LFW/lfw_align_112/Woodrow_Stanley/Woodrow_Stanley_0001.jpg\n",
            "LFW/lfw_align_112/George_Plimpton/\n",
            "LFW/lfw_align_112/George_Plimpton/George_Plimpton_0001.jpg\n",
            "LFW/lfw_align_112/Earl_Scruggs/\n",
            "LFW/lfw_align_112/Earl_Scruggs/Earl_Scruggs_0001.jpg\n",
            "LFW/lfw_align_112/Rhett_Warrener/\n",
            "LFW/lfw_align_112/Rhett_Warrener/Rhett_Warrener_0001.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0008.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0007.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0003.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0004.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0001.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0002.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0006.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0005.jpg\n",
            "LFW/lfw_align_112/Ray_Romano/Ray_Romano_0009.jpg\n",
            "LFW/lfw_align_112/Anna_Jones/\n",
            "LFW/lfw_align_112/Anna_Jones/Anna_Jones_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Mendes/\n",
            "LFW/lfw_align_112/Joe_Mendes/Joe_Mendes_0001.jpg\n",
            "LFW/lfw_align_112/Stanley_Tong/\n",
            "LFW/lfw_align_112/Stanley_Tong/Stanley_Tong_0001.jpg\n",
            "LFW/lfw_align_112/Stanley_Tong/Stanley_Tong_0002.jpg\n",
            "LFW/lfw_align_112/Nate_Hybl/\n",
            "LFW/lfw_align_112/Nate_Hybl/Nate_Hybl_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Piper/\n",
            "LFW/lfw_align_112/Jim_Piper/Jim_Piper_0001.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0012.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0001.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0014.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0013.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0016.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0008.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0005.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0010.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0015.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0006.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0003.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0004.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0002.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0007.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0011.jpg\n",
            "LFW/lfw_align_112/Trent_Lott/Trent_Lott_0009.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0004.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0001.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0005.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0007.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0008.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0003.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0002.jpg\n",
            "LFW/lfw_align_112/Maria_Shriver/Maria_Shriver_0006.jpg\n",
            "LFW/lfw_align_112/Rupert_Grint/\n",
            "LFW/lfw_align_112/Rupert_Grint/Rupert_Grint_0002.jpg\n",
            "LFW/lfw_align_112/Rupert_Grint/Rupert_Grint_0003.jpg\n",
            "LFW/lfw_align_112/Rupert_Grint/Rupert_Grint_0001.jpg\n",
            "LFW/lfw_align_112/Han_Sung_Joo/\n",
            "LFW/lfw_align_112/Han_Sung_Joo/Han_Sung_Joo_0001.jpg\n",
            "LFW/lfw_align_112/Brent_Coles/\n",
            "LFW/lfw_align_112/Brent_Coles/Brent_Coles_0001.jpg\n",
            "LFW/lfw_align_112/Peter_OToole/\n",
            "LFW/lfw_align_112/Peter_OToole/Peter_OToole_0001.jpg\n",
            "LFW/lfw_align_112/John_Hartson/\n",
            "LFW/lfw_align_112/John_Hartson/John_Hartson_0001.jpg\n",
            "LFW/lfw_align_112/Ricardo_Mayorga/\n",
            "LFW/lfw_align_112/Ricardo_Mayorga/Ricardo_Mayorga_0001.jpg\n",
            "LFW/lfw_align_112/Claudio_Abbado/\n",
            "LFW/lfw_align_112/Claudio_Abbado/Claudio_Abbado_0001.jpg\n",
            "LFW/lfw_align_112/Michael_McNeely/\n",
            "LFW/lfw_align_112/Michael_McNeely/Michael_McNeely_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Villafane/\n",
            "LFW/lfw_align_112/Jamie_Villafane/Jamie_Villafane_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Villafane/Jamie_Villafane_0002.jpg\n",
            "LFW/lfw_align_112/Rollie_Massimino/\n",
            "LFW/lfw_align_112/Rollie_Massimino/Rollie_Massimino_0001.jpg\n",
            "LFW/lfw_align_112/Hashim_Thaci/\n",
            "LFW/lfw_align_112/Hashim_Thaci/Hashim_Thaci_0002.jpg\n",
            "LFW/lfw_align_112/Hashim_Thaci/Hashim_Thaci_0001.jpg\n",
            "LFW/lfw_align_112/Momcilo_Perisic/\n",
            "LFW/lfw_align_112/Momcilo_Perisic/Momcilo_Perisic_0001.jpg\n",
            "LFW/lfw_align_112/Pete_Aldridge/\n",
            "LFW/lfw_align_112/Pete_Aldridge/Pete_Aldridge_0001.jpg\n",
            "LFW/lfw_align_112/Marwan_Muasher/\n",
            "LFW/lfw_align_112/Marwan_Muasher/Marwan_Muasher_0001.jpg\n",
            "LFW/lfw_align_112/Barry_Ford/\n",
            "LFW/lfw_align_112/Barry_Ford/Barry_Ford_0001.jpg\n",
            "LFW/lfw_align_112/Chandrika_Kumaratunga/\n",
            "LFW/lfw_align_112/Chandrika_Kumaratunga/Chandrika_Kumaratunga_0001.jpg\n",
            "LFW/lfw_align_112/Darren_Campel/\n",
            "LFW/lfw_align_112/Darren_Campel/Darren_Campel_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Smothers/\n",
            "LFW/lfw_align_112/Tom_Smothers/Tom_Smothers_0001.jpg\n",
            "LFW/lfw_align_112/Evan_Marriott/\n",
            "LFW/lfw_align_112/Evan_Marriott/Evan_Marriott_0001.jpg\n",
            "LFW/lfw_align_112/Begum_Khaleda_Zia/\n",
            "LFW/lfw_align_112/Begum_Khaleda_Zia/Begum_Khaleda_Zia_0001.jpg\n",
            "LFW/lfw_align_112/Begum_Khaleda_Zia/Begum_Khaleda_Zia_0002.jpg\n",
            "LFW/lfw_align_112/Sargis_Sargsian/\n",
            "LFW/lfw_align_112/Sargis_Sargsian/Sargis_Sargsian_0001.jpg\n",
            "LFW/lfw_align_112/Uzi_Even/\n",
            "LFW/lfw_align_112/Uzi_Even/Uzi_Even_0001.jpg\n",
            "LFW/lfw_align_112/Keith_Tyson/\n",
            "LFW/lfw_align_112/Keith_Tyson/Keith_Tyson_0001.jpg\n",
            "LFW/lfw_align_112/Cristiano_da_Matta/\n",
            "LFW/lfw_align_112/Cristiano_da_Matta/Cristiano_da_Matta_0001.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0005.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0008.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0011.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0010.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0009.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0006.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0004.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0001.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0012.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0002.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0007.jpg\n",
            "LFW/lfw_align_112/Howard_Dean/Howard_Dean_0003.jpg\n",
            "LFW/lfw_align_112/Alejandro_Lembo/\n",
            "LFW/lfw_align_112/Alejandro_Lembo/Alejandro_Lembo_0001.jpg\n",
            "LFW/lfw_align_112/Calvin_Joseph_Coleman/\n",
            "LFW/lfw_align_112/Calvin_Joseph_Coleman/Calvin_Joseph_Coleman_0001.jpg\n",
            "LFW/lfw_align_112/James_Ivory/\n",
            "LFW/lfw_align_112/James_Ivory/James_Ivory_0001.jpg\n",
            "LFW/lfw_align_112/James_Ivory/James_Ivory_0002.jpg\n",
            "LFW/lfw_align_112/Boris_Trajkovski/\n",
            "LFW/lfw_align_112/Boris_Trajkovski/Boris_Trajkovski_0001.jpg\n",
            "LFW/lfw_align_112/Tangra_Riggle/\n",
            "LFW/lfw_align_112/Tangra_Riggle/Tangra_Riggle_0001.jpg\n",
            "LFW/lfw_align_112/Chip_Ganassi/\n",
            "LFW/lfw_align_112/Chip_Ganassi/Chip_Ganassi_0001.jpg\n",
            "LFW/lfw_align_112/Wes_Craven/\n",
            "LFW/lfw_align_112/Wes_Craven/Wes_Craven_0001.jpg\n",
            "LFW/lfw_align_112/Dan_Boyle/\n",
            "LFW/lfw_align_112/Dan_Boyle/Dan_Boyle_0001.jpg\n",
            "LFW/lfw_align_112/Harriet_Lessy/\n",
            "LFW/lfw_align_112/Harriet_Lessy/Harriet_Lessy_0001.jpg\n",
            "LFW/lfw_align_112/Kathryn_Grayson/\n",
            "LFW/lfw_align_112/Kathryn_Grayson/Kathryn_Grayson_0001.jpg\n",
            "LFW/lfw_align_112/Nathan_Doudney/\n",
            "LFW/lfw_align_112/Nathan_Doudney/Nathan_Doudney_0001.jpg\n",
            "LFW/lfw_align_112/Mitchell_Garabedian/\n",
            "LFW/lfw_align_112/Mitchell_Garabedian/Mitchell_Garabedian_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0009.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0010.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0006.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0007.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0008.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0011.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0005.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0012.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0004.jpg\n",
            "LFW/lfw_align_112/Michael_Jackson/Michael_Jackson_0003.jpg\n",
            "LFW/lfw_align_112/Manuel_Jesus/\n",
            "LFW/lfw_align_112/Manuel_Jesus/Manuel_Jesus_0001.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0002.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0009.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0006.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0001.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0008.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0007.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0005.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0003.jpg\n",
            "LFW/lfw_align_112/Zhu_Rongji/Zhu_Rongji_0004.jpg\n",
            "LFW/lfw_align_112/Gil_Cates/\n",
            "LFW/lfw_align_112/Gil_Cates/Gil_Cates_0001.jpg\n",
            "LFW/lfw_align_112/BB_King/\n",
            "LFW/lfw_align_112/BB_King/BB_King_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0018.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0010.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0015.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0005.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0008.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0026.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0012.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0031.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0028.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0004.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0022.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0011.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0019.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0029.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0013.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0032.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0006.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0030.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0025.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0024.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0020.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0016.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0017.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0014.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0023.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0021.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0003.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0002.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0009.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0007.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0033.jpg\n",
            "LFW/lfw_align_112/Tom_Ridge/Tom_Ridge_0027.jpg\n",
            "LFW/lfw_align_112/Horst_Koehler/\n",
            "LFW/lfw_align_112/Horst_Koehler/Horst_Koehler_0002.jpg\n",
            "LFW/lfw_align_112/Horst_Koehler/Horst_Koehler_0003.jpg\n",
            "LFW/lfw_align_112/Horst_Koehler/Horst_Koehler_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Gordon_Card/\n",
            "LFW/lfw_align_112/Robert_Gordon_Card/Robert_Gordon_Card_0001.jpg\n",
            "LFW/lfw_align_112/Kurt_Thomas/\n",
            "LFW/lfw_align_112/Kurt_Thomas/Kurt_Thomas_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Jeffords/\n",
            "LFW/lfw_align_112/Jim_Jeffords/Jim_Jeffords_0001.jpg\n",
            "LFW/lfw_align_112/Dawn_Staley/\n",
            "LFW/lfw_align_112/Dawn_Staley/Dawn_Staley_0001.jpg\n",
            "LFW/lfw_align_112/Nancy_Demme/\n",
            "LFW/lfw_align_112/Nancy_Demme/Nancy_Demme_0001.jpg\n",
            "LFW/lfw_align_112/Nancy_Demme/Nancy_Demme_0002.jpg\n",
            "LFW/lfw_align_112/Leonardo_Fernandez/\n",
            "LFW/lfw_align_112/Leonardo_Fernandez/Leonardo_Fernandez_0001.jpg\n",
            "LFW/lfw_align_112/Beatriz_Merino/\n",
            "LFW/lfw_align_112/Beatriz_Merino/Beatriz_Merino_0001.jpg\n",
            "LFW/lfw_align_112/Mike_Sweeney/\n",
            "LFW/lfw_align_112/Mike_Sweeney/Mike_Sweeney_0001.jpg\n",
            "LFW/lfw_align_112/Nelson_Acosta/\n",
            "LFW/lfw_align_112/Nelson_Acosta/Nelson_Acosta_0001.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0004.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0002.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0005.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0006.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0008.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0003.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0001.jpg\n",
            "LFW/lfw_align_112/Ana_Palacio/Ana_Palacio_0007.jpg\n",
            "LFW/lfw_align_112/Cherry_Jones/\n",
            "LFW/lfw_align_112/Cherry_Jones/Cherry_Jones_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Caine/\n",
            "LFW/lfw_align_112/Michael_Caine/Michael_Caine_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Caine/Michael_Caine_0003.jpg\n",
            "LFW/lfw_align_112/Michael_Caine/Michael_Caine_0004.jpg\n",
            "LFW/lfw_align_112/Michael_Caine/Michael_Caine_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0002.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0005.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0001.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0003.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0006.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0004.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0008.jpg\n",
            "LFW/lfw_align_112/Robert_Redford/Robert_Redford_0007.jpg\n",
            "LFW/lfw_align_112/Vin_Diesel/\n",
            "LFW/lfw_align_112/Vin_Diesel/Vin_Diesel_0002.jpg\n",
            "LFW/lfw_align_112/Vin_Diesel/Vin_Diesel_0001.jpg\n",
            "LFW/lfw_align_112/Ben_Betts/\n",
            "LFW/lfw_align_112/Ben_Betts/Ben_Betts_0001.jpg\n",
            "LFW/lfw_align_112/Johnny_Benson/\n",
            "LFW/lfw_align_112/Johnny_Benson/Johnny_Benson_0001.jpg\n",
            "LFW/lfw_align_112/Mauro_Viza/\n",
            "LFW/lfw_align_112/Mauro_Viza/Mauro_Viza_0001.jpg\n",
            "LFW/lfw_align_112/Bertie_Ahern/\n",
            "LFW/lfw_align_112/Bertie_Ahern/Bertie_Ahern_0005.jpg\n",
            "LFW/lfw_align_112/Bertie_Ahern/Bertie_Ahern_0002.jpg\n",
            "LFW/lfw_align_112/Bertie_Ahern/Bertie_Ahern_0001.jpg\n",
            "LFW/lfw_align_112/Bertie_Ahern/Bertie_Ahern_0004.jpg\n",
            "LFW/lfw_align_112/Bertie_Ahern/Bertie_Ahern_0003.jpg\n",
            "LFW/lfw_align_112/Carol_Williams/\n",
            "LFW/lfw_align_112/Carol_Williams/Carol_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Jamie_Carey/\n",
            "LFW/lfw_align_112/Jamie_Carey/Jamie_Carey_0001.jpg\n",
            "LFW/lfw_align_112/Hugh_Hefner/\n",
            "LFW/lfw_align_112/Hugh_Hefner/Hugh_Hefner_0001.jpg\n",
            "LFW/lfw_align_112/Rainer_Schuettler/\n",
            "LFW/lfw_align_112/Rainer_Schuettler/Rainer_Schuettler_0004.jpg\n",
            "LFW/lfw_align_112/Rainer_Schuettler/Rainer_Schuettler_0003.jpg\n",
            "LFW/lfw_align_112/Rainer_Schuettler/Rainer_Schuettler_0002.jpg\n",
            "LFW/lfw_align_112/Rainer_Schuettler/Rainer_Schuettler_0001.jpg\n",
            "LFW/lfw_align_112/Rainer_Schuettler/Rainer_Schuettler_0005.jpg\n",
            "LFW/lfw_align_112/Ms_Dynamite/\n",
            "LFW/lfw_align_112/Ms_Dynamite/Ms_Dynamite_0001.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0007.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0004.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0009.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0002.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0011.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0008.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0010.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0003.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0012.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0013.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0001.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0014.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0005.jpg\n",
            "LFW/lfw_align_112/Mahathir_Mohamad/Mahathir_Mohamad_0006.jpg\n",
            "LFW/lfw_align_112/Javier_Vargas/\n",
            "LFW/lfw_align_112/Javier_Vargas/Javier_Vargas_0001.jpg\n",
            "LFW/lfw_align_112/Justin_Marshall/\n",
            "LFW/lfw_align_112/Justin_Marshall/Justin_Marshall_0001.jpg\n",
            "LFW/lfw_align_112/Jamir_Miller/\n",
            "LFW/lfw_align_112/Jamir_Miller/Jamir_Miller_0001.jpg\n",
            "LFW/lfw_align_112/Aby_Har-Even/\n",
            "LFW/lfw_align_112/Aby_Har-Even/Aby_Har-Even_0001.jpg\n",
            "LFW/lfw_align_112/Wolfgang_Schuessel/\n",
            "LFW/lfw_align_112/Wolfgang_Schuessel/Wolfgang_Schuessel_0003.jpg\n",
            "LFW/lfw_align_112/Wolfgang_Schuessel/Wolfgang_Schuessel_0004.jpg\n",
            "LFW/lfw_align_112/Wolfgang_Schuessel/Wolfgang_Schuessel_0001.jpg\n",
            "LFW/lfw_align_112/Wolfgang_Schuessel/Wolfgang_Schuessel_0002.jpg\n",
            "LFW/lfw_align_112/Marlene_Weingartner/\n",
            "LFW/lfw_align_112/Marlene_Weingartner/Marlene_Weingartner_0001.jpg\n",
            "LFW/lfw_align_112/Marlene_Weingartner/Marlene_Weingartner_0002.jpg\n",
            "LFW/lfw_align_112/Julien_Varlet/\n",
            "LFW/lfw_align_112/Julien_Varlet/Julien_Varlet_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Bacanovic/\n",
            "LFW/lfw_align_112/Peter_Bacanovic/Peter_Bacanovic_0001.jpg\n",
            "LFW/lfw_align_112/Peter_Bacanovic/Peter_Bacanovic_0002.jpg\n",
            "LFW/lfw_align_112/Christopher_Conyers/\n",
            "LFW/lfw_align_112/Christopher_Conyers/Christopher_Conyers_0001.jpg\n",
            "LFW/lfw_align_112/Mohammed_Al_Hindi/\n",
            "LFW/lfw_align_112/Mohammed_Al_Hindi/Mohammed_Al_Hindi_0001.jpg\n",
            "LFW/lfw_align_112/Dennis_Kozlowski/\n",
            "LFW/lfw_align_112/Dennis_Kozlowski/Dennis_Kozlowski_0002.jpg\n",
            "LFW/lfw_align_112/Dennis_Kozlowski/Dennis_Kozlowski_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Rice/\n",
            "LFW/lfw_align_112/Jerry_Rice/Jerry_Rice_0001.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0004.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0001.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0006.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0005.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0003.jpg\n",
            "LFW/lfw_align_112/Monica_Seles/Monica_Seles_0002.jpg\n",
            "LFW/lfw_align_112/Robin_Williams/\n",
            "LFW/lfw_align_112/Robin_Williams/Robin_Williams_0001.jpg\n",
            "LFW/lfw_align_112/King_Gyanendra/\n",
            "LFW/lfw_align_112/King_Gyanendra/King_Gyanendra_0001.jpg\n",
            "LFW/lfw_align_112/Sally_Clark/\n",
            "LFW/lfw_align_112/Sally_Clark/Sally_Clark_0001.jpg\n",
            "LFW/lfw_align_112/Kurt_Hellstrom/\n",
            "LFW/lfw_align_112/Kurt_Hellstrom/Kurt_Hellstrom_0001.jpg\n",
            "LFW/lfw_align_112/Herb_Dhaliwal/\n",
            "LFW/lfw_align_112/Herb_Dhaliwal/Herb_Dhaliwal_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Scully/\n",
            "LFW/lfw_align_112/Tom_Scully/Tom_Scully_0001.jpg\n",
            "LFW/lfw_align_112/Wilbert_Foy/\n",
            "LFW/lfw_align_112/Wilbert_Foy/Wilbert_Foy_0001.jpg\n",
            "LFW/lfw_align_112/Joseph_Ganim/\n",
            "LFW/lfw_align_112/Joseph_Ganim/Joseph_Ganim_0001.jpg\n",
            "LFW/lfw_align_112/Stacy_Nelson/\n",
            "LFW/lfw_align_112/Stacy_Nelson/Stacy_Nelson_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Garner/\n",
            "LFW/lfw_align_112/Joe_Garner/Joe_Garner_0001.jpg\n",
            "LFW/lfw_align_112/Pat_Summitt/\n",
            "LFW/lfw_align_112/Pat_Summitt/Pat_Summitt_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Denzel/\n",
            "LFW/lfw_align_112/Michael_Denzel/Michael_Denzel_0001.jpg\n",
            "LFW/lfw_align_112/Emily_Stevens/\n",
            "LFW/lfw_align_112/Emily_Stevens/Emily_Stevens_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Nichols/\n",
            "LFW/lfw_align_112/Joe_Nichols/Joe_Nichols_0003.jpg\n",
            "LFW/lfw_align_112/Joe_Nichols/Joe_Nichols_0001.jpg\n",
            "LFW/lfw_align_112/Joe_Nichols/Joe_Nichols_0002.jpg\n",
            "LFW/lfw_align_112/Joe_Nichols/Joe_Nichols_0004.jpg\n",
            "LFW/lfw_align_112/Lena_Olin/\n",
            "LFW/lfw_align_112/Lena_Olin/Lena_Olin_0001.jpg\n",
            "LFW/lfw_align_112/Saoud_Al_Faisal/\n",
            "LFW/lfw_align_112/Saoud_Al_Faisal/Saoud_Al_Faisal_0001.jpg\n",
            "LFW/lfw_align_112/Massoud_Barzani/\n",
            "LFW/lfw_align_112/Massoud_Barzani/Massoud_Barzani_0001.jpg\n",
            "LFW/lfw_align_112/Maritza_Macias_Furano/\n",
            "LFW/lfw_align_112/Maritza_Macias_Furano/Maritza_Macias_Furano_0001.jpg\n",
            "LFW/lfw_align_112/Richard_Harris/\n",
            "LFW/lfw_align_112/Richard_Harris/Richard_Harris_0001.jpg\n",
            "LFW/lfw_align_112/Ivan_Shvedoff/\n",
            "LFW/lfw_align_112/Ivan_Shvedoff/Ivan_Shvedoff_0001.jpg\n",
            "LFW/lfw_align_112/David_Heyman/\n",
            "LFW/lfw_align_112/David_Heyman/David_Heyman_0002.jpg\n",
            "LFW/lfw_align_112/David_Heyman/David_Heyman_0001.jpg\n",
            "LFW/lfw_align_112/Tatiana_Panova/\n",
            "LFW/lfw_align_112/Tatiana_Panova/Tatiana_Panova_0001.jpg\n",
            "LFW/lfw_align_112/James_Gandolfini/\n",
            "LFW/lfw_align_112/James_Gandolfini/James_Gandolfini_0001.jpg\n",
            "LFW/lfw_align_112/James_Gandolfini/James_Gandolfini_0003.jpg\n",
            "LFW/lfw_align_112/James_Gandolfini/James_Gandolfini_0002.jpg\n",
            "LFW/lfw_align_112/Bertrand_Delanoe/\n",
            "LFW/lfw_align_112/Bertrand_Delanoe/Bertrand_Delanoe_0001.jpg\n",
            "LFW/lfw_align_112/Choi_Sung-hong/\n",
            "LFW/lfw_align_112/Choi_Sung-hong/Choi_Sung-hong_0004.jpg\n",
            "LFW/lfw_align_112/Choi_Sung-hong/Choi_Sung-hong_0001.jpg\n",
            "LFW/lfw_align_112/Choi_Sung-hong/Choi_Sung-hong_0002.jpg\n",
            "LFW/lfw_align_112/Choi_Sung-hong/Choi_Sung-hong_0005.jpg\n",
            "LFW/lfw_align_112/Choi_Sung-hong/Choi_Sung-hong_0003.jpg\n",
            "LFW/lfw_align_112/Chok_Tong_Goh/\n",
            "LFW/lfw_align_112/Chok_Tong_Goh/Chok_Tong_Goh_0002.jpg\n",
            "LFW/lfw_align_112/Chok_Tong_Goh/Chok_Tong_Goh_0001.jpg\n",
            "LFW/lfw_align_112/Linda_Mason/\n",
            "LFW/lfw_align_112/Linda_Mason/Linda_Mason_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Andretti/\n",
            "LFW/lfw_align_112/Michael_Andretti/Michael_Andretti_0001.jpg\n",
            "LFW/lfw_align_112/Laura_Elena_Harring/\n",
            "LFW/lfw_align_112/Laura_Elena_Harring/Laura_Elena_Harring_0001.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0003.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0004.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0007.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0001.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0006.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0008.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0002.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0005.jpg\n",
            "LFW/lfw_align_112/Jeong_Se-hyun/Jeong_Se-hyun_0009.jpg\n",
            "LFW/lfw_align_112/Leticia_Dolera/\n",
            "LFW/lfw_align_112/Leticia_Dolera/Leticia_Dolera_0001.jpg\n",
            "LFW/lfw_align_112/Gavin_Degraw/\n",
            "LFW/lfw_align_112/Gavin_Degraw/Gavin_Degraw_0001.jpg\n",
            "LFW/lfw_align_112/Ramon_Ponce_de_Leon/\n",
            "LFW/lfw_align_112/Ramon_Ponce_de_Leon/Ramon_Ponce_de_Leon_0001.jpg\n",
            "LFW/lfw_align_112/Scott_OGrady/\n",
            "LFW/lfw_align_112/Scott_OGrady/Scott_OGrady_0001.jpg\n",
            "LFW/lfw_align_112/Donald_Hays/\n",
            "LFW/lfw_align_112/Donald_Hays/Donald_Hays_0001.jpg\n",
            "LFW/lfw_align_112/Jessica_Capshaw/\n",
            "LFW/lfw_align_112/Jessica_Capshaw/Jessica_Capshaw_0001.jpg\n",
            "LFW/lfw_align_112/Lenny_Wilkens/\n",
            "LFW/lfw_align_112/Lenny_Wilkens/Lenny_Wilkens_0002.jpg\n",
            "LFW/lfw_align_112/Lenny_Wilkens/Lenny_Wilkens_0001.jpg\n",
            "LFW/lfw_align_112/Lenny_Wilkens/Lenny_Wilkens_0003.jpg\n",
            "LFW/lfw_align_112/Suzanne_Somers/\n",
            "LFW/lfw_align_112/Suzanne_Somers/Suzanne_Somers_0001.jpg\n",
            "LFW/lfw_align_112/Serge_Tchuruk/\n",
            "LFW/lfw_align_112/Serge_Tchuruk/Serge_Tchuruk_0001.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0004.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0007.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0006.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0008.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0005.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0002.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0001.jpg\n",
            "LFW/lfw_align_112/Li_Zhaoxing/Li_Zhaoxing_0003.jpg\n",
            "LFW/lfw_align_112/Antony_Leung/\n",
            "LFW/lfw_align_112/Antony_Leung/Antony_Leung_0003.jpg\n",
            "LFW/lfw_align_112/Antony_Leung/Antony_Leung_0004.jpg\n",
            "LFW/lfw_align_112/Antony_Leung/Antony_Leung_0001.jpg\n",
            "LFW/lfw_align_112/Antony_Leung/Antony_Leung_0002.jpg\n",
            "LFW/lfw_align_112/Patsy_Hardy/\n",
            "LFW/lfw_align_112/Patsy_Hardy/Patsy_Hardy_0001.jpg\n",
            "LFW/lfw_align_112/Bruce_Willis/\n",
            "LFW/lfw_align_112/Bruce_Willis/Bruce_Willis_0001.jpg\n",
            "LFW/lfw_align_112/Heather_Willson/\n",
            "LFW/lfw_align_112/Heather_Willson/Heather_Willson_0001.jpg\n",
            "LFW/lfw_align_112/Taku_Yamasaki/\n",
            "LFW/lfw_align_112/Taku_Yamasaki/Taku_Yamasaki_0001.jpg\n",
            "LFW/lfw_align_112/Lee_Hyung-taik/\n",
            "LFW/lfw_align_112/Lee_Hyung-taik/Lee_Hyung-taik_0001.jpg\n",
            "LFW/lfw_align_112/Hushiar_Zebari/\n",
            "LFW/lfw_align_112/Hushiar_Zebari/Hushiar_Zebari_0001.jpg\n",
            "LFW/lfw_align_112/Dionigi_Tettamanzi/\n",
            "LFW/lfw_align_112/Dionigi_Tettamanzi/Dionigi_Tettamanzi_0001.jpg\n",
            "LFW/lfw_align_112/T_Boone_Pickens/\n",
            "LFW/lfw_align_112/T_Boone_Pickens/T_Boone_Pickens_0001.jpg\n",
            "LFW/lfw_align_112/Brendan_Gaughan/\n",
            "LFW/lfw_align_112/Brendan_Gaughan/Brendan_Gaughan_0001.jpg\n",
            "LFW/lfw_align_112/Arantxa_Sanchez-Vicario/\n",
            "LFW/lfw_align_112/Arantxa_Sanchez-Vicario/Arantxa_Sanchez-Vicario_0001.jpg\n",
            "LFW/lfw_align_112/Arantxa_Sanchez-Vicario/Arantxa_Sanchez-Vicario_0002.jpg\n",
            "LFW/lfw_align_112/Paul_Walker/\n",
            "LFW/lfw_align_112/Paul_Walker/Paul_Walker_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Springer/\n",
            "LFW/lfw_align_112/Jerry_Springer/Jerry_Springer_0003.jpg\n",
            "LFW/lfw_align_112/Jerry_Springer/Jerry_Springer_0002.jpg\n",
            "LFW/lfw_align_112/Jerry_Springer/Jerry_Springer_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Springer/Jerry_Springer_0004.jpg\n",
            "LFW/lfw_align_112/Jeffrey_Pfeffer/\n",
            "LFW/lfw_align_112/Jeffrey_Pfeffer/Jeffrey_Pfeffer_0001.jpg\n",
            "LFW/lfw_align_112/Mark_Butcher/\n",
            "LFW/lfw_align_112/Mark_Butcher/Mark_Butcher_0001.jpg\n",
            "LFW/lfw_align_112/Ed_Rosenthal/\n",
            "LFW/lfw_align_112/Ed_Rosenthal/Ed_Rosenthal_0002.jpg\n",
            "LFW/lfw_align_112/Ed_Rosenthal/Ed_Rosenthal_0003.jpg\n",
            "LFW/lfw_align_112/Ed_Rosenthal/Ed_Rosenthal_0001.jpg\n",
            "LFW/lfw_align_112/Masahiko_Nagasawa/\n",
            "LFW/lfw_align_112/Masahiko_Nagasawa/Masahiko_Nagasawa_0001.jpg\n",
            "LFW/lfw_align_112/Masahiko_Nagasawa/Masahiko_Nagasawa_0002.jpg\n",
            "LFW/lfw_align_112/Deena_Burnett/\n",
            "LFW/lfw_align_112/Deena_Burnett/Deena_Burnett_0001.jpg\n",
            "LFW/lfw_align_112/Miguel_Jimenez/\n",
            "LFW/lfw_align_112/Miguel_Jimenez/Miguel_Jimenez_0001.jpg\n",
            "LFW/lfw_align_112/Jeff_Hornacek/\n",
            "LFW/lfw_align_112/Jeff_Hornacek/Jeff_Hornacek_0001.jpg\n",
            "LFW/lfw_align_112/Ralph_Goodale/\n",
            "LFW/lfw_align_112/Ralph_Goodale/Ralph_Goodale_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Donovan/\n",
            "LFW/lfw_align_112/Michael_Donovan/Michael_Donovan_0001.jpg\n",
            "LFW/lfw_align_112/Dan_Kellner/\n",
            "LFW/lfw_align_112/Dan_Kellner/Dan_Kellner_0001.jpg\n",
            "LFW/lfw_align_112/Nida_Blanca/\n",
            "LFW/lfw_align_112/Nida_Blanca/Nida_Blanca_0001.jpg\n",
            "LFW/lfw_align_112/Roberto_Carlos/\n",
            "LFW/lfw_align_112/Roberto_Carlos/Roberto_Carlos_0004.jpg\n",
            "LFW/lfw_align_112/Roberto_Carlos/Roberto_Carlos_0003.jpg\n",
            "LFW/lfw_align_112/Roberto_Carlos/Roberto_Carlos_0002.jpg\n",
            "LFW/lfw_align_112/Roberto_Carlos/Roberto_Carlos_0001.jpg\n",
            "LFW/lfw_align_112/Kate_Richardson/\n",
            "LFW/lfw_align_112/Kate_Richardson/Kate_Richardson_0001.jpg\n",
            "LFW/lfw_align_112/Roberto_Cercelletta/\n",
            "LFW/lfw_align_112/Roberto_Cercelletta/Roberto_Cercelletta_0001.jpg\n",
            "LFW/lfw_align_112/Laura_Bozzo/\n",
            "LFW/lfw_align_112/Laura_Bozzo/Laura_Bozzo_0001.jpg\n",
            "LFW/lfw_align_112/Robin_McLaurin_Williams/\n",
            "LFW/lfw_align_112/Robin_McLaurin_Williams/Robin_McLaurin_Williams_0002.jpg\n",
            "LFW/lfw_align_112/Robin_McLaurin_Williams/Robin_McLaurin_Williams_0001.jpg\n",
            "LFW/lfw_align_112/Yasushi_Akashi/\n",
            "LFW/lfw_align_112/Yasushi_Akashi/Yasushi_Akashi_0001.jpg\n",
            "LFW/lfw_align_112/Tyrone_Medley/\n",
            "LFW/lfw_align_112/Tyrone_Medley/Tyrone_Medley_0001.jpg\n",
            "LFW/lfw_align_112/Dick_Latessa/\n",
            "LFW/lfw_align_112/Dick_Latessa/Dick_Latessa_0001.jpg\n",
            "LFW/lfw_align_112/Dick_Latessa/Dick_Latessa_0002.jpg\n",
            "LFW/lfw_align_112/Mariana_Pollack/\n",
            "LFW/lfw_align_112/Mariana_Pollack/Mariana_Pollack_0002.jpg\n",
            "LFW/lfw_align_112/Mariana_Pollack/Mariana_Pollack_0001.jpg\n",
            "LFW/lfw_align_112/Mariana_Pollack/Mariana_Pollack_0003.jpg\n",
            "LFW/lfw_align_112/Kelvin_Sampson/\n",
            "LFW/lfw_align_112/Kelvin_Sampson/Kelvin_Sampson_0003.jpg\n",
            "LFW/lfw_align_112/Kelvin_Sampson/Kelvin_Sampson_0002.jpg\n",
            "LFW/lfw_align_112/Kelvin_Sampson/Kelvin_Sampson_0001.jpg\n",
            "LFW/lfw_align_112/Carlos_Beltran/\n",
            "LFW/lfw_align_112/Carlos_Beltran/Carlos_Beltran_0001.jpg\n",
            "LFW/lfw_align_112/Jerry_Sloan/\n",
            "LFW/lfw_align_112/Jerry_Sloan/Jerry_Sloan_0001.jpg\n",
            "LFW/lfw_align_112/Simon_Chalk/\n",
            "LFW/lfw_align_112/Simon_Chalk/Simon_Chalk_0001.jpg\n",
            "LFW/lfw_align_112/James_Baker/\n",
            "LFW/lfw_align_112/James_Baker/James_Baker_0001.jpg\n",
            "LFW/lfw_align_112/Leandro_Garcia/\n",
            "LFW/lfw_align_112/Leandro_Garcia/Leandro_Garcia_0001.jpg\n",
            "LFW/lfw_align_112/Laurence_Tribe/\n",
            "LFW/lfw_align_112/Laurence_Tribe/Laurence_Tribe_0001.jpg\n",
            "LFW/lfw_align_112/Andrew_Bunner/\n",
            "LFW/lfw_align_112/Andrew_Bunner/Andrew_Bunner_0002.jpg\n",
            "LFW/lfw_align_112/Andrew_Bunner/Andrew_Bunner_0001.jpg\n",
            "LFW/lfw_align_112/James_Mathis/\n",
            "LFW/lfw_align_112/James_Mathis/James_Mathis_0001.jpg\n",
            "LFW/lfw_align_112/Iain_Anderson/\n",
            "LFW/lfw_align_112/Iain_Anderson/Iain_Anderson_0001.jpg\n",
            "LFW/lfw_align_112/Roy_Blunt/\n",
            "LFW/lfw_align_112/Roy_Blunt/Roy_Blunt_0001.jpg\n",
            "LFW/lfw_align_112/Mario_Alfaro-Lopez/\n",
            "LFW/lfw_align_112/Mario_Alfaro-Lopez/Mario_Alfaro-Lopez_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0006.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0005.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0008.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0001.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0009.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0002.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0003.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0004.jpg\n",
            "LFW/lfw_align_112/Charles_Taylor/Charles_Taylor_0007.jpg\n",
            "LFW/lfw_align_112/Eva_Marie_Saint/\n",
            "LFW/lfw_align_112/Eva_Marie_Saint/Eva_Marie_Saint_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Doer/\n",
            "LFW/lfw_align_112/Gary_Doer/Gary_Doer_0003.jpg\n",
            "LFW/lfw_align_112/Gary_Doer/Gary_Doer_0001.jpg\n",
            "LFW/lfw_align_112/Gary_Doer/Gary_Doer_0002.jpg\n",
            "LFW/lfw_align_112/Rachel_Corrie/\n",
            "LFW/lfw_align_112/Rachel_Corrie/Rachel_Corrie_0001.jpg\n",
            "LFW/lfw_align_112/Milo_Maestrecampo/\n",
            "LFW/lfw_align_112/Milo_Maestrecampo/Milo_Maestrecampo_0001.jpg\n",
            "LFW/lfw_align_112/Milo_Maestrecampo/Milo_Maestrecampo_0003.jpg\n",
            "LFW/lfw_align_112/Milo_Maestrecampo/Milo_Maestrecampo_0002.jpg\n",
            "LFW/lfw_align_112/Joe_DeLamielleure/\n",
            "LFW/lfw_align_112/Joe_DeLamielleure/Joe_DeLamielleure_0001.jpg\n",
            "LFW/lfw_align_112/Aitor_Gonzalez/\n",
            "LFW/lfw_align_112/Aitor_Gonzalez/Aitor_Gonzalez_0001.jpg\n",
            "LFW/lfw_align_112/Aitor_Gonzalez/Aitor_Gonzalez_0002.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0005.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0003.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0006.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0001.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0008.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0004.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0009.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0002.jpg\n",
            "LFW/lfw_align_112/Jimmy_Carter/Jimmy_Carter_0007.jpg\n",
            "LFW/lfw_align_112/Sammy_Knight/\n",
            "LFW/lfw_align_112/Sammy_Knight/Sammy_Knight_0001.jpg\n",
            "LFW/lfw_align_112/Amy_Gale/\n",
            "LFW/lfw_align_112/Amy_Gale/Amy_Gale_0001.jpg\n",
            "LFW/lfw_align_112/Ted_Christopher/\n",
            "LFW/lfw_align_112/Ted_Christopher/Ted_Christopher_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Anderson/\n",
            "LFW/lfw_align_112/Jim_Anderson/Jim_Anderson_0001.jpg\n",
            "LFW/lfw_align_112/Cheryl_James/\n",
            "LFW/lfw_align_112/Cheryl_James/Cheryl_James_0001.jpg\n",
            "LFW/lfw_align_112/Emmanuelle_Jagodsinski/\n",
            "LFW/lfw_align_112/Emmanuelle_Jagodsinski/Emmanuelle_Jagodsinski_0001.jpg\n",
            "LFW/lfw_align_112/Daisy_Fuentes/\n",
            "LFW/lfw_align_112/Daisy_Fuentes/Daisy_Fuentes_0002.jpg\n",
            "LFW/lfw_align_112/Daisy_Fuentes/Daisy_Fuentes_0001.jpg\n",
            "LFW/lfw_align_112/Daisy_Fuentes/Daisy_Fuentes_0004.jpg\n",
            "LFW/lfw_align_112/Daisy_Fuentes/Daisy_Fuentes_0003.jpg\n",
            "LFW/lfw_align_112/Lea_Fastow/\n",
            "LFW/lfw_align_112/Lea_Fastow/Lea_Fastow_0001.jpg\n",
            "LFW/lfw_align_112/Lea_Fastow/Lea_Fastow_0002.jpg\n",
            "LFW/lfw_align_112/Thomas_Gottschalk/\n",
            "LFW/lfw_align_112/Thomas_Gottschalk/Thomas_Gottschalk_0001.jpg\n",
            "LFW/lfw_align_112/Leah_Remini/\n",
            "LFW/lfw_align_112/Leah_Remini/Leah_Remini_0001.jpg\n",
            "LFW/lfw_align_112/Jane_Pauley/\n",
            "LFW/lfw_align_112/Jane_Pauley/Jane_Pauley_0002.jpg\n",
            "LFW/lfw_align_112/Jane_Pauley/Jane_Pauley_0001.jpg\n",
            "LFW/lfw_align_112/Kristen_Breitweiser/\n",
            "LFW/lfw_align_112/Kristen_Breitweiser/Kristen_Breitweiser_0002.jpg\n",
            "LFW/lfw_align_112/Kristen_Breitweiser/Kristen_Breitweiser_0003.jpg\n",
            "LFW/lfw_align_112/Kristen_Breitweiser/Kristen_Breitweiser_0001.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0006.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0007.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0009.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0004.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0008.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0003.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0001.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0005.jpg\n",
            "LFW/lfw_align_112/Kate_Hudson/Kate_Hudson_0002.jpg\n",
            "LFW/lfw_align_112/Jeanne_Moreau/\n",
            "LFW/lfw_align_112/Jeanne_Moreau/Jeanne_Moreau_0001.jpg\n",
            "LFW/lfw_align_112/Jeanne_Moreau/Jeanne_Moreau_0002.jpg\n",
            "LFW/lfw_align_112/Noer_Muis/\n",
            "LFW/lfw_align_112/Noer_Muis/Noer_Muis_0001.jpg\n",
            "LFW/lfw_align_112/Hestrie_Cloette/\n",
            "LFW/lfw_align_112/Hestrie_Cloette/Hestrie_Cloette_0001.jpg\n",
            "LFW/lfw_align_112/Coco_dEste/\n",
            "LFW/lfw_align_112/Coco_dEste/Coco_dEste_0001.jpg\n",
            "LFW/lfw_align_112/Alastair_Campbell/\n",
            "LFW/lfw_align_112/Alastair_Campbell/Alastair_Campbell_0002.jpg\n",
            "LFW/lfw_align_112/Alastair_Campbell/Alastair_Campbell_0004.jpg\n",
            "LFW/lfw_align_112/Alastair_Campbell/Alastair_Campbell_0001.jpg\n",
            "LFW/lfw_align_112/Alastair_Campbell/Alastair_Campbell_0003.jpg\n",
            "LFW/lfw_align_112/Alastair_Campbell/Alastair_Campbell_0005.jpg\n",
            "LFW/lfw_align_112/Marcus_Allen/\n",
            "LFW/lfw_align_112/Marcus_Allen/Marcus_Allen_0001.jpg\n",
            "LFW/lfw_align_112/Manuel_Gehring/\n",
            "LFW/lfw_align_112/Manuel_Gehring/Manuel_Gehring_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Marshall/\n",
            "LFW/lfw_align_112/Frank_Marshall/Frank_Marshall_0001.jpg\n",
            "LFW/lfw_align_112/Lewis_Booth/\n",
            "LFW/lfw_align_112/Lewis_Booth/Lewis_Booth_0001.jpg\n",
            "LFW/lfw_align_112/Dyab_Abou_Jahjah/\n",
            "LFW/lfw_align_112/Dyab_Abou_Jahjah/Dyab_Abou_Jahjah_0001.jpg\n",
            "LFW/lfw_align_112/Jason_Mewes/\n",
            "LFW/lfw_align_112/Jason_Mewes/Jason_Mewes_0001.jpg\n",
            "LFW/lfw_align_112/Matt_Doherty/\n",
            "LFW/lfw_align_112/Matt_Doherty/Matt_Doherty_0003.jpg\n",
            "LFW/lfw_align_112/Matt_Doherty/Matt_Doherty_0001.jpg\n",
            "LFW/lfw_align_112/Matt_Doherty/Matt_Doherty_0002.jpg\n",
            "LFW/lfw_align_112/Jonathan_Byrd/\n",
            "LFW/lfw_align_112/Jonathan_Byrd/Jonathan_Byrd_0001.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0009.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0008.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0007.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0004.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0010.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0006.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0003.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0005.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0002.jpg\n",
            "LFW/lfw_align_112/Jacques_Rogge/Jacques_Rogge_0001.jpg\n",
            "LFW/lfw_align_112/Judi_Dench/\n",
            "LFW/lfw_align_112/Judi_Dench/Judi_Dench_0002.jpg\n",
            "LFW/lfw_align_112/Judi_Dench/Judi_Dench_0001.jpg\n",
            "LFW/lfw_align_112/Fatma_Kusibeh/\n",
            "LFW/lfw_align_112/Fatma_Kusibeh/Fatma_Kusibeh_0001.jpg\n",
            "LFW/lfw_align_112/Janet_Thorpe/\n",
            "LFW/lfw_align_112/Janet_Thorpe/Janet_Thorpe_0002.jpg\n",
            "LFW/lfw_align_112/Janet_Thorpe/Janet_Thorpe_0001.jpg\n",
            "LFW/lfw_align_112/Kyle_Shewfelt/\n",
            "LFW/lfw_align_112/Kyle_Shewfelt/Kyle_Shewfelt_0001.jpg\n",
            "LFW/lfw_align_112/Anibal_Ibarra/\n",
            "LFW/lfw_align_112/Anibal_Ibarra/Anibal_Ibarra_0002.jpg\n",
            "LFW/lfw_align_112/Anibal_Ibarra/Anibal_Ibarra_0003.jpg\n",
            "LFW/lfw_align_112/Anibal_Ibarra/Anibal_Ibarra_0001.jpg\n",
            "LFW/lfw_align_112/Rodney_Rempt/\n",
            "LFW/lfw_align_112/Rodney_Rempt/Rodney_Rempt_0001.jpg\n",
            "LFW/lfw_align_112/AJ_Cook/\n",
            "LFW/lfw_align_112/AJ_Cook/AJ_Cook_0001.jpg\n",
            "LFW/lfw_align_112/Donna_Ralston/\n",
            "LFW/lfw_align_112/Donna_Ralston/Donna_Ralston_0001.jpg\n",
            "LFW/lfw_align_112/John_Danforth/\n",
            "LFW/lfw_align_112/John_Danforth/John_Danforth_0001.jpg\n",
            "LFW/lfw_align_112/Al_Cardenas/\n",
            "LFW/lfw_align_112/Al_Cardenas/Al_Cardenas_0001.jpg\n",
            "LFW/lfw_align_112/Princess_Masako/\n",
            "LFW/lfw_align_112/Princess_Masako/Princess_Masako_0001.jpg\n",
            "LFW/lfw_align_112/Princess_Masako/Princess_Masako_0002.jpg\n",
            "LFW/lfw_align_112/JT_Snow/\n",
            "LFW/lfw_align_112/JT_Snow/JT_Snow_0001.jpg\n",
            "LFW/lfw_align_112/Ken_Kutaragi/\n",
            "LFW/lfw_align_112/Ken_Kutaragi/Ken_Kutaragi_0001.jpg\n",
            "LFW/lfw_align_112/Nadia_Petrova/\n",
            "LFW/lfw_align_112/Nadia_Petrova/Nadia_Petrova_0001.jpg\n",
            "LFW/lfw_align_112/Nadia_Petrova/Nadia_Petrova_0005.jpg\n",
            "LFW/lfw_align_112/Nadia_Petrova/Nadia_Petrova_0004.jpg\n",
            "LFW/lfw_align_112/Nadia_Petrova/Nadia_Petrova_0003.jpg\n",
            "LFW/lfw_align_112/Nadia_Petrova/Nadia_Petrova_0002.jpg\n",
            "LFW/lfw_align_112/Tony_LaRussa/\n",
            "LFW/lfw_align_112/Tony_LaRussa/Tony_LaRussa_0001.jpg\n",
            "LFW/lfw_align_112/John_Philip_Elkann/\n",
            "LFW/lfw_align_112/John_Philip_Elkann/John_Philip_Elkann_0001.jpg\n",
            "LFW/lfw_align_112/Michel_Therrien/\n",
            "LFW/lfw_align_112/Michel_Therrien/Michel_Therrien_0001.jpg\n",
            "LFW/lfw_align_112/Michel_Therrien/Michel_Therrien_0002.jpg\n",
            "LFW/lfw_align_112/Carl_Pope/\n",
            "LFW/lfw_align_112/Carl_Pope/Carl_Pope_0001.jpg\n",
            "LFW/lfw_align_112/Ronald_White/\n",
            "LFW/lfw_align_112/Ronald_White/Ronald_White_0001.jpg\n",
            "LFW/lfw_align_112/Gao_Qiang/\n",
            "LFW/lfw_align_112/Gao_Qiang/Gao_Qiang_0001.jpg\n",
            "LFW/lfw_align_112/Gao_Qiang/Gao_Qiang_0002.jpg\n",
            "LFW/lfw_align_112/Bryant_Young/\n",
            "LFW/lfw_align_112/Bryant_Young/Bryant_Young_0001.jpg\n",
            "LFW/lfw_align_112/Lemuel_Montulo/\n",
            "LFW/lfw_align_112/Lemuel_Montulo/Lemuel_Montulo_0001.jpg\n",
            "LFW/lfw_align_112/Fred_Funk/\n",
            "LFW/lfw_align_112/Fred_Funk/Fred_Funk_0001.jpg\n",
            "LFW/lfw_align_112/Fred_Funk/Fred_Funk_0002.jpg\n",
            "LFW/lfw_align_112/Steven_Kinlock/\n",
            "LFW/lfw_align_112/Steven_Kinlock/Steven_Kinlock_0001.jpg\n",
            "LFW/lfw_align_112/Rich_Brooks/\n",
            "LFW/lfw_align_112/Rich_Brooks/Rich_Brooks_0001.jpg\n",
            "LFW/lfw_align_112/Tyler_Grillo/\n",
            "LFW/lfw_align_112/Tyler_Grillo/Tyler_Grillo_0001.jpg\n",
            "LFW/lfw_align_112/Salman_Khan/\n",
            "LFW/lfw_align_112/Salman_Khan/Salman_Khan_0001.jpg\n",
            "LFW/lfw_align_112/Stella_Keitel/\n",
            "LFW/lfw_align_112/Stella_Keitel/Stella_Keitel_0001.jpg\n",
            "LFW/lfw_align_112/John_Scarlett/\n",
            "LFW/lfw_align_112/John_Scarlett/John_Scarlett_0001.jpg\n",
            "LFW/lfw_align_112/Shaul_Mofaz/\n",
            "LFW/lfw_align_112/Shaul_Mofaz/Shaul_Mofaz_0003.jpg\n",
            "LFW/lfw_align_112/Shaul_Mofaz/Shaul_Mofaz_0001.jpg\n",
            "LFW/lfw_align_112/Shaul_Mofaz/Shaul_Mofaz_0002.jpg\n",
            "LFW/lfw_align_112/Francisco_Maturana/\n",
            "LFW/lfw_align_112/Francisco_Maturana/Francisco_Maturana_0001.jpg\n",
            "LFW/lfw_align_112/Magui_Serna/\n",
            "LFW/lfw_align_112/Magui_Serna/Magui_Serna_0001.jpg\n",
            "LFW/lfw_align_112/Magui_Serna/Magui_Serna_0002.jpg\n",
            "LFW/lfw_align_112/Ben_Glisan/\n",
            "LFW/lfw_align_112/Ben_Glisan/Ben_Glisan_0002.jpg\n",
            "LFW/lfw_align_112/Ben_Glisan/Ben_Glisan_0001.jpg\n",
            "LFW/lfw_align_112/Carlo_Azeglio_Ciampi/\n",
            "LFW/lfw_align_112/Carlo_Azeglio_Ciampi/Carlo_Azeglio_Ciampi_0001.jpg\n",
            "LFW/lfw_align_112/Gore_Vidal/\n",
            "LFW/lfw_align_112/Gore_Vidal/Gore_Vidal_0001.jpg\n",
            "LFW/lfw_align_112/Noer_Moeis/\n",
            "LFW/lfw_align_112/Noer_Moeis/Noer_Moeis_0001.jpg\n",
            "LFW/lfw_align_112/Rachel_Roy/\n",
            "LFW/lfw_align_112/Rachel_Roy/Rachel_Roy_0001.jpg\n",
            "LFW/lfw_align_112/Alexis_Bledel/\n",
            "LFW/lfw_align_112/Alexis_Bledel/Alexis_Bledel_0001.jpg\n",
            "LFW/lfw_align_112/Nick_Markakis/\n",
            "LFW/lfw_align_112/Nick_Markakis/Nick_Markakis_0001.jpg\n",
            "LFW/lfw_align_112/Caroline_Link/\n",
            "LFW/lfw_align_112/Caroline_Link/Caroline_Link_0001.jpg\n",
            "LFW/lfw_align_112/Aline_Chretien/\n",
            "LFW/lfw_align_112/Aline_Chretien/Aline_Chretien_0001.jpg\n",
            "LFW/lfw_align_112/Kevin_Gil/\n",
            "LFW/lfw_align_112/Kevin_Gil/Kevin_Gil_0001.jpg\n",
            "LFW/lfw_align_112/Dragan_Covic/\n",
            "LFW/lfw_align_112/Dragan_Covic/Dragan_Covic_0001.jpg\n",
            "LFW/lfw_align_112/Winston_Churchill/\n",
            "LFW/lfw_align_112/Winston_Churchill/Winston_Churchill_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Cantrell/\n",
            "LFW/lfw_align_112/Bob_Cantrell/Bob_Cantrell_0001.jpg\n",
            "LFW/lfw_align_112/Catherine_Woodard/\n",
            "LFW/lfw_align_112/Catherine_Woodard/Catherine_Woodard_0001.jpg\n",
            "LFW/lfw_align_112/Patricia_Wartusch/\n",
            "LFW/lfw_align_112/Patricia_Wartusch/Patricia_Wartusch_0001.jpg\n",
            "LFW/lfw_align_112/Jorge_Alberto_Galindo/\n",
            "LFW/lfw_align_112/Jorge_Alberto_Galindo/Jorge_Alberto_Galindo_0001.jpg\n",
            "LFW/lfw_align_112/Sean_Hayes/\n",
            "LFW/lfw_align_112/Sean_Hayes/Sean_Hayes_0001.jpg\n",
            "LFW/lfw_align_112/Sean_Hayes/Sean_Hayes_0002.jpg\n",
            "LFW/lfw_align_112/Kai-Uwe_Ricke/\n",
            "LFW/lfw_align_112/Kai-Uwe_Ricke/Kai-Uwe_Ricke_0001.jpg\n",
            "LFW/lfw_align_112/Chhouk_Rin/\n",
            "LFW/lfw_align_112/Chhouk_Rin/Chhouk_Rin_0001.jpg\n",
            "LFW/lfw_align_112/Tom_Miller/\n",
            "LFW/lfw_align_112/Tom_Miller/Tom_Miller_0001.jpg\n",
            "LFW/lfw_align_112/Daniel_Patrick_Moynihan/\n",
            "LFW/lfw_align_112/Daniel_Patrick_Moynihan/Daniel_Patrick_Moynihan_0001.jpg\n",
            "LFW/lfw_align_112/Alain_Cervantes/\n",
            "LFW/lfw_align_112/Alain_Cervantes/Alain_Cervantes_0001.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Taylor/\n",
            "LFW/lfw_align_112/Elizabeth_Taylor/Elizabeth_Taylor_0002.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Taylor/Elizabeth_Taylor_0001.jpg\n",
            "LFW/lfw_align_112/Rafael_Ramirez/\n",
            "LFW/lfw_align_112/Rafael_Ramirez/Rafael_Ramirez_0001.jpg\n",
            "LFW/lfw_align_112/Rafael_Ramirez/Rafael_Ramirez_0003.jpg\n",
            "LFW/lfw_align_112/Rafael_Ramirez/Rafael_Ramirez_0004.jpg\n",
            "LFW/lfw_align_112/Rafael_Ramirez/Rafael_Ramirez_0002.jpg\n",
            "LFW/lfw_align_112/Giulietta_Masina/\n",
            "LFW/lfw_align_112/Giulietta_Masina/Giulietta_Masina_0001.jpg\n",
            "LFW/lfw_align_112/Uzi_Landau/\n",
            "LFW/lfw_align_112/Uzi_Landau/Uzi_Landau_0001.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0006.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0005.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0001.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0004.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0003.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0009.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0007.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0010.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0012.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0011.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0002.jpg\n",
            "LFW/lfw_align_112/Rubens_Barrichello/Rubens_Barrichello_0008.jpg\n",
            "LFW/lfw_align_112/Brenda_Magana/\n",
            "LFW/lfw_align_112/Brenda_Magana/Brenda_Magana_0001.jpg\n",
            "LFW/lfw_align_112/Phil_Bredesen/\n",
            "LFW/lfw_align_112/Phil_Bredesen/Phil_Bredesen_0001.jpg\n",
            "LFW/lfw_align_112/Joanne_Woodward/\n",
            "LFW/lfw_align_112/Joanne_Woodward/Joanne_Woodward_0001.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0012.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0005.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0011.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0009.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0001.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0013.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0006.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0004.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0008.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0007.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0003.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0002.jpg\n",
            "LFW/lfw_align_112/Lucio_Gutierrez/Lucio_Gutierrez_0010.jpg\n",
            "LFW/lfw_align_112/Elizabeth_Regan/\n",
            "LFW/lfw_align_112/Elizabeth_Regan/Elizabeth_Regan_0001.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0004.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0003.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0006.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0007.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0002.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0005.jpg\n",
            "LFW/lfw_align_112/Ben_Affleck/Ben_Affleck_0001.jpg\n",
            "LFW/lfw_align_112/Rich_Gannon/\n",
            "LFW/lfw_align_112/Rich_Gannon/Rich_Gannon_0002.jpg\n",
            "LFW/lfw_align_112/Rich_Gannon/Rich_Gannon_0001.jpg\n",
            "LFW/lfw_align_112/Fernando_Alonso/\n",
            "LFW/lfw_align_112/Fernando_Alonso/Fernando_Alonso_0001.jpg\n",
            "LFW/lfw_align_112/Armando_Calderon_Sol/\n",
            "LFW/lfw_align_112/Armando_Calderon_Sol/Armando_Calderon_Sol_0001.jpg\n",
            "LFW/lfw_align_112/Markus_Beyer/\n",
            "LFW/lfw_align_112/Markus_Beyer/Markus_Beyer_0001.jpg\n",
            "LFW/lfw_align_112/Anderson_Varejao/\n",
            "LFW/lfw_align_112/Anderson_Varejao/Anderson_Varejao_0001.jpg\n",
            "LFW/lfw_align_112/Chris_Noth/\n",
            "LFW/lfw_align_112/Chris_Noth/Chris_Noth_0001.jpg\n",
            "LFW/lfw_align_112/Gunilla_Backman/\n",
            "LFW/lfw_align_112/Gunilla_Backman/Gunilla_Backman_0001.jpg\n",
            "LFW/lfw_align_112/Prem_Kumar_Nair/\n",
            "LFW/lfw_align_112/Prem_Kumar_Nair/Prem_Kumar_Nair_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Edmonds/\n",
            "LFW/lfw_align_112/Jim_Edmonds/Jim_Edmonds_0001.jpg\n",
            "LFW/lfw_align_112/Jim_Edmonds/Jim_Edmonds_0002.jpg\n",
            "LFW/lfw_align_112/Alicia_Keys/\n",
            "LFW/lfw_align_112/Alicia_Keys/Alicia_Keys_0001.jpg\n",
            "LFW/lfw_align_112/Sam_Gerald/\n",
            "LFW/lfw_align_112/Sam_Gerald/Sam_Gerald_0001.jpg\n",
            "LFW/lfw_align_112/Tomas_Enge/\n",
            "LFW/lfw_align_112/Tomas_Enge/Tomas_Enge_0001.jpg\n",
            "LFW/lfw_align_112/Linda_Dano/\n",
            "LFW/lfw_align_112/Linda_Dano/Linda_Dano_0001.jpg\n",
            "LFW/lfw_align_112/Franz_Beckenbauer/\n",
            "LFW/lfw_align_112/Franz_Beckenbauer/Franz_Beckenbauer_0002.jpg\n",
            "LFW/lfw_align_112/Franz_Beckenbauer/Franz_Beckenbauer_0001.jpg\n",
            "LFW/lfw_align_112/Billy_Graham/\n",
            "LFW/lfw_align_112/Billy_Graham/Billy_Graham_0002.jpg\n",
            "LFW/lfw_align_112/Billy_Graham/Billy_Graham_0001.jpg\n",
            "LFW/lfw_align_112/Mary_Anne_Souza/\n",
            "LFW/lfw_align_112/Mary_Anne_Souza/Mary_Anne_Souza_0001.jpg\n",
            "LFW/lfw_align_112/Morris_Watts/\n",
            "LFW/lfw_align_112/Morris_Watts/Morris_Watts_0001.jpg\n",
            "LFW/lfw_align_112/David_Chase/\n",
            "LFW/lfw_align_112/David_Chase/David_Chase_0001.jpg\n",
            "LFW/lfw_align_112/Nathalia_Gillot/\n",
            "LFW/lfw_align_112/Nathalia_Gillot/Nathalia_Gillot_0001.jpg\n",
            "LFW/lfw_align_112/Scott_Gorelick/\n",
            "LFW/lfw_align_112/Scott_Gorelick/Scott_Gorelick_0001.jpg\n",
            "LFW/lfw_align_112/Patrick_Stewart/\n",
            "LFW/lfw_align_112/Patrick_Stewart/Patrick_Stewart_0002.jpg\n",
            "LFW/lfw_align_112/Patrick_Stewart/Patrick_Stewart_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Riley/\n",
            "LFW/lfw_align_112/Bob_Riley/Bob_Riley_0001.jpg\n",
            "LFW/lfw_align_112/Dean_Barkley/\n",
            "LFW/lfw_align_112/Dean_Barkley/Dean_Barkley_0001.jpg\n",
            "LFW/lfw_align_112/Dean_Barkley/Dean_Barkley_0002.jpg\n",
            "LFW/lfw_align_112/Dean_Barkley/Dean_Barkley_0003.jpg\n",
            "LFW/lfw_align_112/Dean_Barkley/Dean_Barkley_0004.jpg\n",
            "LFW/lfw_align_112/Brandon_Inge/\n",
            "LFW/lfw_align_112/Brandon_Inge/Brandon_Inge_0001.jpg\n",
            "LFW/lfw_align_112/John_Wayne/\n",
            "LFW/lfw_align_112/John_Wayne/John_Wayne_0001.jpg\n",
            "LFW/lfw_align_112/Wolfgang_Becker/\n",
            "LFW/lfw_align_112/Wolfgang_Becker/Wolfgang_Becker_0001.jpg\n",
            "LFW/lfw_align_112/Ahmed_Ahmed/\n",
            "LFW/lfw_align_112/Ahmed_Ahmed/Ahmed_Ahmed_0001.jpg\n",
            "LFW/lfw_align_112/Angelica_Romero/\n",
            "LFW/lfw_align_112/Angelica_Romero/Angelica_Romero_0001.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0008.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0003.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0005.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0004.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0015.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0014.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0013.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0006.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0011.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0012.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0001.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0007.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0009.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0002.jpg\n",
            "LFW/lfw_align_112/Dominique_de_Villepin/Dominique_de_Villepin_0010.jpg\n",
            "LFW/lfw_align_112/Ivan_Stambolic/\n",
            "LFW/lfw_align_112/Ivan_Stambolic/Ivan_Stambolic_0001.jpg\n",
            "LFW/lfw_align_112/Shaun_Pollock/\n",
            "LFW/lfw_align_112/Shaun_Pollock/Shaun_Pollock_0001.jpg\n",
            "LFW/lfw_align_112/Mahdi_Al_Bassam/\n",
            "LFW/lfw_align_112/Mahdi_Al_Bassam/Mahdi_Al_Bassam_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Patrick_King/\n",
            "LFW/lfw_align_112/Michael_Patrick_King/Michael_Patrick_King_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Patrick_King/Michael_Patrick_King_0001.jpg\n",
            "LFW/lfw_align_112/Eileen_Spina/\n",
            "LFW/lfw_align_112/Eileen_Spina/Eileen_Spina_0001.jpg\n",
            "LFW/lfw_align_112/Tommy_Robredo/\n",
            "LFW/lfw_align_112/Tommy_Robredo/Tommy_Robredo_0001.jpg\n",
            "LFW/lfw_align_112/Tommy_Robredo/Tommy_Robredo_0002.jpg\n",
            "LFW/lfw_align_112/Tommy_Robredo/Tommy_Robredo_0003.jpg\n",
            "LFW/lfw_align_112/Baburam_Bhattari/\n",
            "LFW/lfw_align_112/Baburam_Bhattari/Baburam_Bhattari_0001.jpg\n",
            "LFW/lfw_align_112/Sam_Torrance/\n",
            "LFW/lfw_align_112/Sam_Torrance/Sam_Torrance_0003.jpg\n",
            "LFW/lfw_align_112/Sam_Torrance/Sam_Torrance_0002.jpg\n",
            "LFW/lfw_align_112/Sam_Torrance/Sam_Torrance_0001.jpg\n",
            "LFW/lfw_align_112/John_Gruden/\n",
            "LFW/lfw_align_112/John_Gruden/John_Gruden_0001.jpg\n",
            "LFW/lfw_align_112/Melana_Scantlin/\n",
            "LFW/lfw_align_112/Melana_Scantlin/Melana_Scantlin_0001.jpg\n",
            "LFW/lfw_align_112/Laurence_Fishburne/\n",
            "LFW/lfw_align_112/Laurence_Fishburne/Laurence_Fishburne_0001.jpg\n",
            "LFW/lfw_align_112/Enrique_Iglesias/\n",
            "LFW/lfw_align_112/Enrique_Iglesias/Enrique_Iglesias_0001.jpg\n",
            "LFW/lfw_align_112/Paul_Kelleher/\n",
            "LFW/lfw_align_112/Paul_Kelleher/Paul_Kelleher_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Van_Ecke/\n",
            "LFW/lfw_align_112/Frank_Van_Ecke/Frank_Van_Ecke_0001.jpg\n",
            "LFW/lfw_align_112/Julio_Cesar_Chavez/\n",
            "LFW/lfw_align_112/Julio_Cesar_Chavez/Julio_Cesar_Chavez_0001.jpg\n",
            "LFW/lfw_align_112/Saman_Shali/\n",
            "LFW/lfw_align_112/Saman_Shali/Saman_Shali_0001.jpg\n",
            "LFW/lfw_align_112/Madonna/\n",
            "LFW/lfw_align_112/Madonna/Madonna_0001.jpg\n",
            "LFW/lfw_align_112/Madonna/Madonna_0004.jpg\n",
            "LFW/lfw_align_112/Madonna/Madonna_0003.jpg\n",
            "LFW/lfw_align_112/Madonna/Madonna_0005.jpg\n",
            "LFW/lfw_align_112/Madonna/Madonna_0002.jpg\n",
            "LFW/lfw_align_112/Daniele_Hypolito/\n",
            "LFW/lfw_align_112/Daniele_Hypolito/Daniele_Hypolito_0001.jpg\n",
            "LFW/lfw_align_112/Ed_Book/\n",
            "LFW/lfw_align_112/Ed_Book/Ed_Book_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Beauprez/\n",
            "LFW/lfw_align_112/Bob_Beauprez/Bob_Beauprez_0001.jpg\n",
            "LFW/lfw_align_112/Bob_Beauprez/Bob_Beauprez_0002.jpg\n",
            "LFW/lfw_align_112/Bob_Ferguson/\n",
            "LFW/lfw_align_112/Bob_Ferguson/Bob_Ferguson_0001.jpg\n",
            "LFW/lfw_align_112/Valentina_Tereshkova/\n",
            "LFW/lfw_align_112/Valentina_Tereshkova/Valentina_Tereshkova_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Phelps/\n",
            "LFW/lfw_align_112/Michael_Phelps/Michael_Phelps_0004.jpg\n",
            "LFW/lfw_align_112/Michael_Phelps/Michael_Phelps_0002.jpg\n",
            "LFW/lfw_align_112/Michael_Phelps/Michael_Phelps_0001.jpg\n",
            "LFW/lfw_align_112/Michael_Phelps/Michael_Phelps_0003.jpg\n",
            "LFW/lfw_align_112/Michael_Phelps/Michael_Phelps_0005.jpg\n",
            "LFW/lfw_align_112/Denzel_Washington/\n",
            "LFW/lfw_align_112/Denzel_Washington/Denzel_Washington_0003.jpg\n",
            "LFW/lfw_align_112/Denzel_Washington/Denzel_Washington_0001.jpg\n",
            "LFW/lfw_align_112/Denzel_Washington/Denzel_Washington_0002.jpg\n",
            "LFW/lfw_align_112/Denzel_Washington/Denzel_Washington_0005.jpg\n",
            "LFW/lfw_align_112/Denzel_Washington/Denzel_Washington_0004.jpg\n",
            "LFW/lfw_align_112/Jayne_Yarris/\n",
            "LFW/lfw_align_112/Jayne_Yarris/Jayne_Yarris_0001.jpg\n",
            "LFW/lfw_align_112/Tara_Dawn_Christensen/\n",
            "LFW/lfw_align_112/Tara_Dawn_Christensen/Tara_Dawn_Christensen_0001.jpg\n",
            "LFW/lfw_align_112/Lawrence_Vito/\n",
            "LFW/lfw_align_112/Lawrence_Vito/Lawrence_Vito_0001.jpg\n",
            "LFW/lfw_align_112/Svend_Robinson/\n",
            "LFW/lfw_align_112/Svend_Robinson/Svend_Robinson_0001.jpg\n",
            "LFW/lfw_align_112/Shia_LaBeouf/\n",
            "LFW/lfw_align_112/Shia_LaBeouf/Shia_LaBeouf_0001.jpg\n",
            "LFW/lfw_align_112/Shia_LaBeouf/Shia_LaBeouf_0002.jpg\n",
            "LFW/lfw_align_112/Frank_Stallone/\n",
            "LFW/lfw_align_112/Frank_Stallone/Frank_Stallone_0001.jpg\n",
            "LFW/lfw_align_112/Frank_Stallone/Frank_Stallone_0002.jpg\n",
            "LFW/lfw_align_112/Peter_Lundgren/\n",
            "LFW/lfw_align_112/Peter_Lundgren/Peter_Lundgren_0001.jpg\n",
            "LFW/pairs.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idd67f1Qi7V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "17a5e9c6-789d-47bd-80cd-e51adc491ad7"
      },
      "source": [
        "!unzip casia-maxpy-clean.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  casia-maxpy-clean.zip\n",
            "   creating: casia-maxpy-clean/\n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.z02  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/casia-maxpy-clean/\n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.z02  \n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.z05  \n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.z05  \n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.z04  \n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.z04  \n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.z03  \n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.z03  \n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.zip  \n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.zip  \n",
            "  inflating: casia-maxpy-clean/CASIA-maxpy-clean.z01  \n",
            "  inflating: __MACOSX/casia-maxpy-clean/._CASIA-maxpy-clean.z01  \n",
            "  inflating: __MACOSX/._casia-maxpy-clean  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYOH-5STjQ1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59fc5c43-3d27-465f-965a-6f9a975e5cdf"
      },
      "source": [
        "!cd casia-maxpy-clean;zip -F CASIA-maxpy-clean.zip --out CASIA-maxpy-clean_fix.zip;unzip CASIA-maxpy-clean_fix.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "  inflating: CASIA-maxpy-clean/4933278/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4933278/033.jpg  \n",
            "   creating: CASIA-maxpy-clean/4936479/\n",
            "  inflating: CASIA-maxpy-clean/4936479/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4936479/045.jpg  \n",
            "   creating: CASIA-maxpy-clean/4939017/\n",
            "  inflating: CASIA-maxpy-clean/4939017/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4939017/047.jpg  \n",
            "   creating: CASIA-maxpy-clean/4941095/\n",
            "  inflating: CASIA-maxpy-clean/4941095/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4941095/009.jpg  \n",
            "   creating: CASIA-maxpy-clean/4947229/\n",
            "  inflating: CASIA-maxpy-clean/4947229/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4947229/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/4958201/\n",
            "  inflating: CASIA-maxpy-clean/4958201/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4958201/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/4959070/\n",
            "  inflating: CASIA-maxpy-clean/4959070/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4959070/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/4968394/\n",
            "  inflating: CASIA-maxpy-clean/4968394/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4968394/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/4972453/\n",
            "  inflating: CASIA-maxpy-clean/4972453/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4972453/032.jpg  \n",
            "   creating: CASIA-maxpy-clean/4976467/\n",
            "  inflating: CASIA-maxpy-clean/4976467/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4976467/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/4978948/\n",
            "  inflating: CASIA-maxpy-clean/4978948/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4978948/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/4990126/\n",
            "  inflating: CASIA-maxpy-clean/4990126/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990126/051.jpg  \n",
            "   creating: CASIA-maxpy-clean/4990557/\n",
            "  inflating: CASIA-maxpy-clean/4990557/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4990557/029.jpg  \n",
            "   creating: CASIA-maxpy-clean/4992243/\n",
            "  inflating: CASIA-maxpy-clean/4992243/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4992243/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/4997946/\n",
            "  inflating: CASIA-maxpy-clean/4997946/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/4997946/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5015107/\n",
            "  inflating: CASIA-maxpy-clean/5015107/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5015107/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5037351/\n",
            "  inflating: CASIA-maxpy-clean/5037351/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037351/029.jpg  \n",
            "   creating: CASIA-maxpy-clean/5037999/\n",
            "  inflating: CASIA-maxpy-clean/5037999/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5037999/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5040896/\n",
            "  inflating: CASIA-maxpy-clean/5040896/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5040896/032.jpg  \n",
            "   creating: CASIA-maxpy-clean/5043972/\n",
            "  inflating: CASIA-maxpy-clean/5043972/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5043972/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5052065/\n",
            "  inflating: CASIA-maxpy-clean/5052065/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052065/026.jpg  \n",
            "   creating: CASIA-maxpy-clean/5052211/\n",
            "  inflating: CASIA-maxpy-clean/5052211/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052211/050.jpg  \n",
            "   creating: CASIA-maxpy-clean/5052472/\n",
            "  inflating: CASIA-maxpy-clean/5052472/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5052472/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5059998/\n",
            "  inflating: CASIA-maxpy-clean/5059998/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5059998/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/5070789/\n",
            "  inflating: CASIA-maxpy-clean/5070789/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/054.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/055.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/056.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5070789/057.jpg  \n",
            "   creating: CASIA-maxpy-clean/5072416/\n",
            "  inflating: CASIA-maxpy-clean/5072416/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5072416/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5073287/\n",
            "  inflating: CASIA-maxpy-clean/5073287/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5073287/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5077481/\n",
            "  inflating: CASIA-maxpy-clean/5077481/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077481/031.jpg  \n",
            "   creating: CASIA-maxpy-clean/5077890/\n",
            "  inflating: CASIA-maxpy-clean/5077890/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5077890/012.jpg  \n",
            "   creating: CASIA-maxpy-clean/5083794/\n",
            "  inflating: CASIA-maxpy-clean/5083794/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5083794/030.jpg  \n",
            "   creating: CASIA-maxpy-clean/5092806/\n",
            "  inflating: CASIA-maxpy-clean/5092806/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5092806/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5096181/\n",
            "  inflating: CASIA-maxpy-clean/5096181/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5096181/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5097279/\n",
            "  inflating: CASIA-maxpy-clean/5097279/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5097279/034.jpg  \n",
            "   creating: CASIA-maxpy-clean/5100367/\n",
            "  inflating: CASIA-maxpy-clean/5100367/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5100367/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5101061/\n",
            "  inflating: CASIA-maxpy-clean/5101061/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5101061/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5103952/\n",
            "  inflating: CASIA-maxpy-clean/5103952/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5103952/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5104893/\n",
            "  inflating: CASIA-maxpy-clean/5104893/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5104893/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5105498/\n",
            "  inflating: CASIA-maxpy-clean/5105498/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5105498/043.jpg  \n",
            "   creating: CASIA-maxpy-clean/5106460/\n",
            "  inflating: CASIA-maxpy-clean/5106460/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5106460/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5110983/\n",
            "  inflating: CASIA-maxpy-clean/5110983/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5110983/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5117177/\n",
            "  inflating: CASIA-maxpy-clean/5117177/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/054.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/055.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5117177/056.jpg  \n",
            "   creating: CASIA-maxpy-clean/5121337/\n",
            "  inflating: CASIA-maxpy-clean/5121337/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5121337/034.jpg  \n",
            "   creating: CASIA-maxpy-clean/5123271/\n",
            "  inflating: CASIA-maxpy-clean/5123271/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5123271/032.jpg  \n",
            "   creating: CASIA-maxpy-clean/5134284/\n",
            "  inflating: CASIA-maxpy-clean/5134284/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5134284/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/5136263/\n",
            "  inflating: CASIA-maxpy-clean/5136263/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5136263/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5142374/\n",
            "  inflating: CASIA-maxpy-clean/5142374/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5142374/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5145425/\n",
            "  inflating: CASIA-maxpy-clean/5145425/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5145425/029.jpg  \n",
            "   creating: CASIA-maxpy-clean/5153308/\n",
            "  inflating: CASIA-maxpy-clean/5153308/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153308/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/5153714/\n",
            "  inflating: CASIA-maxpy-clean/5153714/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5153714/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5157403/\n",
            "  inflating: CASIA-maxpy-clean/5157403/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5157403/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5162684/\n",
            "  inflating: CASIA-maxpy-clean/5162684/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5162684/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/5164828/\n",
            "  inflating: CASIA-maxpy-clean/5164828/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5164828/045.jpg  \n",
            "   creating: CASIA-maxpy-clean/5166378/\n",
            "  inflating: CASIA-maxpy-clean/5166378/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5166378/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5178348/\n",
            "  inflating: CASIA-maxpy-clean/5178348/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5178348/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5182593/\n",
            "  inflating: CASIA-maxpy-clean/5182593/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5182593/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5191227/\n",
            "  inflating: CASIA-maxpy-clean/5191227/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5191227/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/5194953/\n",
            "  inflating: CASIA-maxpy-clean/5194953/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5194953/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5196965/\n",
            "  inflating: CASIA-maxpy-clean/5196965/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5196965/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5197836/\n",
            "  inflating: CASIA-maxpy-clean/5197836/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5197836/036.jpg  \n",
            "   creating: CASIA-maxpy-clean/5202053/\n",
            "  inflating: CASIA-maxpy-clean/5202053/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5202053/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5208973/\n",
            "  inflating: CASIA-maxpy-clean/5208973/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5208973/030.jpg  \n",
            "   creating: CASIA-maxpy-clean/5211815/\n",
            "  inflating: CASIA-maxpy-clean/5211815/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5211815/042.jpg  \n",
            "   creating: CASIA-maxpy-clean/5216165/\n",
            "  inflating: CASIA-maxpy-clean/5216165/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5216165/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5223413/\n",
            "  inflating: CASIA-maxpy-clean/5223413/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5223413/047.jpg  \n",
            "   creating: CASIA-maxpy-clean/5225691/\n",
            "  inflating: CASIA-maxpy-clean/5225691/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5225691/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5227176/\n",
            "  inflating: CASIA-maxpy-clean/5227176/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5227176/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5230942/\n",
            "  inflating: CASIA-maxpy-clean/5230942/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5230942/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5242830/\n",
            "  inflating: CASIA-maxpy-clean/5242830/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5242830/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5243044/\n",
            "  inflating: CASIA-maxpy-clean/5243044/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5243044/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5249697/\n",
            "  inflating: CASIA-maxpy-clean/5249697/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5249697/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/5251016/\n",
            "  inflating: CASIA-maxpy-clean/5251016/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5251016/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/5256067/\n",
            "  inflating: CASIA-maxpy-clean/5256067/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5256067/049.jpg  \n",
            "   creating: CASIA-maxpy-clean/5261198/\n",
            "  inflating: CASIA-maxpy-clean/5261198/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5261198/012.jpg  \n",
            "   creating: CASIA-maxpy-clean/5266282/\n",
            "  inflating: CASIA-maxpy-clean/5266282/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5266282/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/5274598/\n",
            "  inflating: CASIA-maxpy-clean/5274598/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5274598/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5280381/\n",
            "  inflating: CASIA-maxpy-clean/5280381/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5280381/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5283241/\n",
            "  inflating: CASIA-maxpy-clean/5283241/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5283241/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5287030/\n",
            "  inflating: CASIA-maxpy-clean/5287030/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5287030/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5293695/\n",
            "  inflating: CASIA-maxpy-clean/5293695/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5293695/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5306079/\n",
            "  inflating: CASIA-maxpy-clean/5306079/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5306079/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5307554/\n",
            "  inflating: CASIA-maxpy-clean/5307554/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5307554/037.jpg  \n",
            "   creating: CASIA-maxpy-clean/5308070/\n",
            "  inflating: CASIA-maxpy-clean/5308070/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308070/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5308173/\n",
            "  inflating: CASIA-maxpy-clean/5308173/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5308173/030.jpg  \n",
            "   creating: CASIA-maxpy-clean/5327610/\n",
            "  inflating: CASIA-maxpy-clean/5327610/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5327610/033.jpg  \n",
            "   creating: CASIA-maxpy-clean/5336712/\n",
            "  inflating: CASIA-maxpy-clean/5336712/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5336712/024.jpg  \n",
            "   creating: CASIA-maxpy-clean/5342419/\n",
            "  inflating: CASIA-maxpy-clean/5342419/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5342419/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5349303/\n",
            "  inflating: CASIA-maxpy-clean/5349303/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5349303/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5350882/\n",
            "  inflating: CASIA-maxpy-clean/5350882/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5350882/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5356118/\n",
            "  inflating: CASIA-maxpy-clean/5356118/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5356118/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5360404/\n",
            "  inflating: CASIA-maxpy-clean/5360404/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5360404/036.jpg  \n",
            "   creating: CASIA-maxpy-clean/5373057/\n",
            "  inflating: CASIA-maxpy-clean/5373057/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5373057/011.jpg  \n",
            "   creating: CASIA-maxpy-clean/5375322/\n",
            "  inflating: CASIA-maxpy-clean/5375322/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5375322/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5398610/\n",
            "  inflating: CASIA-maxpy-clean/5398610/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5398610/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5399497/\n",
            "  inflating: CASIA-maxpy-clean/5399497/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5399497/011.jpg  \n",
            "   creating: CASIA-maxpy-clean/5423912/\n",
            "  inflating: CASIA-maxpy-clean/5423912/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5423912/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5431622/\n",
            "  inflating: CASIA-maxpy-clean/5431622/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5431622/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5438205/\n",
            "  inflating: CASIA-maxpy-clean/5438205/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438205/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/5438769/\n",
            "  inflating: CASIA-maxpy-clean/5438769/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5438769/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5444355/\n",
            "  inflating: CASIA-maxpy-clean/5444355/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5444355/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/5445402/\n",
            "  inflating: CASIA-maxpy-clean/5445402/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5445402/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5446745/\n",
            "  inflating: CASIA-maxpy-clean/5446745/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5446745/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5449559/\n",
            "  inflating: CASIA-maxpy-clean/5449559/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5449559/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5475949/\n",
            "  inflating: CASIA-maxpy-clean/5475949/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5475949/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5476978/\n",
            "  inflating: CASIA-maxpy-clean/5476978/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5476978/039.jpg  \n",
            "   creating: CASIA-maxpy-clean/5478140/\n",
            "  inflating: CASIA-maxpy-clean/5478140/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5478140/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5486964/\n",
            "  inflating: CASIA-maxpy-clean/5486964/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5486964/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5490181/\n",
            "  inflating: CASIA-maxpy-clean/5490181/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5490181/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5498592/\n",
            "  inflating: CASIA-maxpy-clean/5498592/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5498592/028.jpg  \n",
            "   creating: CASIA-maxpy-clean/5513628/\n",
            "  inflating: CASIA-maxpy-clean/5513628/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5513628/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5540978/\n",
            "  inflating: CASIA-maxpy-clean/5540978/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5540978/039.jpg  \n",
            "   creating: CASIA-maxpy-clean/5544358/\n",
            "  inflating: CASIA-maxpy-clean/5544358/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5544358/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5549723/\n",
            "  inflating: CASIA-maxpy-clean/5549723/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5549723/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5553911/\n",
            "  inflating: CASIA-maxpy-clean/5553911/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5553911/045.jpg  \n",
            "   creating: CASIA-maxpy-clean/5554445/\n",
            "  inflating: CASIA-maxpy-clean/5554445/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5554445/046.jpg  \n",
            "   creating: CASIA-maxpy-clean/5559534/\n",
            "  inflating: CASIA-maxpy-clean/5559534/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559534/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5559738/\n",
            "  inflating: CASIA-maxpy-clean/5559738/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5559738/024.jpg  \n",
            "   creating: CASIA-maxpy-clean/5562809/\n",
            "  inflating: CASIA-maxpy-clean/5562809/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5562809/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5569034/\n",
            "  inflating: CASIA-maxpy-clean/5569034/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569034/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5569981/\n",
            "  inflating: CASIA-maxpy-clean/5569981/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5569981/029.jpg  \n",
            "   creating: CASIA-maxpy-clean/5583008/\n",
            "  inflating: CASIA-maxpy-clean/5583008/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5583008/036.jpg  \n",
            "   creating: CASIA-maxpy-clean/5586718/\n",
            "  inflating: CASIA-maxpy-clean/5586718/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5586718/035.jpg  \n",
            "   creating: CASIA-maxpy-clean/5587543/\n",
            "  inflating: CASIA-maxpy-clean/5587543/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/054.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/056.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/057.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/058.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/059.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/060.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/061.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/062.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/063.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/064.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/066.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/068.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/069.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/070.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5587543/071.jpg  \n",
            "   creating: CASIA-maxpy-clean/5592004/\n",
            "  inflating: CASIA-maxpy-clean/5592004/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5592004/024.jpg  \n",
            "   creating: CASIA-maxpy-clean/5597972/\n",
            "  inflating: CASIA-maxpy-clean/5597972/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5597972/026.jpg  \n",
            "   creating: CASIA-maxpy-clean/5604363/\n",
            "  inflating: CASIA-maxpy-clean/5604363/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5604363/039.jpg  \n",
            "   creating: CASIA-maxpy-clean/5605786/\n",
            "  inflating: CASIA-maxpy-clean/5605786/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5605786/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/5613764/\n",
            "  inflating: CASIA-maxpy-clean/5613764/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5613764/042.jpg  \n",
            "   creating: CASIA-maxpy-clean/5617665/\n",
            "  inflating: CASIA-maxpy-clean/5617665/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5617665/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/5621307/\n",
            "  inflating: CASIA-maxpy-clean/5621307/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5621307/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5625014/\n",
            "  inflating: CASIA-maxpy-clean/5625014/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5625014/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5629608/\n",
            "  inflating: CASIA-maxpy-clean/5629608/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5629608/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5630205/\n",
            "  inflating: CASIA-maxpy-clean/5630205/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5630205/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5652218/\n",
            "  inflating: CASIA-maxpy-clean/5652218/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5652218/040.jpg  \n",
            "   creating: CASIA-maxpy-clean/5655015/\n",
            "  inflating: CASIA-maxpy-clean/5655015/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5655015/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5657590/\n",
            "  inflating: CASIA-maxpy-clean/5657590/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5657590/024.jpg  \n",
            "   creating: CASIA-maxpy-clean/5664432/\n",
            "  inflating: CASIA-maxpy-clean/5664432/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5664432/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5666714/\n",
            "  inflating: CASIA-maxpy-clean/5666714/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5666714/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5675606/\n",
            "  inflating: CASIA-maxpy-clean/5675606/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5675606/032.jpg  \n",
            "   creating: CASIA-maxpy-clean/5680642/\n",
            "  inflating: CASIA-maxpy-clean/5680642/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5680642/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/5681403/\n",
            "  inflating: CASIA-maxpy-clean/5681403/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681403/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5681882/\n",
            "  inflating: CASIA-maxpy-clean/5681882/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5681882/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5693349/\n",
            "  inflating: CASIA-maxpy-clean/5693349/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5693349/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5694385/\n",
            "  inflating: CASIA-maxpy-clean/5694385/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5694385/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5702934/\n",
            "  inflating: CASIA-maxpy-clean/5702934/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5702934/024.jpg  \n",
            "   creating: CASIA-maxpy-clean/5713413/\n",
            "  inflating: CASIA-maxpy-clean/5713413/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5713413/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/5725535/\n",
            "  inflating: CASIA-maxpy-clean/5725535/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725535/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5725655/\n",
            "  inflating: CASIA-maxpy-clean/5725655/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5725655/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5728349/\n",
            "  inflating: CASIA-maxpy-clean/5728349/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5728349/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5734180/\n",
            "  inflating: CASIA-maxpy-clean/5734180/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5734180/020.jpg  \n",
            "   creating: CASIA-maxpy-clean/5743918/\n",
            "  inflating: CASIA-maxpy-clean/5743918/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5743918/029.jpg  \n",
            "   creating: CASIA-maxpy-clean/5768706/\n",
            "  inflating: CASIA-maxpy-clean/5768706/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5768706/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/5786096/\n",
            "  inflating: CASIA-maxpy-clean/5786096/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5786096/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5801165/\n",
            "  inflating: CASIA-maxpy-clean/5801165/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5801165/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5803970/\n",
            "  inflating: CASIA-maxpy-clean/5803970/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5803970/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5804957/\n",
            "  inflating: CASIA-maxpy-clean/5804957/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5804957/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5809867/\n",
            "  inflating: CASIA-maxpy-clean/5809867/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5809867/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/5811827/\n",
            "  inflating: CASIA-maxpy-clean/5811827/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5811827/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5831542/\n",
            "  inflating: CASIA-maxpy-clean/5831542/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5831542/047.jpg  \n",
            "   creating: CASIA-maxpy-clean/5847762/\n",
            "  inflating: CASIA-maxpy-clean/5847762/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5847762/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/5861947/\n",
            "  inflating: CASIA-maxpy-clean/5861947/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5861947/014.jpg  \n",
            "   creating: CASIA-maxpy-clean/5873163/\n",
            "  inflating: CASIA-maxpy-clean/5873163/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873163/012.jpg  \n",
            "   creating: CASIA-maxpy-clean/5873274/\n",
            "  inflating: CASIA-maxpy-clean/5873274/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5873274/012.jpg  \n",
            "   creating: CASIA-maxpy-clean/5875151/\n",
            "  inflating: CASIA-maxpy-clean/5875151/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5875151/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5902240/\n",
            "  inflating: CASIA-maxpy-clean/5902240/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5902240/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/5903584/\n",
            "  inflating: CASIA-maxpy-clean/5903584/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5903584/032.jpg  \n",
            "   creating: CASIA-maxpy-clean/5908190/\n",
            "  inflating: CASIA-maxpy-clean/5908190/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5908190/042.jpg  \n",
            "   creating: CASIA-maxpy-clean/5911712/\n",
            "  inflating: CASIA-maxpy-clean/5911712/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5911712/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5928307/\n",
            "  inflating: CASIA-maxpy-clean/5928307/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5928307/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/5934349/\n",
            "  inflating: CASIA-maxpy-clean/5934349/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5934349/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/5937337/\n",
            "  inflating: CASIA-maxpy-clean/5937337/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5937337/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/5942362/\n",
            "  inflating: CASIA-maxpy-clean/5942362/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/054.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/055.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/056.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/057.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/059.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/060.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/061.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/062.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/063.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/065.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/066.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/067.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/068.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/069.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/070.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/071.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/072.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/073.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/075.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/076.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/077.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/078.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/079.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/080.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/081.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/082.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/083.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/084.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/085.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/086.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/088.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/090.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/092.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/093.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/094.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/095.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5942362/096.jpg  \n",
            "   creating: CASIA-maxpy-clean/5949163/\n",
            "  inflating: CASIA-maxpy-clean/5949163/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5949163/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5961035/\n",
            "  inflating: CASIA-maxpy-clean/5961035/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5961035/013.jpg  \n",
            "   creating: CASIA-maxpy-clean/5971068/\n",
            "  inflating: CASIA-maxpy-clean/5971068/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5971068/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/5997749/\n",
            "  inflating: CASIA-maxpy-clean/5997749/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/5997749/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/6002497/\n",
            "  inflating: CASIA-maxpy-clean/6002497/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6002497/018.jpg  \n",
            "   creating: CASIA-maxpy-clean/6031709/\n",
            "  inflating: CASIA-maxpy-clean/6031709/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6031709/037.jpg  \n",
            "   creating: CASIA-maxpy-clean/6039617/\n",
            "  inflating: CASIA-maxpy-clean/6039617/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6039617/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/6062003/\n",
            "  inflating: CASIA-maxpy-clean/6062003/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6062003/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/6074154/\n",
            "  inflating: CASIA-maxpy-clean/6074154/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6074154/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/6079503/\n",
            "  inflating: CASIA-maxpy-clean/6079503/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6079503/034.jpg  \n",
            "   creating: CASIA-maxpy-clean/6109225/\n",
            "  inflating: CASIA-maxpy-clean/6109225/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6109225/042.jpg  \n",
            "   creating: CASIA-maxpy-clean/6121003/\n",
            "  inflating: CASIA-maxpy-clean/6121003/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6121003/039.jpg  \n",
            "   creating: CASIA-maxpy-clean/6152976/\n",
            "  inflating: CASIA-maxpy-clean/6152976/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6152976/052.jpg  \n",
            "   creating: CASIA-maxpy-clean/6166434/\n",
            "  inflating: CASIA-maxpy-clean/6166434/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6166434/046.jpg  \n",
            "   creating: CASIA-maxpy-clean/6188016/\n",
            "  inflating: CASIA-maxpy-clean/6188016/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6188016/019.jpg  \n",
            "   creating: CASIA-maxpy-clean/6189742/\n",
            "  inflating: CASIA-maxpy-clean/6189742/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6189742/052.jpg  \n",
            "   creating: CASIA-maxpy-clean/6212900/\n",
            "  inflating: CASIA-maxpy-clean/6212900/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6212900/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/6219361/\n",
            "  inflating: CASIA-maxpy-clean/6219361/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6219361/035.jpg  \n",
            "   creating: CASIA-maxpy-clean/6230258/\n",
            "  inflating: CASIA-maxpy-clean/6230258/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6230258/054.jpg  \n",
            "   creating: CASIA-maxpy-clean/6234845/\n",
            "  inflating: CASIA-maxpy-clean/6234845/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6234845/022.jpg  \n",
            "   creating: CASIA-maxpy-clean/6249350/\n",
            "  inflating: CASIA-maxpy-clean/6249350/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6249350/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/6252408/\n",
            "  inflating: CASIA-maxpy-clean/6252408/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6252408/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/6266630/\n",
            "  inflating: CASIA-maxpy-clean/6266630/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6266630/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/6293841/\n",
            "  inflating: CASIA-maxpy-clean/6293841/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6293841/030.jpg  \n",
            "   creating: CASIA-maxpy-clean/6304911/\n",
            "  inflating: CASIA-maxpy-clean/6304911/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6304911/026.jpg  \n",
            "   creating: CASIA-maxpy-clean/6305385/\n",
            "  inflating: CASIA-maxpy-clean/6305385/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6305385/023.jpg  \n",
            "   creating: CASIA-maxpy-clean/6317420/\n",
            "  inflating: CASIA-maxpy-clean/6317420/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6317420/031.jpg  \n",
            "   creating: CASIA-maxpy-clean/6353801/\n",
            "  inflating: CASIA-maxpy-clean/6353801/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6353801/021.jpg  \n",
            "   creating: CASIA-maxpy-clean/6383100/\n",
            "  inflating: CASIA-maxpy-clean/6383100/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6383100/015.jpg  \n",
            "   creating: CASIA-maxpy-clean/6392685/\n",
            "  inflating: CASIA-maxpy-clean/6392685/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6392685/040.jpg  \n",
            "   creating: CASIA-maxpy-clean/6418193/\n",
            "  inflating: CASIA-maxpy-clean/6418193/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6418193/025.jpg  \n",
            "   creating: CASIA-maxpy-clean/6432391/\n",
            "  inflating: CASIA-maxpy-clean/6432391/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6432391/054.jpg  \n",
            "   creating: CASIA-maxpy-clean/6487924/\n",
            "  inflating: CASIA-maxpy-clean/6487924/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6487924/016.jpg  \n",
            "   creating: CASIA-maxpy-clean/6489352/\n",
            "  inflating: CASIA-maxpy-clean/6489352/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6489352/017.jpg  \n",
            "   creating: CASIA-maxpy-clean/6549862/\n",
            "  inflating: CASIA-maxpy-clean/6549862/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6549862/027.jpg  \n",
            "   creating: CASIA-maxpy-clean/6573530/\n",
            "  inflating: CASIA-maxpy-clean/6573530/001.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/002.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/003.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/004.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/005.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/006.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/007.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/008.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/009.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/010.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/011.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/012.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/013.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/014.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/015.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/016.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/017.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/018.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/019.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/020.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/021.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/022.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/023.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/024.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/025.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/026.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/027.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/028.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/029.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/030.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/031.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/032.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/033.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/034.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/035.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/036.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/037.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/038.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/039.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/040.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/041.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/042.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/043.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/044.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/045.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/046.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/047.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/048.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/049.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/050.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/051.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/052.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/053.jpg  \n",
            "  inflating: CASIA-maxpy-clean/6573530/054.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6uuXlM3i9QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/face_recognition')#切换工作目录"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfJr6IfB8t3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python generate_CASIA_WebFace.py\n",
        "python generate_csv_files.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POTR_arB8b5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ed30c08-ed3d-4436-d74a-888d0b11d490"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "ArcFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:22.867628\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:21.465603\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:20.355772\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:19.971218\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:19.142595\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:18.612055\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:17.626167\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch Loss: 19.891716 Accuracy: 0.001208\n",
            "Eval Epoch Average Acc: 0.8163, Average Threshold: 0.1976\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:16.798555\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:17.030262\tAcc:0.011719 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:16.036381\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:15.620205\tAcc:0.011719 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:15.098526\tAcc:0.035156 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:14.842890\tAcc:0.050781 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:14.515158\tAcc:0.019531 LR:0.0010000\n",
            "Train Epoch Loss: 15.773181 Accuracy: 0.024866\n",
            "Eval Epoch Average Acc: 0.8240, Average Threshold: 0.1686\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:13.602710\tAcc:0.046875 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:13.449241\tAcc:0.058594 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:13.485094\tAcc:0.066406 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:12.972017\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:12.328297\tAcc:0.078125 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:12.828059\tAcc:0.089844 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:12.570345\tAcc:0.078125 LR:0.0010000\n",
            "Train Epoch Loss: 13.046335 Accuracy: 0.074398\n",
            "Eval Epoch Average Acc: 0.8325, Average Threshold: 0.1745\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:11.527102\tAcc:0.113281 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:10.895613\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:11.180339\tAcc:0.160156 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:11.010590\tAcc:0.164062 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:11.334859\tAcc:0.128906 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:11.022041\tAcc:0.144531 LR:0.0010000\n",
            "Train Epoch Loss: 11.166580 Accuracy: 0.140513\n",
            "Eval Epoch Average Acc: 0.8407, Average Threshold: 0.1807\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:9.436703\tAcc:0.210938 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:9.180363\tAcc:0.234375 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:8.756492\tAcc:0.269531 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:8.872958\tAcc:0.250000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:8.307193\tAcc:0.269531 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:9.618969\tAcc:0.222656 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:8.976775\tAcc:0.257812 LR:0.0001000\n",
            "Train Epoch Loss: 8.811414 Accuracy: 0.252923\n",
            "Eval Epoch Average Acc: 0.8438, Average Threshold: 0.1697\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:8.440163\tAcc:0.273438 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:8.337809\tAcc:0.300781 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:7.324583\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:8.075409\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:8.137099\tAcc:0.308594 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:8.223244\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:7.895742\tAcc:0.265625 LR:0.0001000\n",
            "Train Epoch Loss: 8.123784 Accuracy: 0.291275\n",
            "Eval Epoch Average Acc: 0.8398, Average Threshold: 0.1748\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:7.359702\tAcc:0.332031 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:7.224883\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:8.257817\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:7.898667\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:7.743490\tAcc:0.316406 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:7.573265\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:7.240183\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch Loss: 7.646946 Accuracy: 0.316172\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1770\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:6.356863\tAcc:0.363281 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:6.973596\tAcc:0.363281 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:6.936415\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:7.149498\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:7.994617\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:7.486259\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:7.755099\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch Loss: 7.235841 Accuracy: 0.336858\n",
            "Eval Epoch Average Acc: 0.8457, Average Threshold: 0.1496\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:6.822321\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:6.706376\tAcc:0.355469 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:7.077533\tAcc:0.347656 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:7.198249\tAcc:0.347656 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:6.702941\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:7.005271\tAcc:0.355469 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:7.063995\tAcc:0.355469 LR:0.0001000\n",
            "Train Epoch Loss: 6.847141 Accuracy: 0.359031\n",
            "Eval Epoch Average Acc: 0.8407, Average Threshold: 0.1569\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:6.871280\tAcc:0.347656 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:6.418736\tAcc:0.394531 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:6.752980\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:7.154964\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:7.277998\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:6.341458\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:6.597529\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch Loss: 6.483549 Accuracy: 0.377564\n",
            "Eval Epoch Average Acc: 0.8400, Average Threshold: 0.1619\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:5.891840\tAcc:0.378906 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:6.158693\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:6.840045\tAcc:0.355469 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:6.134862\tAcc:0.382812 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:6.232522\tAcc:0.414062 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:5.911220\tAcc:0.421875 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:6.150632\tAcc:0.386719 LR:0.0000100\n",
            "Train Epoch Loss: 6.023839 Accuracy: 0.401533\n",
            "Eval Epoch Average Acc: 0.8440, Average Threshold: 0.1665\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:5.815309\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:6.377334\tAcc:0.390625 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:5.983828\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:5.580919\tAcc:0.414062 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:5.766878\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:5.540041\tAcc:0.441406 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:6.455012\tAcc:0.359375 LR:0.0000100\n",
            "Train Epoch Loss: 5.955723 Accuracy: 0.405590\n",
            "Eval Epoch Average Acc: 0.8410, Average Threshold: 0.1805\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:6.248290\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:5.397517\tAcc:0.417969 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:5.873802\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:5.636537\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:6.020479\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:5.944677\tAcc:0.375000 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:5.692834\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch Loss: 5.917184 Accuracy: 0.408469\n",
            "Eval Epoch Average Acc: 0.8407, Average Threshold: 0.1672\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:5.612854\tAcc:0.417969 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:5.668844\tAcc:0.398438 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:5.292967\tAcc:0.433594 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:5.846754\tAcc:0.421875 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:6.172514\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:5.385895\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:6.322068\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch Loss: 5.866247 Accuracy: 0.411334\n",
            "Eval Epoch Average Acc: 0.8397, Average Threshold: 0.1747\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:6.041608\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:5.901807\tAcc:0.378906 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:6.059684\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:5.619018\tAcc:0.425781 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:6.008060\tAcc:0.351562 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:6.105440\tAcc:0.390625 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:6.580018\tAcc:0.382812 LR:0.0000100\n",
            "Train Epoch Loss: 5.826646 Accuracy: 0.414291\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1782\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:5.798703\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:6.217731\tAcc:0.386719 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:5.352933\tAcc:0.421875 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:6.030643\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:5.462921\tAcc:0.390625 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:5.150021\tAcc:0.472656 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:5.591606\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch Loss: 5.770378 Accuracy: 0.416242\n",
            "Eval Epoch Average Acc: 0.8432, Average Threshold: 0.1623\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:5.425724\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:5.300805\tAcc:0.449219 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:5.290052\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:5.912841\tAcc:0.433594 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:5.864562\tAcc:0.398438 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:6.285944\tAcc:0.410156 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:5.607109\tAcc:0.421875 LR:0.0000100\n",
            "Train Epoch Loss: 5.747493 Accuracy: 0.416707\n",
            "Eval Epoch Average Acc: 0.8393, Average Threshold: 0.1627\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:5.698216\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:5.751569\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:5.963572\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:5.132256\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:5.865288\tAcc:0.367188 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:5.454537\tAcc:0.425781 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:6.094058\tAcc:0.382812 LR:0.0000100\n",
            "Train Epoch Loss: 5.693464 Accuracy: 0.420036\n",
            "Eval Epoch Average Acc: 0.8375, Average Threshold: 0.1685\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:6.265967\tAcc:0.363281 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:5.916552\tAcc:0.410156 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:5.711499\tAcc:0.425781 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:5.521827\tAcc:0.425781 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:5.646564\tAcc:0.410156 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:6.014345\tAcc:0.382812 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:6.160782\tAcc:0.378906 LR:0.0000010\n",
            "Train Epoch Loss: 5.654202 Accuracy: 0.422110\n",
            "Eval Epoch Average Acc: 0.8397, Average Threshold: 0.1662\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:5.861562\tAcc:0.355469 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:5.477660\tAcc:0.429688 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:5.936437\tAcc:0.449219 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:5.075725\tAcc:0.449219 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:6.074473\tAcc:0.402344 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:5.389728\tAcc:0.457031 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:5.450737\tAcc:0.390625 LR:0.0000010\n",
            "Train Epoch Loss: 5.655574 Accuracy: 0.421661\n",
            "Eval Epoch Average Acc: 0.8388, Average Threshold: 0.1621\n",
            "Best acc on LFW: 0.8456666666666667, best threshold: 0.14955999999999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iofDpRC2VdkP",
        "colab_type": "text"
      },
      "source": [
        "**ArcFace+ResNet18-IR:**\n",
        "\n",
        "Train Epoch Loss: 5.655574 \n",
        "\n",
        "Accuracy: 0.421661\n",
        "\n",
        "Eval Epoch Average Acc: 0.8388\n",
        "\n",
        "Average Threshold: 0.1621\n",
        "\n",
        "Best acc on LFW: **0.8456666666666667**\n",
        "\n",
        "best threshold: 0.14955999999999997"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhzNm4wdnbrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7046ed2-ed24-4df2-cb1f-5a946a9421cb"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "CosFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:18.020449\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:16.191698\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:15.236010\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:14.292567\tAcc:0.003906 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:14.219253\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:12.924738\tAcc:0.027344 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:13.000652\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch Loss: 14.551601 Accuracy: 0.003484\n",
            "Eval Epoch Average Acc: 0.8075, Average Threshold: 0.2042\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:11.886840\tAcc:0.011719 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:11.144527\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:11.079730\tAcc:0.031250 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:11.127630\tAcc:0.046875 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:10.298020\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:10.297375\tAcc:0.070312 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:9.996297\tAcc:0.042969 LR:0.0010000\n",
            "Train Epoch Loss: 10.973672 Accuracy: 0.041651\n",
            "Eval Epoch Average Acc: 0.8273, Average Threshold: 0.1894\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:9.795833\tAcc:0.066406 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:9.112793\tAcc:0.105469 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:8.770572\tAcc:0.105469 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:8.479033\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:8.676682\tAcc:0.113281 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:8.529053\tAcc:0.156250 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:8.427945\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch Loss: 8.948803 Accuracy: 0.107703\n",
            "Eval Epoch Average Acc: 0.8333, Average Threshold: 0.1833\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:7.734661\tAcc:0.148438 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:7.939458\tAcc:0.171875 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:7.642020\tAcc:0.160156 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:7.695607\tAcc:0.179688 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:7.130504\tAcc:0.195312 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:7.289131\tAcc:0.230469 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:7.138066\tAcc:0.226562 LR:0.0010000\n",
            "Train Epoch Loss: 7.597692 Accuracy: 0.179531\n",
            "Eval Epoch Average Acc: 0.8427, Average Threshold: 0.1808\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:6.153738\tAcc:0.253906 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:6.252666\tAcc:0.257812 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:6.129736\tAcc:0.316406 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:5.655638\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:5.180398\tAcc:0.335938 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:5.858006\tAcc:0.312500 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:5.161996\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch Loss: 5.858468 Accuracy: 0.291925\n",
            "Eval Epoch Average Acc: 0.8452, Average Threshold: 0.1481\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:5.579289\tAcc:0.335938 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:5.733106\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:5.725697\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:5.314414\tAcc:0.324219 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:5.346286\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:5.810139\tAcc:0.332031 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:5.727882\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch Loss: 5.377934 Accuracy: 0.333390\n",
            "Eval Epoch Average Acc: 0.8468, Average Threshold: 0.1612\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:5.312076\tAcc:0.339844 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:4.942011\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:5.155142\tAcc:0.355469 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:5.058909\tAcc:0.363281 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:4.806404\tAcc:0.371094 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:4.974258\tAcc:0.394531 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:5.100555\tAcc:0.371094 LR:0.0001000\n",
            "Train Epoch Loss: 5.046541 Accuracy: 0.357312\n",
            "Eval Epoch Average Acc: 0.8420, Average Threshold: 0.1651\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:4.645562\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:5.044197\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:4.427590\tAcc:0.378906 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:4.784729\tAcc:0.355469 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:4.262586\tAcc:0.410156 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:5.018147\tAcc:0.371094 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:4.610818\tAcc:0.394531 LR:0.0001000\n",
            "Train Epoch Loss: 4.750980 Accuracy: 0.381993\n",
            "Eval Epoch Average Acc: 0.8488, Average Threshold: 0.1647\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:4.421853\tAcc:0.414062 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:4.698864\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:4.975611\tAcc:0.363281 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:4.362293\tAcc:0.417969 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:4.298296\tAcc:0.429688 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:3.876575\tAcc:0.480469 LR:0.0001000\n",
            "Train Epoch Loss: 4.465474 Accuracy: 0.403019\n",
            "Eval Epoch Average Acc: 0.8427, Average Threshold: 0.1685\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:4.394300\tAcc:0.398438 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:3.876529\tAcc:0.457031 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:4.201249\tAcc:0.402344 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:4.089826\tAcc:0.421875 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:3.661726\tAcc:0.484375 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:4.354829\tAcc:0.429688 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:4.670466\tAcc:0.425781 LR:0.0001000\n",
            "Train Epoch Loss: 4.183568 Accuracy: 0.427282\n",
            "Eval Epoch Average Acc: 0.8465, Average Threshold: 0.1672\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:3.745222\tAcc:0.441406 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:3.995516\tAcc:0.453125 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:4.096617\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:4.078920\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:3.634013\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:4.265660\tAcc:0.417969 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:4.285048\tAcc:0.402344 LR:0.0000100\n",
            "Train Epoch Loss: 3.842293 Accuracy: 0.455710\n",
            "Eval Epoch Average Acc: 0.8452, Average Threshold: 0.1508\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:4.011607\tAcc:0.410156 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:3.901202\tAcc:0.449219 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:3.895545\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:3.506145\tAcc:0.488281 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:4.625896\tAcc:0.394531 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:3.788140\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:3.657207\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch Loss: 3.784039 Accuracy: 0.459255\n",
            "Eval Epoch Average Acc: 0.8443, Average Threshold: 0.1502\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:3.546894\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:3.953623\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:3.584941\tAcc:0.449219 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:3.889458\tAcc:0.464844 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:3.468815\tAcc:0.488281 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:3.491992\tAcc:0.480469 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:3.241799\tAcc:0.511719 LR:0.0000100\n",
            "Train Epoch Loss: 3.765309 Accuracy: 0.461856\n",
            "Eval Epoch Average Acc: 0.8435, Average Threshold: 0.1499\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:3.741629\tAcc:0.472656 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:3.479081\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:3.544845\tAcc:0.496094 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:3.930864\tAcc:0.472656 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:3.530982\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:3.974302\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch Loss: 3.725491 Accuracy: 0.464380\n",
            "Eval Epoch Average Acc: 0.8438, Average Threshold: 0.1543\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:3.337371\tAcc:0.488281 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:3.794019\tAcc:0.421875 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:3.677579\tAcc:0.457031 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:3.644073\tAcc:0.480469 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:3.758461\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:3.752914\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:3.850061\tAcc:0.519531 LR:0.0000100\n",
            "Train Epoch Loss: 3.697284 Accuracy: 0.468685\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1606\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:3.301203\tAcc:0.511719 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:3.509428\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:3.541135\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:3.746261\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:3.786164\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:3.946086\tAcc:0.433594 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:3.838878\tAcc:0.441406 LR:0.0000100\n",
            "Train Epoch Loss: 3.657309 Accuracy: 0.471673\n",
            "Eval Epoch Average Acc: 0.8403, Average Threshold: 0.1647\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:3.499000\tAcc:0.453125 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:4.067243\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:4.261359\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:3.803375\tAcc:0.433594 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:3.149712\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:3.968714\tAcc:0.425781 LR:0.0000100\n",
            "Train Epoch Loss: 3.629616 Accuracy: 0.474506\n",
            "Eval Epoch Average Acc: 0.8425, Average Threshold: 0.1529\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:3.589943\tAcc:0.464844 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:3.659410\tAcc:0.488281 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:3.501074\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:3.885072\tAcc:0.457031 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:3.373561\tAcc:0.488281 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:3.632594\tAcc:0.464844 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:3.523175\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch Loss: 3.593370 Accuracy: 0.477820\n",
            "Eval Epoch Average Acc: 0.8392, Average Threshold: 0.1628\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:3.394413\tAcc:0.507812 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:3.760386\tAcc:0.457031 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:3.264380\tAcc:0.500000 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:3.455948\tAcc:0.500000 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:3.627014\tAcc:0.503906 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:3.162452\tAcc:0.500000 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:3.898613\tAcc:0.398438 LR:0.0000010\n",
            "Train Epoch Loss: 3.573205 Accuracy: 0.478253\n",
            "Eval Epoch Average Acc: 0.8400, Average Threshold: 0.1559\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:3.121399\tAcc:0.523438 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:3.693269\tAcc:0.449219 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:3.438032\tAcc:0.437500 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:3.402923\tAcc:0.484375 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:3.717492\tAcc:0.406250 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:2.979445\tAcc:0.535156 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:3.100511\tAcc:0.531250 LR:0.0000010\n",
            "Train Epoch Loss: 3.558539 Accuracy: 0.480715\n",
            "Eval Epoch Average Acc: 0.8410, Average Threshold: 0.1565\n",
            "Best acc on LFW: 0.8488333333333333, best threshold: 0.16470600000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb2B5zzaWdCk",
        "colab_type": "text"
      },
      "source": [
        "**CosFace+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 3.558539\n",
        "\n",
        "Accuracy: 0.480715\n",
        "\n",
        "Eval Epoch Average Acc: 0.8410\n",
        "\n",
        "Average Threshold: 0.1565\n",
        "\n",
        "Best acc on LFW: **0.8488333333333333**\n",
        "\n",
        "best threshold: 0.16470600000000002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNWgIBUoWTKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "70ebfd09-741a-4b42-98ee-8db5ef23a0b4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average_meter.py\t   LFW\t\t\t\tResNet.py\n",
            "CASIA_anno.txt\t\t   LFW_CASIAWebFace_Dataset.py\tSEResNet_IR.py\n",
            "casia-maxpy-clean\t   LFW.tar.gz\t\t\ttrainer.py\n",
            "casia-maxpy-clean.zip\t   loss_function.py\t\ttrain.py\n",
            "checkpoints\t\t   __MACOSX\n",
            "generate_CASIA_WebFace.py  __pycache__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK-Wq9QTUkVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6bcbf415-6e53-4784-8f63-ae51adfdc264"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "SphereFace\n",
            "/content/drive/My Drive/face_recognition/loss_function.py:88: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(self.weight)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:7.131711\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:5.557907\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:4.681879\tAcc:0.167969 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:4.578175\tAcc:0.175781 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:4.031675\tAcc:0.261719 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:3.658443\tAcc:0.335938 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:3.167630\tAcc:0.437500 LR:0.0010000\n",
            "Train Epoch Loss: 4.435069 Accuracy: 0.230533\n",
            "Eval Epoch Average Acc: 0.8052, Average Threshold: 0.1998\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:2.564295\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:2.671263\tAcc:0.464844 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:2.695690\tAcc:0.519531 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:2.542413\tAcc:0.503906 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:2.211786\tAcc:0.605469 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:2.393611\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:1.997351\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch Loss: 2.481102 Accuracy: 0.525277\n",
            "Eval Epoch Average Acc: 0.8252, Average Threshold: 0.1928\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:1.923620\tAcc:0.636719 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.814103\tAcc:0.652344 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.664958\tAcc:0.695312 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:1.772733\tAcc:0.640625 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:1.693761\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:1.668254\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:1.806066\tAcc:0.628906 LR:0.0010000\n",
            "Train Epoch Loss: 1.754905 Accuracy: 0.657645\n",
            "Eval Epoch Average Acc: 0.8295, Average Threshold: 0.1710\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:1.187864\tAcc:0.781250 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:1.055850\tAcc:0.789062 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.452693\tAcc:0.718750 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:1.475286\tAcc:0.750000 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:1.331586\tAcc:0.730469 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.479537\tAcc:0.683594 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.363142\tAcc:0.730469 LR:0.0010000\n",
            "Train Epoch Loss: 1.318986 Accuracy: 0.735775\n",
            "Eval Epoch Average Acc: 0.8405, Average Threshold: 0.1776\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:1.074108\tAcc:0.800781 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.659103\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.564578\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:0.711067\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.815961\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.720138\tAcc:0.832031 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.812112\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch Loss: 0.754902 Accuracy: 0.856360\n",
            "Eval Epoch Average Acc: 0.8475, Average Threshold: 0.1577\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.583388\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.444266\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.588606\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:0.644362\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.727851\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.601688\tAcc:0.886719 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.581255\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch Loss: 0.635676 Accuracy: 0.884462\n",
            "Eval Epoch Average Acc: 0.8463, Average Threshold: 0.1623\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.481625\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.656101\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.589073\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:0.522380\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.572409\tAcc:0.925781 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.595533\tAcc:0.886719 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.614251\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch Loss: 0.560192 Accuracy: 0.900008\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1501\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.500124\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.565009\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.502300\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:0.403754\tAcc:0.941406 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.531626\tAcc:0.902344 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.650588\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.490257\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch Loss: 0.503698 Accuracy: 0.913060\n",
            "Eval Epoch Average Acc: 0.8467, Average Threshold: 0.1508\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.547139\tAcc:0.902344 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.328033\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.405254\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:0.483565\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.432239\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.428884\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.488546\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch Loss: 0.444531 Accuracy: 0.926655\n",
            "Eval Epoch Average Acc: 0.8452, Average Threshold: 0.1570\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.358368\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.340811\tAcc:0.957031 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.336152\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:0.402026\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.414331\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.453994\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.465895\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch Loss: 0.399180 Accuracy: 0.937338\n",
            "Eval Epoch Average Acc: 0.8445, Average Threshold: 0.1600\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.250730\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.287689\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.314388\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:0.352469\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.391809\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.382923\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.421988\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.336647 Accuracy: 0.951475\n",
            "Eval Epoch Average Acc: 0.8435, Average Threshold: 0.1528\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.353833\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.333515\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.388153\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:0.278636\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.336497\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.238128\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.399766\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch Loss: 0.350797 Accuracy: 0.950747\n",
            "Eval Epoch Average Acc: 0.8470, Average Threshold: 0.1556\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.297650\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.319505\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.299246\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:0.267802\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.332518\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.401307\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.350156\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch Loss: 0.364128 Accuracy: 0.947790\n",
            "Eval Epoch Average Acc: 0.8458, Average Threshold: 0.1595\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.292471\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.430889\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.371449\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:0.431691\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.329300\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.511421\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.387463\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.383314 Accuracy: 0.944321\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1575\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.378027\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.292890\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.487422\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:0.576209\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.519943\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.463872\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.506862\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch Loss: 0.410835 Accuracy: 0.940420\n",
            "Eval Epoch Average Acc: 0.8453, Average Threshold: 0.1583\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.415946\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.363530\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.389001\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:0.501465\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.592164\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.509935\tAcc:0.902344 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.386953\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.432022 Accuracy: 0.936162\n",
            "Eval Epoch Average Acc: 0.8455, Average Threshold: 0.1499\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.470648\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.541019\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.614521\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:0.482263\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.454716\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.436950\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.527389\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch Loss: 0.461119 Accuracy: 0.930046\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.462029\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.482174\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.379660\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:0.418697\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.555933\tAcc:0.902344 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.442666\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.441695\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch Loss: 0.497644 Accuracy: 0.922629\n",
            "Eval Epoch Average Acc: 0.8455, Average Threshold: 0.1440\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.437225\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.416057\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.588180\tAcc:0.902344 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:0.402348\tAcc:0.941406 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.694199\tAcc:0.898438 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.567822\tAcc:0.882812 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.532865\tAcc:0.917969 LR:0.0000010\n",
            "Train Epoch Loss: 0.532130 Accuracy: 0.914903\n",
            "Eval Epoch Average Acc: 0.8475, Average Threshold: 0.1493\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.609308\tAcc:0.878906 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.454613\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.740356\tAcc:0.855469 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:0.474429\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.575013\tAcc:0.910156 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.530636\tAcc:0.917969 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.539226\tAcc:0.894531 LR:0.0000010\n",
            "Train Epoch Loss: 0.586987 Accuracy: 0.902764\n",
            "Eval Epoch Average Acc: 0.8467, Average Threshold: 0.1472\n",
            "Best acc on LFW: 0.8474999999999999, best threshold: 0.1576565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BgkdSxRW_yA",
        "colab_type": "text"
      },
      "source": [
        "**SphereFace+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.586987\n",
        "\n",
        "Accuracy: 0.902764\n",
        "\n",
        "Eval Epoch Average Acc: 0.8467\n",
        "\n",
        "Average Threshold: 0.1472\n",
        "\n",
        "Best acc on LFW: **0.8474999999999999**\n",
        "\n",
        "best threshold: 0.1576565"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HRPiML2HW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d37a48f6-e7d4-41c4-ee67-298a09f5205b"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:6.914123\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:5.893065\tAcc:0.074219 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:5.435833\tAcc:0.148438 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:4.993375\tAcc:0.207031 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:4.802594\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:4.380730\tAcc:0.281250 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:4.294127\tAcc:0.277344 LR:0.0010000\n",
            "Train Epoch Loss: 5.132602 Accuracy: 0.176728\n",
            "Eval Epoch Average Acc: 0.8102, Average Threshold: 0.1870\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:4.036437\tAcc:0.359375 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:3.746383\tAcc:0.398438 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:3.748130\tAcc:0.398438 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:3.500742\tAcc:0.445312 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:3.540380\tAcc:0.414062 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:3.516386\tAcc:0.480469 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:3.308254\tAcc:0.480469 LR:0.0010000\n",
            "Train Epoch Loss: 3.621430 Accuracy: 0.404583\n",
            "Eval Epoch Average Acc: 0.8200, Average Threshold: 0.1638\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:2.926268\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:2.734089\tAcc:0.582031 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:2.891995\tAcc:0.527344 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:2.731273\tAcc:0.566406 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:2.746288\tAcc:0.574219 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:2.728639\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:2.561917\tAcc:0.570312 LR:0.0010000\n",
            "Train Epoch Loss: 2.732677 Accuracy: 0.557126\n",
            "Eval Epoch Average Acc: 0.8383, Average Threshold: 0.1671\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:2.182083\tAcc:0.667969 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:2.128712\tAcc:0.710938 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:2.067202\tAcc:0.652344 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:2.050576\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:2.008570\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.945868\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:2.046687\tAcc:0.675781 LR:0.0010000\n",
            "Train Epoch Loss: 2.091571 Accuracy: 0.673067\n",
            "Eval Epoch Average Acc: 0.8373, Average Threshold: 0.1524\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:1.567292\tAcc:0.765625 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:1.348526\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:1.592170\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:1.406364\tAcc:0.785156 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:1.362965\tAcc:0.808594 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:1.473068\tAcc:0.800781 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:1.368987\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch Loss: 1.427498 Accuracy: 0.805497\n",
            "Eval Epoch Average Acc: 0.8372, Average Threshold: 0.1504\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:1.217288\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:1.189504\tAcc:0.824219 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:1.240821\tAcc:0.847656 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:1.251458\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:1.322963\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:1.114879\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:1.247800\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch Loss: 1.261122 Accuracy: 0.839823\n",
            "Eval Epoch Average Acc: 0.8428, Average Threshold: 0.1489\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:1.221098\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:1.258799\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:1.169558\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:1.148151\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:1.088091\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:1.041467\tAcc:0.878906 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:1.096916\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch Loss: 1.149706 Accuracy: 0.864721\n",
            "Eval Epoch Average Acc: 0.8425, Average Threshold: 0.1447\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.948195\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:1.129735\tAcc:0.878906 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:1.024225\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:1.022231\tAcc:0.894531 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:1.024120\tAcc:0.894531 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:1.067296\tAcc:0.878906 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:1.210702\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch Loss: 1.047744 Accuracy: 0.885825\n",
            "Eval Epoch Average Acc: 0.8408, Average Threshold: 0.1424\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.964580\tAcc:0.886719 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.991520\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.924054\tAcc:0.925781 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:0.982991\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.898624\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.978653\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:1.163876\tAcc:0.855469 LR:0.0001000\n",
            "Train Epoch Loss: 0.954074 Accuracy: 0.906836\n",
            "Eval Epoch Average Acc: 0.8365, Average Threshold: 0.1416\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.878154\tAcc:0.925781 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.795182\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.892239\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:0.832224\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.823434\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.874103\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.967178\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch Loss: 0.868246 Accuracy: 0.924549\n",
            "Eval Epoch Average Acc: 0.8405, Average Threshold: 0.1377\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.842517\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.815096\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.833601\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:0.761059\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.726043\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.741464\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.713598\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.763835 Accuracy: 0.943857\n",
            "Eval Epoch Average Acc: 0.8382, Average Threshold: 0.1341\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.653919\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.842183\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.781439\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:0.682669\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.845499\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.813169\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.737597\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.752227 Accuracy: 0.946071\n",
            "Eval Epoch Average Acc: 0.8367, Average Threshold: 0.1312\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.768091\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.722719\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.717521\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:0.741002\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.833802\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.812610\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.740580\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.740612 Accuracy: 0.948812\n",
            "Eval Epoch Average Acc: 0.8352, Average Threshold: 0.1336\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.769391\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.756289\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.776809\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:0.753711\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.726841\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.747432\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.715730\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.728761 Accuracy: 0.950081\n",
            "Eval Epoch Average Acc: 0.8373, Average Threshold: 0.1329\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.706310\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.754286\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.759920\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:0.739718\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.710350\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.660938\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.652879\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.719755 Accuracy: 0.952559\n",
            "Eval Epoch Average Acc: 0.8368, Average Threshold: 0.1392\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.700688\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.661402\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.689477\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:0.606934\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.735792\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.705205\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.725137\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch Loss: 0.709960 Accuracy: 0.954092\n",
            "Eval Epoch Average Acc: 0.8387, Average Threshold: 0.1351\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.756155\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.722049\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.726555\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:0.674361\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.717262\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.721856\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.695275\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch Loss: 0.701027 Accuracy: 0.956166\n",
            "Eval Epoch Average Acc: 0.8368, Average Threshold: 0.1361\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.526185\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.706523\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.696230\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:0.681528\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.612172\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.680484\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.785170\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.694618 Accuracy: 0.957622\n",
            "Eval Epoch Average Acc: 0.8393, Average Threshold: 0.1310\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.701963\tAcc:0.949219 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.679116\tAcc:0.949219 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.647532\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:0.652747\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.583737\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.682822\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.736477\tAcc:0.957031 LR:0.0000010\n",
            "Train Epoch Loss: 0.678268 Accuracy: 0.959077\n",
            "Eval Epoch Average Acc: 0.8385, Average Threshold: 0.1345\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.659167\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.725866\tAcc:0.941406 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.695721\tAcc:0.957031 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:0.693447\tAcc:0.941406 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.667935\tAcc:0.957031 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.693223\tAcc:0.933594 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.654859\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch Loss: 0.682922 Accuracy: 0.958566\n",
            "Eval Epoch Average Acc: 0.8377, Average Threshold: 0.1338\n",
            "Best acc on LFW: 0.8428333333333334, best threshold: 0.14894499999999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee0okStPXPYg",
        "colab_type": "text"
      },
      "source": [
        "**NormFace+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.682922\n",
        "\n",
        "Accuracy: 0.958566\n",
        "\n",
        "Eval Epoch Average Acc: 0.8377\n",
        "\n",
        "Average Threshold: 0.1338\n",
        "\n",
        "Best acc on LFW: **0.8428333333333334**\n",
        "\n",
        "best threshold: 0.14894499999999997"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HThYZu9Y2Gex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e519aba5-7309-40b2-8017-67ee3209ed4c"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "Softmax\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:6.943231\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:5.544892\tAcc:0.085938 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:4.437482\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:4.089367\tAcc:0.246094 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:3.596916\tAcc:0.347656 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:3.469929\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:3.217741\tAcc:0.410156 LR:0.0010000\n",
            "Train Epoch Loss: 4.257808 Accuracy: 0.248231\n",
            "Eval Epoch Average Acc: 0.8078, Average Threshold: 0.2014\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:2.609752\tAcc:0.464844 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:2.387923\tAcc:0.511719 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:2.544100\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:2.567527\tAcc:0.500000 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:2.508730\tAcc:0.511719 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:2.273715\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:1.975349\tAcc:0.617188 LR:0.0010000\n",
            "Train Epoch Loss: 2.346363 Accuracy: 0.542727\n",
            "Eval Epoch Average Acc: 0.8108, Average Threshold: 0.1789\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:1.738584\tAcc:0.636719 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.577272\tAcc:0.714844 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.603723\tAcc:0.652344 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:1.661957\tAcc:0.628906 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:1.433072\tAcc:0.718750 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:1.578977\tAcc:0.675781 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:1.875830\tAcc:0.617188 LR:0.0010000\n",
            "Train Epoch Loss: 1.635880 Accuracy: 0.667152\n",
            "Eval Epoch Average Acc: 0.8367, Average Threshold: 0.1789\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:1.069604\tAcc:0.777344 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:1.002414\tAcc:0.789062 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.189137\tAcc:0.765625 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:1.212873\tAcc:0.742188 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:1.208291\tAcc:0.738281 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.320177\tAcc:0.738281 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.321803\tAcc:0.734375 LR:0.0010000\n",
            "Train Epoch Loss: 1.197901 Accuracy: 0.750174\n",
            "Eval Epoch Average Acc: 0.8330, Average Threshold: 0.1672\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.710988\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.640449\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.755544\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:0.586081\tAcc:0.886719 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.500962\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.567932\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.511549\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch Loss: 0.629689 Accuracy: 0.871534\n",
            "Eval Epoch Average Acc: 0.8422, Average Threshold: 0.1712\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.529139\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.447186\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.452771\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:0.407795\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.497911\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.512246\tAcc:0.894531 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.491224\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch Loss: 0.501129 Accuracy: 0.901959\n",
            "Eval Epoch Average Acc: 0.8440, Average Threshold: 0.1634\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.393538\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.354554\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.473124\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:0.477312\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.452834\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.510047\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.489533\tAcc:0.902344 LR:0.0001000\n",
            "Train Epoch Loss: 0.425300 Accuracy: 0.917303\n",
            "Eval Epoch Average Acc: 0.8465, Average Threshold: 0.1630\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.338181\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.311923\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.343378\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:0.362354\tAcc:0.917969 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.323658\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.330687\tAcc:0.949219 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.286140\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch Loss: 0.355349 Accuracy: 0.934474\n",
            "Eval Epoch Average Acc: 0.8468, Average Threshold: 0.1494\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.227261\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.226422\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.295961\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:0.312849\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.320547\tAcc:0.941406 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.351853\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.296441\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch Loss: 0.301742 Accuracy: 0.946009\n",
            "Eval Epoch Average Acc: 0.8510, Average Threshold: 0.1575\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.251048\tAcc:0.949219 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.211024\tAcc:0.964844 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.190000\tAcc:0.976562 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:0.273052\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.221721\tAcc:0.957031 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.172506\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.239095\tAcc:0.949219 LR:0.0001000\n",
            "Train Epoch Loss: 0.245321 Accuracy: 0.958582\n",
            "Eval Epoch Average Acc: 0.8463, Average Threshold: 0.1551\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.175490\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.159982\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.162656\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:0.227547\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.157227\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.162593\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.156758\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.191725 Accuracy: 0.970101\n",
            "Eval Epoch Average Acc: 0.8485, Average Threshold: 0.1613\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.169609\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.119673\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.121941\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:0.144208\tAcc:0.988281 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.152850\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.215425\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.155923\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.185255 Accuracy: 0.971944\n",
            "Eval Epoch Average Acc: 0.8497, Average Threshold: 0.1623\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.187541\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.163737\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.156312\tAcc:0.988281 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:0.216930\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.191543\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.192438\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.133840\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch Loss: 0.177780 Accuracy: 0.973663\n",
            "Eval Epoch Average Acc: 0.8442, Average Threshold: 0.1610\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.139974\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.176787\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.136724\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:0.141589\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.194876\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.142321\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.141392\tAcc:0.988281 LR:0.0000100\n",
            "Train Epoch Loss: 0.173594 Accuracy: 0.974297\n",
            "Eval Epoch Average Acc: 0.8487, Average Threshold: 0.1626\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.138772\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.121034\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.194262\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:0.148096\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.189604\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.142898\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.164867\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch Loss: 0.167837 Accuracy: 0.975350\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1625\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.144052\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.153183\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.152276\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:0.111620\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.132333\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.185279\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.177821\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch Loss: 0.161738 Accuracy: 0.976682\n",
            "Eval Epoch Average Acc: 0.8492, Average Threshold: 0.1632\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.141864\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.121904\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.224653\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:0.160935\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.195112\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.112379\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.112632\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch Loss: 0.155453 Accuracy: 0.978865\n",
            "Eval Epoch Average Acc: 0.8475, Average Threshold: 0.1591\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.181704\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.117243\tAcc:0.988281 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.186960\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:0.145331\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.183379\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.140499\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.120835\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch Loss: 0.153374 Accuracy: 0.978354\n",
            "Eval Epoch Average Acc: 0.8477, Average Threshold: 0.1603\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.145970\tAcc:0.980469 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.148715\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.137445\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:0.120122\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.125465\tAcc:0.980469 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.189955\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.141115\tAcc:0.980469 LR:0.0000010\n",
            "Train Epoch Loss: 0.144484 Accuracy: 0.981141\n",
            "Eval Epoch Average Acc: 0.8487, Average Threshold: 0.1634\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.127497\tAcc:0.988281 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.128336\tAcc:0.988281 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.141837\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:0.199944\tAcc:0.972656 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.141278\tAcc:0.980469 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.192409\tAcc:0.972656 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.138178\tAcc:0.980469 LR:0.0000010\n",
            "Train Epoch Loss: 0.145092 Accuracy: 0.980119\n",
            "Eval Epoch Average Acc: 0.8468, Average Threshold: 0.1638\n",
            "Best acc on LFW: 0.851, best threshold: 0.15753499999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4d6E3FXfr2",
        "colab_type": "text"
      },
      "source": [
        "**Softmax+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.145092\n",
        "\n",
        "Accuracy: 0.980119\n",
        "\n",
        "Eval Epoch Average Acc: 0.8468\n",
        "\n",
        "Average Threshold: 0.1638\n",
        "\n",
        "Best acc on LFW: **0.851**\n",
        "\n",
        "best threshold: 0.15753499999999998"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu5dbVGVhNwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3725f48c-0d40-458f-f2f2-ceddccc05548"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "OHEM(normface基础上)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:7.210830\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:6.430557\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:6.156725\tAcc:0.085938 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:6.029995\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:5.563743\tAcc:0.183594 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:5.388025\tAcc:0.222656 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:5.372160\tAcc:0.230469 LR:0.0010000\n",
            "Train Epoch Loss: 6.026806 Accuracy: 0.115847\n",
            "Eval Epoch Average Acc: 0.7847, Average Threshold: 0.2353\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:5.181571\tAcc:0.277344 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:5.052803\tAcc:0.296875 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:4.841953\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:4.899017\tAcc:0.289062 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:4.776899\tAcc:0.386719 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:4.440458\tAcc:0.457031 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:4.406744\tAcc:0.445312 LR:0.0010000\n",
            "Train Epoch Loss: 4.849228 Accuracy: 0.339258\n",
            "Eval Epoch Average Acc: 0.8093, Average Threshold: 0.1926\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:4.095160\tAcc:0.468750 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:4.322525\tAcc:0.445312 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:3.948076\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:4.121017\tAcc:0.472656 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:3.796337\tAcc:0.558594 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:3.746891\tAcc:0.511719 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:3.791972\tAcc:0.527344 LR:0.0010000\n",
            "Train Epoch Loss: 4.004721 Accuracy: 0.497267\n",
            "Eval Epoch Average Acc: 0.8198, Average Threshold: 0.1758\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:3.492194\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:3.101257\tAcc:0.628906 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:3.394617\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:3.354380\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:3.166926\tAcc:0.667969 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:3.082284\tAcc:0.636719 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:3.122429\tAcc:0.664062 LR:0.0010000\n",
            "Train Epoch Loss: 3.245142 Accuracy: 0.617094\n",
            "Eval Epoch Average Acc: 0.8195, Average Threshold: 0.1943\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:2.795534\tAcc:0.683594 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:2.297938\tAcc:0.734375 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:2.331794\tAcc:0.777344 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:2.327021\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:2.447324\tAcc:0.722656 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:2.246355\tAcc:0.769531 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:2.292530\tAcc:0.781250 LR:0.0001000\n",
            "Train Epoch Loss: 2.374428 Accuracy: 0.753627\n",
            "Eval Epoch Average Acc: 0.8423, Average Threshold: 0.1563\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:2.098597\tAcc:0.792969 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:2.290174\tAcc:0.769531 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:2.297324\tAcc:0.769531 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:2.074997\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:1.928622\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:2.246710\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:1.848943\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch Loss: 2.136091 Accuracy: 0.791453\n",
            "Eval Epoch Average Acc: 0.8432, Average Threshold: 0.1614\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:1.866132\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:1.869511\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:1.771738\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:2.027244\tAcc:0.792969 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:1.980193\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:2.077298\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:1.663156\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch Loss: 1.972345 Accuracy: 0.816536\n",
            "Eval Epoch Average Acc: 0.8430, Average Threshold: 0.1582\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:1.857884\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:2.081060\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:2.035328\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:1.832735\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:1.847914\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:1.763299\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:2.021150\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch Loss: 1.823205 Accuracy: 0.838662\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1509\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:1.547521\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:1.816238\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:1.617016\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:1.741311\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:1.626512\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:1.550043\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:1.756066\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch Loss: 1.684177 Accuracy: 0.859426\n",
            "Eval Epoch Average Acc: 0.8368, Average Threshold: 0.1539\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:1.627815\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:1.479838\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:1.508868\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:1.618900\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:1.619603\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:1.613868\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:1.443379\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch Loss: 1.542743 Accuracy: 0.880746\n",
            "Eval Epoch Average Acc: 0.8443, Average Threshold: 0.1460\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:1.391896\tAcc:0.886719 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:1.411000\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:1.444068\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:1.447633\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:1.378551\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:1.475922\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:1.194498\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 1.386325 Accuracy: 0.904885\n",
            "Eval Epoch Average Acc: 0.8437, Average Threshold: 0.1432\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:1.307196\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:1.447238\tAcc:0.910156 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:1.335201\tAcc:0.894531 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:1.297757\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:1.423415\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:1.259707\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:1.382922\tAcc:0.902344 LR:0.0000100\n",
            "Train Epoch Loss: 1.368974 Accuracy: 0.907858\n",
            "Eval Epoch Average Acc: 0.8422, Average Threshold: 0.1488\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:1.272163\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:1.248476\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:1.307764\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:1.370250\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:1.332008\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:1.272961\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:1.415779\tAcc:0.878906 LR:0.0000100\n",
            "Train Epoch Loss: 1.349752 Accuracy: 0.909700\n",
            "Eval Epoch Average Acc: 0.8410, Average Threshold: 0.1485\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:1.246159\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:1.283946\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:1.246971\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:1.225090\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:1.299403\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:1.447121\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:1.500544\tAcc:0.878906 LR:0.0000100\n",
            "Train Epoch Loss: 1.330689 Accuracy: 0.914903\n",
            "Eval Epoch Average Acc: 0.8435, Average Threshold: 0.1549\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:1.175050\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:1.221594\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:1.443464\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:1.251401\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:1.302171\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:1.367856\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:1.223789\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch Loss: 1.319666 Accuracy: 0.915228\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1518\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:1.218179\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:1.265596\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:1.301427\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:1.347441\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:1.214152\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:1.418499\tAcc:0.886719 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:1.396614\tAcc:0.894531 LR:0.0000100\n",
            "Train Epoch Loss: 1.305229 Accuracy: 0.918712\n",
            "Eval Epoch Average Acc: 0.8457, Average Threshold: 0.1501\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:1.217887\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:1.199547\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:1.421700\tAcc:0.910156 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:1.391966\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:1.245602\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:1.124001\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:1.273757\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch Loss: 1.291391 Accuracy: 0.918929\n",
            "Eval Epoch Average Acc: 0.8407, Average Threshold: 0.1480\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:1.226720\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:1.293825\tAcc:0.933594 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:1.218173\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:1.287226\tAcc:0.925781 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:1.312593\tAcc:0.910156 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:1.362236\tAcc:0.902344 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:1.205284\tAcc:0.917969 LR:0.0000100\n",
            "Train Epoch Loss: 1.271981 Accuracy: 0.922737\n",
            "Eval Epoch Average Acc: 0.8438, Average Threshold: 0.1440\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:1.436379\tAcc:0.871094 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:1.335114\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:1.342785\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:1.251956\tAcc:0.925781 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:1.166529\tAcc:0.917969 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:1.252007\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:1.201782\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch Loss: 1.260861 Accuracy: 0.923527\n",
            "Eval Epoch Average Acc: 0.8422, Average Threshold: 0.1433\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:1.316054\tAcc:0.917969 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:1.262766\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:1.152723\tAcc:0.949219 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:1.382433\tAcc:0.906250 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:1.310429\tAcc:0.906250 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:1.344420\tAcc:0.906250 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:1.337157\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch Loss: 1.254958 Accuracy: 0.924348\n",
            "Eval Epoch Average Acc: 0.8430, Average Threshold: 0.1410\n",
            "Best acc on LFW: 0.8456666666666667, best threshold: 0.15010500000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmMQS0iHXwgr",
        "colab_type": "text"
      },
      "source": [
        "**OHEM(normface基础上)+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 1.254958\n",
        "\n",
        "Accuracy: 0.924348\n",
        "\n",
        "Eval Epoch Average Acc: 0.8430\n",
        "\n",
        "Average Threshold: 0.1410\n",
        "\n",
        "Best acc on LFW: **0.8456666666666667**\n",
        "\n",
        "best threshold: 0.15010500000000002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ZuXy9S4y3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2dcdadf2-19fe-40d1-dab0-46a84400ad8a"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "FocalLoss(normface基础上)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:3.422438\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:2.875630\tAcc:0.066406 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:2.594859\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (47%)]\tLoss:2.433343\tAcc:0.171875 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:2.249733\tAcc:0.218750 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:2.207954\tAcc:0.222656 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:1.965237\tAcc:0.316406 LR:0.0010000\n",
            "Train Epoch Loss: 2.485867 Accuracy: 0.176233\n",
            "Eval Epoch Average Acc: 0.7912, Average Threshold: 0.1958\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:1.836043\tAcc:0.355469 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:1.820009\tAcc:0.339844 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:1.734532\tAcc:0.390625 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (47%)]\tLoss:1.720343\tAcc:0.382812 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:1.610012\tAcc:0.425781 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:1.498951\tAcc:0.488281 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:1.572979\tAcc:0.429688 LR:0.0010000\n",
            "Train Epoch Loss: 1.660791 Accuracy: 0.407959\n",
            "Eval Epoch Average Acc: 0.8160, Average Threshold: 0.1888\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:1.218504\tAcc:0.550781 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.214651\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.297923\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (47%)]\tLoss:1.253862\tAcc:0.535156 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:1.203224\tAcc:0.570312 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:1.027964\tAcc:0.621094 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:1.138391\tAcc:0.597656 LR:0.0010000\n",
            "Train Epoch Loss: 1.200658 Accuracy: 0.562035\n",
            "Eval Epoch Average Acc: 0.8267, Average Threshold: 0.1929\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:0.973293\tAcc:0.660156 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:0.887849\tAcc:0.691406 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:0.873451\tAcc:0.695312 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (47%)]\tLoss:0.875667\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:0.874939\tAcc:0.683594 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:0.889950\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:0.858921\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch Loss: 0.891415 Accuracy: 0.675466\n",
            "Eval Epoch Average Acc: 0.8362, Average Threshold: 0.1540\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.655480\tAcc:0.777344 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.626549\tAcc:0.792969 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.580470\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (47%)]\tLoss:0.649924\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.497230\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.620320\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.452222\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch Loss: 0.569968 Accuracy: 0.801146\n",
            "Eval Epoch Average Acc: 0.8397, Average Threshold: 0.1475\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.479392\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.454064\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.469724\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (47%)]\tLoss:0.533982\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.439029\tAcc:0.863281 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.455023\tAcc:0.855469 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.458007\tAcc:0.847656 LR:0.0001000\n",
            "Train Epoch Loss: 0.488677 Accuracy: 0.837207\n",
            "Eval Epoch Average Acc: 0.8388, Average Threshold: 0.1424\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.488500\tAcc:0.855469 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.461206\tAcc:0.832031 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.419634\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (47%)]\tLoss:0.445188\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.392778\tAcc:0.902344 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.409146\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.451749\tAcc:0.839844 LR:0.0001000\n",
            "Train Epoch Loss: 0.436452 Accuracy: 0.862569\n",
            "Eval Epoch Average Acc: 0.8357, Average Threshold: 0.1511\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.345038\tAcc:0.894531 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.346868\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.351738\tAcc:0.886719 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (47%)]\tLoss:0.421371\tAcc:0.871094 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.372864\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.376680\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch Loss: 0.387863 Accuracy: 0.883936\n",
            "Eval Epoch Average Acc: 0.8387, Average Threshold: 0.1276\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.297350\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.332318\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.325402\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (47%)]\tLoss:0.305925\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.383917\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.288743\tAcc:0.925781 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.306909\tAcc:0.910156 LR:0.0001000\n",
            "Train Epoch Loss: 0.343045 Accuracy: 0.903445\n",
            "Eval Epoch Average Acc: 0.8338, Average Threshold: 0.1307\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.300506\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.246694\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.283213\tAcc:0.925781 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (47%)]\tLoss:0.318814\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.267022\tAcc:0.933594 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.299953\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.279345\tAcc:0.941406 LR:0.0001000\n",
            "Train Epoch Loss: 0.300976 Accuracy: 0.922257\n",
            "Eval Epoch Average Acc: 0.8363, Average Threshold: 0.1376\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.252000\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.235735\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.219438\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (47%)]\tLoss:0.218178\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.277841\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.273563\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.251798\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch Loss: 0.248999 Accuracy: 0.942185\n",
            "Eval Epoch Average Acc: 0.8377, Average Threshold: 0.1333\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.248880\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.231930\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.195870\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (47%)]\tLoss:0.219199\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.219442\tAcc:0.949219 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.307090\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.214062\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.242566 Accuracy: 0.945761\n",
            "Eval Epoch Average Acc: 0.8355, Average Threshold: 0.1333\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.219338\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.226672\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.195367\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (47%)]\tLoss:0.249185\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.235647\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.238751\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.216899\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.237586 Accuracy: 0.945792\n",
            "Eval Epoch Average Acc: 0.8360, Average Threshold: 0.1314\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.231082\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.185288\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.203460\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (47%)]\tLoss:0.237992\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.198812\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.193975\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.173520\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch Loss: 0.233420 Accuracy: 0.949075\n",
            "Eval Epoch Average Acc: 0.8358, Average Threshold: 0.1346\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.245650\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.270848\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.212849\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (47%)]\tLoss:0.186313\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.240311\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.216769\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.254723\tAcc:0.941406 LR:0.0000100\n",
            "Train Epoch Loss: 0.227725 Accuracy: 0.951243\n",
            "Eval Epoch Average Acc: 0.8350, Average Threshold: 0.1418\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.217890\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.225937\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.189476\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (47%)]\tLoss:0.201171\tAcc:0.972656 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.209235\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.186352\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.219287\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch Loss: 0.223290 Accuracy: 0.953147\n",
            "Eval Epoch Average Acc: 0.8343, Average Threshold: 0.1412\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.269358\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.265457\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.238030\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (47%)]\tLoss:0.175307\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.201300\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.187409\tAcc:0.964844 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.220300\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.217855 Accuracy: 0.954819\n",
            "Eval Epoch Average Acc: 0.8375, Average Threshold: 0.1368\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.270582\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.191273\tAcc:0.980469 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.213226\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (47%)]\tLoss:0.179552\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.244446\tAcc:0.957031 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.196396\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.203011\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.214940 Accuracy: 0.956460\n",
            "Eval Epoch Average Acc: 0.8362, Average Threshold: 0.1395\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.212746\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.190083\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.223697\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (47%)]\tLoss:0.188980\tAcc:0.964844 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.193609\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.238399\tAcc:0.941406 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.208665\tAcc:0.964844 LR:0.0000010\n",
            "Train Epoch Loss: 0.209008 Accuracy: 0.957715\n",
            "Eval Epoch Average Acc: 0.8352, Average Threshold: 0.1391\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.238047\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.208637\tAcc:0.964844 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.203608\tAcc:0.957031 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (47%)]\tLoss:0.201785\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.253311\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.221913\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.213781\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch Loss: 0.208415 Accuracy: 0.957916\n",
            "Eval Epoch Average Acc: 0.8360, Average Threshold: 0.1416\n",
            "Best acc on LFW: 0.8396666666666667, best threshold: 0.14746050000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA4iNMrYX-pq",
        "colab_type": "text"
      },
      "source": [
        "**FocalLoss(normface基础上)+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.208415\n",
        "\n",
        "Accuracy: 0.957916\n",
        "\n",
        "Eval Epoch Average Acc: 0.8360\n",
        "\n",
        "Average Threshold: 0.1416\n",
        "\n",
        "Best acc on LFW: **0.8396666666666667**\n",
        "\n",
        "best threshold: 0.14746050000000002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7mbPPbPO96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd21f3cc-0f93-476b-929e-f425efe93391"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "CosFace\n",
            "Contrastive（Scratch）\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:216.978058\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:156.932144\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:127.229462\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:91.501236\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:79.224113\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:97.172852\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:100.992523\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:82.219795\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:72.094696\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:49.361641\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:56.986443\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:55.846466\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:62.446449\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 94.160332 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6135, Average Threshold: -0.2263\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:45.015209\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:41.735962\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:54.060837\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:25.685974\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:15.848047\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:28.001987\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:21.738316\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:18.733910\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:10.641468\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:10.678432\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:12.618599\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:9.166975\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:11.319155\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 22.615988 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5778, Average Threshold: -0.9217\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:10.476904\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:10.093320\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:6.772649\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:6.366742\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:6.620331\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:6.392929\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:5.069488\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:4.801411\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:4.824129\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:2.550970\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:2.860308\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:2.809794\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:3.071296\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 5.000618 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5437, Average Threshold: -0.2357\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:2.692520\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:1.875624\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:2.106551\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:2.295545\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.796556\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:1.481613\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:1.769978\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:0.932781\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:0.683089\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:1.277881\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.441150\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:0.695723\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.118381\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 1.661684 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5848, Average Threshold: -0.9709\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.576267\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:0.568431\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:1.409760\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:1.009680\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.917454\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:0.588108\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:1.150039\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:1.192341\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.603899\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:0.791321\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:1.053166\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:1.183724\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.855596\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.823741 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5852, Average Threshold: -0.9763\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.678601\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:0.496485\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.705895\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:0.542534\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.826631\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:0.521143\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.760190\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:0.373150\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.665366\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:0.962113\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.537326\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:0.926733\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.715291\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.704589 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5833, Average Threshold: -0.9616\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.855840\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.487703\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.654720\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:0.649538\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:1.062582\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.653850\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.639184\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:0.164598\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.741487\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:0.798936\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.454641\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.629112\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.696232\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.633373 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5885, Average Threshold: 0.8917\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.659672\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.629231\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.783680\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.472779\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.576151\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.699173\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.629735\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.526088\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.545140\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.242509\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.263583\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.226975\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.849440\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.529312 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5903, Average Threshold: 0.7641\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.399758\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.349826\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.414980\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.326894\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.400686\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.389865\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.284539\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.435569\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.196353\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.213503\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.137533\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.307658\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.288735\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.441397 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5797, Average Threshold: -0.1109\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.242932\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.212550\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.625122\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.300324\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.366930\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.511600\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.315763\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.286176\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.446011\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.473120\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.316776\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.305719\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.351848\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.359318 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5958, Average Threshold: -0.9590\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.256411\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.145287\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.280682\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.240056\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.320525\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.410520\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.268326\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.491513\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.380795\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.129274\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.475074\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.123476\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.258594\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.304621 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5817, Average Threshold: -0.6301\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.245499\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.448394\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.160950\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.239186\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.216771\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.269566\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.223531\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.465690\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.207491\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.185122\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.259506\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.281342\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.171916\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.280652 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5807, Average Threshold: 0.0002\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.285509\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.293488\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.348542\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.115510\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.245118\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.238524\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.249186\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.269711\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.284764\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:0.509631\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.201051\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.236850\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.193269\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.277495 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5787, Average Threshold: -0.1061\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.271566\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.560285\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.185230\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:0.457682\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.274563\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.673744\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.129531\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.341253\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.145271\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.219983\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.111882\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:0.371683\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.331257\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.260342 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5923, Average Threshold: 0.9809\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.263077\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:0.195412\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.236332\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:0.439399\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.249150\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:0.215844\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.154690\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:0.252611\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.488029\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:0.256513\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.206861\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:0.243064\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.198711\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.249134 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5820, Average Threshold: 0.1934\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.253708\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:0.351453\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.284660\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.252657\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.258767\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:0.107125\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:0.116804\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:0.298624\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.163423\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:0.245449\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.177073\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:0.301881\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.451539\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.237367 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5887, Average Threshold: 0.7972\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.092182\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:0.272027\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.162143\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:0.102973\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.307936\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:0.159381\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:0.122053\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:0.227569\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.144403\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.309047\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.365904\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:0.104197\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.253104\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.225396 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5863, Average Threshold: 0.9910\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.223637\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:0.204144\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.149339\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:0.259353\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.196803\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:0.300572\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:0.240328\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:0.177497\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.155237\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:0.166337\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.257861\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.218277\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.311824\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.208749 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5918, Average Threshold: 0.9931\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.193363\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:0.248295\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.299133\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:0.223004\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.285142\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:0.184602\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.071853\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:0.136822\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.113256\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:0.119123\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.215357\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:0.169704\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.243544\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 0.196821 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5927, Average Threshold: 0.9926\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.134367\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:0.188346\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.179886\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:0.254138\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.195122\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:0.170118\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:0.158955\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:0.240797\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.133243\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:0.163747\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.263103\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:0.147005\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.384017\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 0.193708 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5912, Average Threshold: 0.9912\n",
            "Best acc on LFW: 0.6135, best threshold: -0.22625752764795187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-jdw7GaYMds",
        "colab_type": "text"
      },
      "source": [
        "**Contrastive（Scratch）CosFace+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.193708\n",
        "\n",
        "Accuracy: 0.000000\n",
        "Eval Epoch Average Acc: 0.5912\n",
        "\n",
        "Average Threshold: 0.9912\n",
        "\n",
        "Best acc on LFW: **0.6135**\n",
        "\n",
        "best threshold: -0.22625752764795187"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Hlibq216xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "bc93960b-d26e-42f9-b5f9-7c79c5c63a54"
      },
      "source": [
        "!python generate_csv_files.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading image paths ...\n",
            "Number of files: 173312\n",
            "\n",
            "Generating csv file ...\n",
            "100% 173312/173312 [00:00<00:00, 314661.95it/s]\n",
            "\n",
            "Done! Elapsed time: 0.02 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg_ljZyn8rlA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afe82413-8706-476a-b9b9-35e6735f8fab"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41791\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41799\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41810\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41825\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41832\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41843\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41857\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41883\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41892\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41912\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41922\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41940\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41962\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41975\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41980\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 41989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42000\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42021\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42033\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42044\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42057\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42065\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42073\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42082\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42096\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42102\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42118\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42127\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42135\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42154\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42168\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42185\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42197\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42203\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42214\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42223\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42234\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42244\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42256\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42261\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42268\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42276\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42289\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42297\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42302\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42307\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42317\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42325\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42340\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42351\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42359\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42373\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42386\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42397\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42404\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42413\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42431\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42440\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42456\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42464\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42483\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42491\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42494\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42510\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42520\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42544\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42554\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42564\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42573\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42587\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42596\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42606\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42622\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42630\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42640\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42647\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42658\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42677\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42684\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42703\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 42710\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42732\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42746\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42759\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42763\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42773\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42778\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42798\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42805\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42810\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42815\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42826\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42834\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42841\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42865\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42875\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42883\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42892\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42906\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42923\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42930\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42940\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42951\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42959\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42967\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42974\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 42998\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43006\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43012\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43021\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43032\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43034\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43044\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43059\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43068\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43077\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43089\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43100\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43114\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43133\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43141\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43149\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43161\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43170\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43179\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43196\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43209\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43219\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43229\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43239\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43247\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43256\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43263\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43279\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43291\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43300\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43317\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43325\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43339\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43360\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43370\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43380\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43391\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43399\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43422\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43439\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43447\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43461\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43470\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43492\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43497\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43518\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43525\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43533\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43541\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43557\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43573\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43587\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43594\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43603\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43613\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43623\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43628\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43639\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43644\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43658\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43673\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43681\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43687\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43696\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43703\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43711\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43719\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43727\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43739\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 43748\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43761\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43768\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43775\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43784\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43793\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43805\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43815\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43830\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43840\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43858\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43874\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43882\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43889\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43894\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43913\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43926\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43935\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43943\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43951\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43960\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43970\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43984\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 43994\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44002\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44025\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44036\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44047\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44061\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44073\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44078\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44088\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44098\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44105\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44116\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44134\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44139\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44153\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44165\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44173\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44182\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44195\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44205\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44216\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44224\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44228\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44242\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44255\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44267\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44274\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44282\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44295\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44304\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44318\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44329\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44336\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44345\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44352\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44357\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44365\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44373\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44386\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44394\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44409\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44432\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44442\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44449\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44457\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44471\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44482\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44489\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44518\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44527\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44539\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 44553\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44558\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44566\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44579\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44585\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44593\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44600\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44609\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44615\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44627\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44638\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44644\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44655\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44667\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44680\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44687\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44702\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44716\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44727\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44734\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44741\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44753\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44776\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44779\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44786\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44795\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44807\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44814\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44825\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44833\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44840\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44850\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44857\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44864\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44879\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44886\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44898\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44909\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44921\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44926\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44931\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44944\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44950\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44956\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44968\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 44987\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45000\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45010\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45017\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45030\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45041\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45053\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45062\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45070\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45080\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45095\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45100\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45112\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45120\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45136\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45152\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45163\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45177\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45188\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45198\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45205\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45216\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45222\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45232\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45246\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45260\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45270\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45282\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45289\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45297\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45308\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45318\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45331\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45340\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45347\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45357\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45366\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45379\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45386\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45394\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45415\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45440\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45445\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45462\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45472\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45479\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45493\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45502\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45515\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45521\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45529\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45542\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45552\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45561\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45569\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45579\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45588\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45596\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45624\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45644\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45650\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45661\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45670\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45679\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45690\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45703\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45711\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45720\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45727\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45732\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45745\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45762\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45770\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45779\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45798\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45806\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45818\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45827\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45835\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45864\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45869\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45881\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45890\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45928\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45940\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45947\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45952\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45971\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45981\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 45991\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46003\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46017\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46028\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46040\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46050\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46058\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46068\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46080\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46088\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46096\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46108\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 46114\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46119\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46132\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46145\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46158\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46167\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46172\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46182\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46192\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46202\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46212\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46220\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46230\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46237\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46245\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46258\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46270\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46278\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46319\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46327\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46332\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46341\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46368\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46373\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46379\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46387\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46391\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46397\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46408\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46415\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46425\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46431\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46442\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46467\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46473\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46483\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46492\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46499\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46506\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46513\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46523\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46532\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 46541\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46546\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46556\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46562\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46573\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46590\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46599\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46619\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46624\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46634\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46642\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46649\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46658\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46676\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46681\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46687\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46699\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46714\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46720\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46731\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46740\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46760\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46773\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46798\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46812\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46820\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46828\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46834\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46848\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46861\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46868\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46877\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46890\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46923\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46933\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46941\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46962\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46971\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46983\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 46995\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47008\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47022\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47027\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47038\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47048\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47059\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47071\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47080\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47090\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47097\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47114\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47134\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47152\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47162\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47171\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47182\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47195\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47214\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47224\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47236\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47248\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47254\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47262\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47272\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47279\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47287\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47300\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47316\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47325\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47335\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47346\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47359\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47363\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47370\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47379\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47395\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47408\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47431\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47439\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47447\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47453\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47467\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47485\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47489\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47503\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47516\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47529\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47537\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47547\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47559\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47572\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47578\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47589\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47594\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47603\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47612\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47616\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47622\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 47631\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47638\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47645\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47657\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47689\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47700\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47711\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47719\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47728\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47737\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47744\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47753\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47767\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47774\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47782\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47797\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47806\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47818\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47824\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47836\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47849\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47859\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47868\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47875\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47896\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47905\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47917\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47926\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47939\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47947\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47967\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 47989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48000\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48022\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48027\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48032\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48043\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48052\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48061\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48071\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48081\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48090\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48100\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48111\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48132\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48145\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48157\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48166\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48178\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48195\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48206\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48217\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48228\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48232\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48247\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48256\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48265\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48272\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48282\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48290\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48299\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48304\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48319\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48326\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48334\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48345\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48353\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48364\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48373\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48384\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48396\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48403\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48416\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48425\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48441\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48467\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48475\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48482\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48491\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48503\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48512\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48519\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48526\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48538\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48548\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48562\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48573\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48586\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48598\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48606\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48625\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48635\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48639\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48646\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48657\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48665\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48675\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48684\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48692\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48698\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48709\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 48719\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48743\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48759\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48777\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48788\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48801\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48811\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48822\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48835\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48846\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48856\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48877\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48893\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48904\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48913\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48920\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48927\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48935\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48946\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 48990\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49000\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49020\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49025\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49030\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49042\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49056\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49066\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49072\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49086\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49099\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49108\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49119\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49129\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49162\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49172\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49178\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49193\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49204\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49213\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49225\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49235\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49245\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49256\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 49273\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49278\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49288\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49310\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49317\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49327\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49332\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49342\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49352\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49359\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49366\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49375\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49386\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49399\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49404\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49411\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49417\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49437\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49442\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49454\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49485\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49490\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49498\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49516\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49524\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49535\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49541\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49551\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49562\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49572\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49581\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49588\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49601\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49609\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49629\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49641\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49649\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49654\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49661\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49670\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49676\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49685\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49692\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49698\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49705\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49717\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0494\tNumber of valid training triplets in epoch: 49730\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49739\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49773\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49795\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49808\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49814\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49821\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49828\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49836\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49845\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49853\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49859\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49874\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49898\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49908\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49924\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49936\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49942\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49947\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49957\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49964\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 49994\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50003\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50011\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50023\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50034\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50041\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50048\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50060\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50069\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50077\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50081\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50087\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50092\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50100\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50111\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50120\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50134\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50144\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50162\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50171\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50181\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50190\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50204\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50214\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50225\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50233\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50246\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50255\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50279\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50299\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50309\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50321\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50333\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50343\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50356\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50362\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50372\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50380\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50386\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50392\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50401\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50429\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50442\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50450\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50470\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50478\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50480\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50484\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50492\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50498\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50510\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50521\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50537\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50545\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50557\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50565\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50581\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50590\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50595\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50602\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50612\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50642\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50651\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50672\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50712\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50731\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50738\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50748\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50757\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50763\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50772\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50780\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50793\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50807\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50815\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50822\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50830\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50841\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50864\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50873\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50886\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50892\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50923\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50929\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50937\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50941\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50948\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50952\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50961\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50971\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50982\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 50989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 51001\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 51006\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51019\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51029\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 51037\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51048\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51058\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51068\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51087\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51103\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51108\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51117\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51144\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51152\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51161\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51170\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51182\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51185\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51198\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51207\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51216\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51224\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51230\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51244\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51254\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51260\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51268\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51274\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51284\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51295\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51307\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51316\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51328\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51341\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51349\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51360\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51374\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51390\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51400\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51407\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51423\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51430\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51441\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51448\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51464\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51490\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51499\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51506\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51512\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51519\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51524\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51537\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51543\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51553\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51560\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51571\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51593\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51606\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51625\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51644\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51656\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51664\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51687\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51696\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51706\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51716\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51727\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51736\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51758\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51768\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51776\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51796\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51807\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51820\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51829\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51874\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51884\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51891\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51902\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51912\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51918\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51931\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51939\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51971\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51986\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 51999\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52012\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52020\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52029\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52041\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52056\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52067\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52087\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52095\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52117\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52133\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52147\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52159\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52170\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52187\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52198\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52210\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52218\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52229\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52239\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52251\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52254\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52274\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52285\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52303\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52312\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52320\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52329\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52340\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52347\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52358\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52367\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52389\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52394\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52412\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52417\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 52421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52427\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52438\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52446\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52480\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52491\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52495\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52503\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52514\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52519\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52530\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52537\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52548\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52556\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52561\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52573\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52588\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52597\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52608\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52610\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52619\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52623\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52629\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52640\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52649\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52652\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52661\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52667\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52672\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52680\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52686\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52718\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52729\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52735\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52745\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52759\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52769\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52778\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52789\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52800\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52804\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52811\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52818\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52825\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52831\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52856\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52862\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52873\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52883\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52898\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52908\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52923\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52931\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52945\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52955\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52962\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 52972\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52987\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 52995\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53020\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 53030\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53039\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53047\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53057\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53065\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53077\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53084\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53114\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53146\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53154\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53159\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53168\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53178\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53193\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53201\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53209\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53217\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53223\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53228\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53239\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53251\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53262\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53273\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53288\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53295\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53301\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53320\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53331\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53338\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53349\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53351\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53363\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53377\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53384\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53394\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53401\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53430\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53436\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53441\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53456\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53477\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53485\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53495\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53508\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53517\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53526\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53534\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53538\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53545\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53553\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53559\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53572\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53583\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53595\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53623\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53628\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53640\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53656\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53667\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53676\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53681\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53702\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53714\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53725\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53732\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53743\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53756\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53763\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53770\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53780\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53794\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53807\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53818\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 53832\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53840\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53865\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53878\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53883\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53894\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53905\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53925\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53935\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53944\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53953\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53970\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53982\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53990\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 53997\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54004\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54024\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54033\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54047\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54054\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 54064\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54071\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54077\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54089\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54095\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54101\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54110\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54120\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54127\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54139\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54147\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54156\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54164\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54169\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54175\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54186\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54193\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54200\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54207\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54216\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54227\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54239\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54248\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54255\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54276\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54308\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54322\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54335\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54343\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54361\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54369\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54393\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54409\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54420\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54431\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54438\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54444\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54457\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54468\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54476\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54484\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54489\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54499\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54514\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54529\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54538\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54545\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54553\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54556\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54567\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54575\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54583\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54592\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54603\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54616\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54627\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54641\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54650\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54656\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54668\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54672\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54680\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54688\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54705\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54717\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54722\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54726\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54736\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54747\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54759\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54775\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54791\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54805\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54813\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54818\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54828\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54837\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54846\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54854\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54862\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54870\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54875\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54898\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54907\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54928\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54934\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54945\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54964\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54974\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54986\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 54995\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55005\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55014\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55022\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55033\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55041\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55051\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55066\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55074\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55083\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55095\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55107\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55120\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55133\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55142\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55153\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55160\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55168\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55180\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55190\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55200\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55211\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55224\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55236\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55243\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55258\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55270\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55278\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55292\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55300\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55312\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55318\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55325\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55332\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55348\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55357\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55367\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55372\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55385\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55392\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55400\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55419\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55440\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55453\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55461\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55472\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55486\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55496\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55523\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55532\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55541\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55553\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55562\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55572\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55583\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55595\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55606\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55626\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55641\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55651\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55661\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55689\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55699\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55707\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55717\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55727\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55744\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55754\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55776\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55781\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55791\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55800\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55805\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55814\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55825\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55841\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55860\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55870\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55880\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55889\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55917\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55925\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55936\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55945\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55952\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 55989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56003\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56011\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56022\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56036\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56046\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56055\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56066\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56071\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56082\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56094\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56112\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56118\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56134\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56146\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56152\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56162\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56169\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56180\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56192\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56200\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56208\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56215\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56226\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56237\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56248\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56253\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56259\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56264\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56279\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56293\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56303\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56310\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56317\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56324\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56334\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56339\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56352\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56361\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56369\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56384\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56389\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56404\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56426\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56432\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56447\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56454\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56467\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 56472\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56486\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56500\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56518\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56527\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56533\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56539\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56549\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56561\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56569\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56577\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56588\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56594\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56606\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56622\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56631\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56647\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56655\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56665\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56677\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56690\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56697\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56710\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56715\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56724\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56731\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56739\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56748\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56757\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56764\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56776\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56789\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56799\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56805\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56817\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56824\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56831\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56847\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56863\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56872\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56880\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56895\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56898\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56909\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56926\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56936\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56946\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56954\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56970\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56988\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 56998\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57007\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57019\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57032\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57043\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57055\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57062\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57069\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57078\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57083\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57104\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57113\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57119\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57137\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57155\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57165\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57173\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57180\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57188\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57194\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57204\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57214\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57225\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57237\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57248\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57254\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57265\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57272\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57282\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57291\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57301\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57315\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57324\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57329\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57335\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57347\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57355\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57363\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57374\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57381\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57391\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57396\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57405\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57415\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57435\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57446\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57456\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57488\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57498\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57510\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57517\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57538\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57548\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57565\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57574\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57593\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57602\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57645\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57659\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57669\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57690\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57698\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57712\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57723\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57742\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57759\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57767\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57775\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57789\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57801\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57810\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57817\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57825\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57829\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57856\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57868\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57878\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57899\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57908\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57913\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57920\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57927\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57933\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57940\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57953\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 57962\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 57971\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 57977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 57985\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 57992\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 57999\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58009\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58016\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58021\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58028\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58039\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58044\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58058\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58072\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58085\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58098\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58110\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58147\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58153\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58161\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58169\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58174\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58183\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58194\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58203\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58216\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58220\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58234\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58245\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58251\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58273\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58284\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58306\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58310\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58318\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58334\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58341\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58361\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58372\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58385\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58397\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58417\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58430\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58439\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58447\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58455\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58468\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58478\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58485\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58495\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58505\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58523\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58538\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58544\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58557\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58568\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58578\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 58590\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58599\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58613\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 58625\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 58635\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58641\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58644\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58655\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58660\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58671\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58676\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58685\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58696\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58701\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58710\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58717\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58741\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58750\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58758\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58775\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58791\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58799\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58811\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58821\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58833\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58846\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58856\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58867\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58873\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58882\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58890\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58900\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58907\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58918\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58932\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58942\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58952\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58966\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58973\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58982\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 58991\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59001\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59021\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59030\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59037\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59052\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59059\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59064\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59082\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59091\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59104\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59111\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59122\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59131\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59161\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59173\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59186\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59198\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59210\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59222\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59230\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59237\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59245\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59253\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59259\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59267\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59276\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59287\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59299\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59308\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59315\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59326\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59339\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59346\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59355\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59367\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59376\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59382\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59388\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59395\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59403\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59409\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59417\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59441\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59448\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59454\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59460\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59464\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59473\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 59490\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59494\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59506\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59513\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59522\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59535\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59548\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59560\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59570\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59580\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59590\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59604\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59617\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59629\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59638\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59646\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59658\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59663\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59669\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59691\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59700\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59707\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59724\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59734\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59749\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59760\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59769\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59778\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59785\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59797\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59806\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59817\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59823\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59832\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59838\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59845\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59859\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59869\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59879\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59886\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59894\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59904\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59913\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59922\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59930\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59946\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59957\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59967\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59973\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59985\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 59996\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60007\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60018\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60029\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60036\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60049\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60060\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60064\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60085\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60092\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60100\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60107\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60113\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60118\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60126\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60137\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60147\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60154\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60163\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60172\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60178\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60189\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60202\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60211\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60221\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60229\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60242\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60253\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60264\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60273\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60293\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60305\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60319\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60326\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60336\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60348\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60357\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60363\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60377\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60396\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60402\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60415\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60424\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60438\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60452\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60461\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60481\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60491\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60504\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60517\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60526\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60542\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60549\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60555\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60572\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60578\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60590\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60596\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60608\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60619\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60626\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60649\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60660\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60669\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60692\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60699\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60725\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60734\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60744\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60754\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60767\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60778\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60800\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60814\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60822\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60831\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60841\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60850\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60862\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60872\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60877\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60878\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60895\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60905\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60918\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60927\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60938\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60947\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60957\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60970\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60975\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60986\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 60996\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61008\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61017\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61026\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61036\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61046\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61058\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61069\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61083\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61092\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61101\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61109\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61127\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61135\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61144\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61153\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61165\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61171\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61181\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61193\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61205\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61213\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61221\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61229\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61235\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61250\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61257\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61268\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61278\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61287\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61290\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61304\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61323\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61331\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61337\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61348\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61355\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61368\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61378\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61393\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61416\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61419\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61427\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61439\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61454\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61463\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61470\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61477\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61487\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61499\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61505\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61512\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61524\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61534\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61543\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61555\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61568\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61577\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61585\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61592\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61605\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61613\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61625\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61633\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61645\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61659\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61666\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61669\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61681\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61690\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61700\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61721\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61737\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61745\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61760\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61773\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61786\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61796\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61806\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61816\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61826\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61832\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61840\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61852\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61861\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61869\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61882\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61890\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61899\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61910\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61920\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61933\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61947\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61958\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61967\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61978\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61988\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 61996\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62006\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62024\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62031\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62039\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62053\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62065\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62084\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62095\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62104\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62112\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62143\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62154\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62164\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62176\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62184\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62197\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62204\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62211\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62224\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62238\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62246\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62252\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62268\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62277\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62287\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62299\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62308\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62322\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62331\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62342\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62360\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62376\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62388\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62396\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62407\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62419\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62434\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62442\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62452\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62459\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62469\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62479\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62490\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62500\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62503\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62508\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62517\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62530\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62540\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62554\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62565\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62574\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62583\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62588\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62602\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62618\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62629\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62640\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62652\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62659\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62670\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62720\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62725\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62747\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62763\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62771\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 62783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62787\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62796\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62804\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62814\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62823\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62829\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62836\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62839\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62845\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62860\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62877\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62893\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62906\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62921\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62930\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62938\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62952\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62960\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62968\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62980\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62986\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 62994\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63005\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63011\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63020\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63028\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63035\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63045\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63053\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63061\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63069\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63081\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63084\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63090\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63098\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63108\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63114\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63124\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63138\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63149\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63158\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63165\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63173\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63185\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63195\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63207\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63213\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63219\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63229\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63235\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63250\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63258\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63273\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63281\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63291\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63299\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63311\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63316\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63322\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63326\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63333\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63344\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63349\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63360\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63368\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63375\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63384\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63392\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63401\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63411\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 63429\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63436\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63441\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63452\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63460\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63468\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63478\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63492\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63520\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63525\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63534\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63546\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63555\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63564\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63570\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63580\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63592\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63598\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63621\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63637\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63643\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63651\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63661\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63672\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63688\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63700\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63707\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63720\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63729\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63739\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63750\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63758\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63770\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63775\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63788\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63798\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63812\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63820\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63834\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63843\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63863\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63870\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63885\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63899\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63905\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63914\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63925\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63937\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63944\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63950\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63964\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63975\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63987\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 63995\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64006\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64015\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64027\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64041\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64051\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64060\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64075\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64087\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64102\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64117\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64128\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64134\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64142\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64155\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64164\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64170\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64176\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64184\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64199\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64207\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64214\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64227\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64238\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64245\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64251\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64258\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64271\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64283\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64292\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64296\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64304\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64315\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64329\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64339\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64353\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64360\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64370\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64376\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64391\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64398\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64418\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64434\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64443\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64457\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64467\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64477\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64486\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64493\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64503\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64511\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64520\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64528\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64535\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64544\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64556\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64568\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64578\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64587\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64603\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64611\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64620\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64628\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64646\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64653\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64659\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64667\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64680\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64685\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64697\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64706\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64714\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64728\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64738\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64750\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64762\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64772\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64777\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64785\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64792\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64800\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64807\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64815\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64827\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64842\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64852\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64863\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64872\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64878\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64886\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64897\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64906\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64917\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64927\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64936\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64945\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64956\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64964\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64976\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64985\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 64994\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65007\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65017\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65023\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65032\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65042\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65050\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65062\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65068\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65075\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65080\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65088\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65093\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65106\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65116\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65125\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65133\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65140\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65151\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65162\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65169\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65175\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65188\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65197\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65204\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65212\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65222\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65236\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65248\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65254\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65266\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65275\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65285\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65294\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65307\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65322\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65334\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65344\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65349\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65356\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65363\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65370\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65379\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65387\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65393\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65406\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65410\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65413\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65420\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65432\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65437\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65448\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65460\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65471\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65479\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65488\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65496\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65504\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65515\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65525\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65530\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65541\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65552\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65567\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65580\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65589\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65601\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65621\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65632\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65643\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65648\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65660\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65669\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65684\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65696\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65709\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65722\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65738\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65746\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65757\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65769\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65777\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65783\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65797\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65806\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65817\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65827\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65838\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65846\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65853\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65858\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65869\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65879\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65887\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65892\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65903\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65911\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65924\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65932\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65942\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65950\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65958\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65966\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 65977\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65983\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65993\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 65999\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66012\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66018\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66030\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66038\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66046\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66056\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66059\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66065\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66072\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66079\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66089\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66101\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66104\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66113\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66122\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66131\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66139\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66151\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66167\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66176\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66184\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66191\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66196\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66207\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66217\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66228\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66235\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66238\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66246\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66255\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66267\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66281\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66286\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66295\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66305\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66318\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66328\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66340\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66350\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66362\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66373\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66383\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66389\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66397\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66404\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66413\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66419\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66428\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66435\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66451\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66458\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66463\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66473\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66479\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66487\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66497\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66507\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66512\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66527\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66535\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66544\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66554\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66566\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66576\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66582\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66592\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66598\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66604\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66619\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66624\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66630\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66641\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66651\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66659\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66668\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66682\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66694\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66705\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66714\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66724\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66743\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66764\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66772\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66786\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66797\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66809\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66820\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66829\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66837\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66848\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66857\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66867\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66879\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66888\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66899\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66908\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66918\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66931\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66939\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66948\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66963\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66973\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66980\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 66989\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67002\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67013\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67017\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67026\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67032\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67038\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67048\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67057\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67065\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67072\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67081\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67094\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67107\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67117\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67131\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67137\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67146\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67157\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67167\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67179\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67190\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67202\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67212\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67221\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67227\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67241\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67251\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67261\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67274\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67283\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67293\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67298\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67308\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67317\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67325\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67333\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67342\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67348\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67357\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67362\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67372\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67387\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67400\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67408\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67416\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67425\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67431\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67440\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67447\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67457\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67470\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67474\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67480\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67490\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67496\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67508\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67519\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67529\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67539\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67551\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67561\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67576\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67587\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67599\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67610\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67623\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67636\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67645\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67657\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67664\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67674\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67677\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67688\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67704\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67713\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67725\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67736\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67751\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67755\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67766\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67774\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67782\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67789\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67797\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67812\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67820\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67829\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67839\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67851\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67858\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67865\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 67871\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67880\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67893\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67905\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67913\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67925\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67935\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67942\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67956\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67968\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67980\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67991\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 67997\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68008\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68016\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68020\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68027\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68038\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0505\tNumber of valid training triplets in epoch: 68046\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68054\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68066\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68076\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68086\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68092\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68097\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68112\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68118\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68132\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68139\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68145\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68151\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68156\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68163\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68173\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68186\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68192\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68199\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68212\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68221\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68237\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68247\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68257\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68268\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68276\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68282\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68296\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68309\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68320\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68328\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68336\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68344\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68359\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68367\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68379\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68388\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68400\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68414\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68421\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68435\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68445\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68452\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68466\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68478\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68492\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68496\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68506\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68514\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68523\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68531\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68543\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68547\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68556\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68567\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68576\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68589\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68598\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68609\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68614\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68624\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68633\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68643\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68658\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68664\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68672\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68678\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68686\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68693\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68708\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68711\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68719\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68724\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68733\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68746\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68758\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68765\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68776\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68791\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68799\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68815\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68827\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68835\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68845\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68853\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68866\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68876\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68886\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68892\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68896\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68900\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68909\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68916\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68921\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68935\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68943\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68956\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68968\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68981\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68988\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 68995\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69001\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69012\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69022\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69034\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69042\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69051\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69059\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69066\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69074\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69081\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69090\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69096\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69101\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69110\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69120\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69127\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69137\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69145\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69150\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69159\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69172\n",
            "Epoch 4:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 69182\n",
            "Eval Epoch Average Acc: 0.7968, Average Threshold: 0.2445\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0352\tNumber of valid training triplets in epoch: 9\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 16\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0465\tNumber of valid training triplets in epoch: 26\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 33\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 41\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0521\tNumber of valid training triplets in epoch: 49\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 58\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0451\tNumber of valid training triplets in epoch: 69\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 75\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0507\tNumber of valid training triplets in epoch: 84\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 89\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0468\tNumber of valid training triplets in epoch: 100\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0469\tNumber of valid training triplets in epoch: 111\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0461\tNumber of valid training triplets in epoch: 122\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0447\tNumber of valid training triplets in epoch: 134\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0468\tNumber of valid training triplets in epoch: 140\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0469\tNumber of valid training triplets in epoch: 149\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0471\tNumber of valid training triplets in epoch: 153\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0468\tNumber of valid training triplets in epoch: 163\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0484\tNumber of valid training triplets in epoch: 171\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0491\tNumber of valid training triplets in epoch: 178\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 188\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 195\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0482\tNumber of valid training triplets in epoch: 207\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0479\tNumber of valid training triplets in epoch: 214\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0485\tNumber of valid training triplets in epoch: 219\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0467\tNumber of valid training triplets in epoch: 232\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0467\tNumber of valid training triplets in epoch: 241\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0462\tNumber of valid training triplets in epoch: 253\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0468\tNumber of valid training triplets in epoch: 262\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 267\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 269\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 274\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 281\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0497\tNumber of valid training triplets in epoch: 286\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0501\tNumber of valid training triplets in epoch: 289\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0499\tNumber of valid training triplets in epoch: 294\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0513\tNumber of valid training triplets in epoch: 299\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0513\tNumber of valid training triplets in epoch: 308\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0508\tNumber of valid training triplets in epoch: 320\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 332\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 339\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0502\tNumber of valid training triplets in epoch: 349\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0500\tNumber of valid training triplets in epoch: 356\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 369\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 377\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 384\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 394\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0489\tNumber of valid training triplets in epoch: 401\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0484\tNumber of valid training triplets in epoch: 413\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0485\tNumber of valid training triplets in epoch: 423\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0483\tNumber of valid training triplets in epoch: 436\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0486\tNumber of valid training triplets in epoch: 443\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0488\tNumber of valid training triplets in epoch: 451\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0486\tNumber of valid training triplets in epoch: 462\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0492\tNumber of valid training triplets in epoch: 466\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0495\tNumber of valid training triplets in epoch: 474\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0493\tNumber of valid training triplets in epoch: 484\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0490\tNumber of valid training triplets in epoch: 495\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0496\tNumber of valid training triplets in epoch: 499\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0498\tNumber of valid training triplets in epoch: 506\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 509\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0503\tNumber of valid training triplets in epoch: 517\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0508\tNumber of valid training triplets in epoch: 521\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0511\tNumber of valid training triplets in epoch: 529\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0512\tNumber of valid training triplets in epoch: 536\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0513\tNumber of valid training triplets in epoch: 546\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0510\tNumber of valid training triplets in epoch: 557\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0509\tNumber of valid training triplets in epoch: 565\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0511\tNumber of valid training triplets in epoch: 573\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0512\tNumber of valid training triplets in epoch: 578\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0518\tNumber of valid training triplets in epoch: 583\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0511\tNumber of valid training triplets in epoch: 596\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0504\tNumber of valid training triplets in epoch: 613\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0506\tNumber of valid training triplets in epoch: 619\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0516\tNumber of valid training triplets in epoch: 624\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0514\tNumber of valid training triplets in epoch: 636\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0519\tNumber of valid training triplets in epoch: 642\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0520\tNumber of valid training triplets in epoch: 651\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0519\tNumber of valid training triplets in epoch: 664\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0525\tNumber of valid training triplets in epoch: 669\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0528\tNumber of valid training triplets in epoch: 672\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0529\tNumber of valid training triplets in epoch: 677\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0531\tNumber of valid training triplets in epoch: 685\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0527\tNumber of valid training triplets in epoch: 695\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0529\tNumber of valid training triplets in epoch: 703\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0529\tNumber of valid training triplets in epoch: 710\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0531\tNumber of valid training triplets in epoch: 717\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0530\tNumber of valid training triplets in epoch: 725\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0528\tNumber of valid training triplets in epoch: 738\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0530\tNumber of valid training triplets in epoch: 746\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0531\tNumber of valid training triplets in epoch: 752\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0534\tNumber of valid training triplets in epoch: 760\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0534\tNumber of valid training triplets in epoch: 767\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0537\tNumber of valid training triplets in epoch: 773\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0537\tNumber of valid training triplets in epoch: 775\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0536\tNumber of valid training triplets in epoch: 782\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0535\tNumber of valid training triplets in epoch: 789\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0533\tNumber of valid training triplets in epoch: 798\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0535\tNumber of valid training triplets in epoch: 805\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0539\tNumber of valid training triplets in epoch: 810\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0538\tNumber of valid training triplets in epoch: 819\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0537\tNumber of valid training triplets in epoch: 829\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0536\tNumber of valid training triplets in epoch: 832\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0536\tNumber of valid training triplets in epoch: 838\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0532\tNumber of valid training triplets in epoch: 849\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0531\tNumber of valid training triplets in epoch: 858\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0530\tNumber of valid training triplets in epoch: 866\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0527\tNumber of valid training triplets in epoch: 876\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0527\tNumber of valid training triplets in epoch: 884\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0528\tNumber of valid training triplets in epoch: 891\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0530\tNumber of valid training triplets in epoch: 896\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0533\tNumber of valid training triplets in epoch: 903\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0534\tNumber of valid training triplets in epoch: 908\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0536\tNumber of valid training triplets in epoch: 917\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0538\tNumber of valid training triplets in epoch: 921\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0540\tNumber of valid training triplets in epoch: 927\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0539\tNumber of valid training triplets in epoch: 931\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0539\tNumber of valid training triplets in epoch: 939\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0540\tNumber of valid training triplets in epoch: 946\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0539\tNumber of valid training triplets in epoch: 954\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0539\tNumber of valid training triplets in epoch: 965\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0540\tNumber of valid training triplets in epoch: 972\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0541\tNumber of valid training triplets in epoch: 978\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0541\tNumber of valid training triplets in epoch: 985\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0538\tNumber of valid training triplets in epoch: 996\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 1001\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1005\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1015\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1020\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1026\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1036\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 1043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1050\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1061\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1076\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1083\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1090\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1096\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1106\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1115\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1120\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1131\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 1141\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1151\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1162\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1167\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1172\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1179\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1191\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1200\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1213\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 1218\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1223\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1232\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1241\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1247\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1253\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 1261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 1269\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1280\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1292\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1301\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1307\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1312\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1326\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1332\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1345\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1351\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1357\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1367\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 1376\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1386\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1394\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1398\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1405\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1414\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1421\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1430\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1441\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1449\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1456\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1466\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1480\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1491\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1503\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1508\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1521\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1526\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1534\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1540\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1543\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1550\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1555\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1563\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1571\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1578\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1587\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1596\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1598\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1606\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 1611\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 1618\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 1621\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 1629\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1640\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1647\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1656\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1664\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1671\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1678\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1685\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1695\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1701\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 1708\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1718\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 1724\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 1734\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 1744\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1749\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 1757\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1770\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1776\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1788\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1794\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1803\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1810\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1827\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1837\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1841\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1847\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1857\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1865\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1868\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1877\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1886\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1894\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1899\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1907\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1915\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1923\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0547\tNumber of valid training triplets in epoch: 1932\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1940\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0546\tNumber of valid training triplets in epoch: 1948\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1958\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 1966\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 1977\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 1986\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 1996\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0541\tNumber of valid training triplets in epoch: 2007\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 2012\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2017\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 2022\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 2034\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2044\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2051\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2056\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 2063\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 2072\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0542\tNumber of valid training triplets in epoch: 2081\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2087\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0543\tNumber of valid training triplets in epoch: 2097\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 2105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 2111\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0544\tNumber of valid training triplets in epoch: 2119\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0545\tNumber of valid training triplets in epoch: 2128\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 2132\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 2137\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0548\tNumber of valid training triplets in epoch: 2147\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2155\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2160\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2165\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2171\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2178\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 2184\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2190\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2197\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0553\tNumber of valid training triplets in epoch: 2207\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2213\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2219\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2243\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2254\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2260\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2265\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2275\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2284\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2292\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2301\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2307\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2318\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2328\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2333\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2342\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2349\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2357\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2370\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2380\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2387\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0549\tNumber of valid training triplets in epoch: 2399\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2404\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2410\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2417\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2425\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0550\tNumber of valid training triplets in epoch: 2437\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2441\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2449\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2455\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0551\tNumber of valid training triplets in epoch: 2465\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2472\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2480\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0552\tNumber of valid training triplets in epoch: 2484\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2489\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2496\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2503\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0557\tNumber of valid training triplets in epoch: 2506\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0556\tNumber of valid training triplets in epoch: 2517\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2531\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0556\tNumber of valid training triplets in epoch: 2536\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2546\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2560\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2562\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2571\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2578\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2584\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2589\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2597\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0554\tNumber of valid training triplets in epoch: 2605\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0557\tNumber of valid training triplets in epoch: 2610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0556\tNumber of valid training triplets in epoch: 2623\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2632\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2639\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0555\tNumber of valid training triplets in epoch: 2645\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0556\tNumber of valid training triplets in epoch: 2650\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0556\tNumber of valid training triplets in epoch: 2658\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0557\tNumber of valid training triplets in epoch: 2664\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0557\tNumber of valid training triplets in epoch: 2672\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0557\tNumber of valid training triplets in epoch: 2680\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2684\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2690\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2696\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2706\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2714\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2718\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2723\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2726\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2732\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2743\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2754\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2763\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0558\tNumber of valid training triplets in epoch: 2774\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2778\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2784\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2793\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2802\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0558\tNumber of valid training triplets in epoch: 2814\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2819\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2825\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2831\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2840\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2847\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2853\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 2857\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2867\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 2876\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2887\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2896\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2903\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2910\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2911\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2919\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2929\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2933\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2940\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2944\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0558\tNumber of valid training triplets in epoch: 2957\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0558\tNumber of valid training triplets in epoch: 2965\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2971\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2972\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2979\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2988\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 2997\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 2998\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3008\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3016\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 3021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 3026\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3042\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3044\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3053\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0559\tNumber of valid training triplets in epoch: 3060\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 3066\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 3071\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0560\tNumber of valid training triplets in epoch: 3083\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 3088\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3092\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3098\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3103\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3110\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3119\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3124\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3135\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3140\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3147\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3153\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3162\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3170\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3177\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3182\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3187\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3192\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3197\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3203\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3208\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3214\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3222\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3246\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3259\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3270\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3281\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3287\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3291\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3296\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3305\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3311\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3318\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3321\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3329\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3334\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3340\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3345\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3353\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3358\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3366\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3374\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3379\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3388\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3400\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3408\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 3416\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 3423\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 3428\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3434\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3440\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3448\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3455\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 3465\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3476\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3484\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 3492\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3499\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3503\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3508\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3515\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3520\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3525\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3531\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3541\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3546\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3555\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 3563\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3570\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3575\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3583\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3589\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3592\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3601\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3608\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3611\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3618\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3625\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3633\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3636\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3645\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3652\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3658\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3662\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3668\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3676\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3690\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3695\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3702\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3705\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3714\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3720\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3727\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3737\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3743\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3749\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3755\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3766\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3769\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3776\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3786\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3789\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3794\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3799\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3807\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3814\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3822\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3827\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3837\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3851\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3858\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3865\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3875\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3882\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3890\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3902\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3911\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3920\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3924\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3933\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3942\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 3948\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 3959\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3962\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3966\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 3976\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3986\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 3993\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4001\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4007\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4014\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4022\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4031\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4037\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4051\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4059\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4067\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 4074\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4079\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4084\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4090\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4095\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4102\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4111\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 4121\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4128\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4133\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4141\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4147\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4150\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4157\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4163\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4168\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 4177\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4187\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4191\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4199\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4207\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4213\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 4226\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 4243\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4250\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4257\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4268\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4278\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4284\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4293\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4295\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4299\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4308\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4317\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4323\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4334\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4337\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4343\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4349\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4356\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4371\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4385\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4393\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4405\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4413\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4428\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4438\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4447\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4460\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4466\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4475\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4480\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4487\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4491\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4498\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4506\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4519\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4525\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4533\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4540\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4548\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4550\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4554\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4558\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4566\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4573\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4583\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4592\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4597\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4602\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4608\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4614\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4622\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4627\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4633\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4638\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4646\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4654\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4661\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4666\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4674\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4681\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 4687\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4695\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 4705\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4715\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4721\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4730\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4737\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4744\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4753\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4759\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4765\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4771\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4781\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4789\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4795\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4804\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4812\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4820\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4832\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4835\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4839\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4850\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4859\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4864\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4872\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4878\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4884\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4888\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4895\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4902\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4914\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4919\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4927\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4934\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4944\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4957\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4967\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0561\tNumber of valid training triplets in epoch: 4971\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4973\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4980\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4990\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0562\tNumber of valid training triplets in epoch: 4995\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 4997\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5007\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5013\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5016\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5022\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5032\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5040\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5055\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5065\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5072\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5075\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5080\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5089\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5100\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5115\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5122\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5130\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5138\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5141\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5143\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5149\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5154\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5164\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5171\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5176\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5180\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5185\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5193\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5201\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5210\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5220\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5226\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 5231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5237\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5247\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5256\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5266\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5276\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5285\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5288\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5297\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5309\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5317\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5319\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5323\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5328\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5334\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5337\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5343\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5349\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5360\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5368\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5380\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5387\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5392\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5401\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5414\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5422\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5427\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5434\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5443\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5452\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5459\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0563\tNumber of valid training triplets in epoch: 5463\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5467\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5470\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5478\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5484\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5491\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5495\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5502\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5509\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5519\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5526\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5534\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5536\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5539\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5544\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5552\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5559\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5563\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5569\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5577\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5582\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5592\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5603\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5616\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5622\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5629\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5635\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5642\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5645\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5654\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5660\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5664\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5673\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5678\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5683\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5693\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5700\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5709\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5721\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5729\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5732\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5737\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5747\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5755\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0565\tNumber of valid training triplets in epoch: 5758\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0564\tNumber of valid training triplets in epoch: 5770\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5771\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5778\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5786\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0566\tNumber of valid training triplets in epoch: 5793\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 5796\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 5805\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5808\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5816\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5827\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5831\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5837\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5846\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5849\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 5855\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0567\tNumber of valid training triplets in epoch: 5862\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5868\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5875\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5881\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5887\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5893\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5902\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5909\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5917\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0568\tNumber of valid training triplets in epoch: 5925\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0569\tNumber of valid training triplets in epoch: 5930\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0569\tNumber of valid training triplets in epoch: 5935\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5940\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0569\tNumber of valid training triplets in epoch: 5944\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0569\tNumber of valid training triplets in epoch: 5948\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5954\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5961\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5967\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5970\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5983\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5990\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5993\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 5995\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6000\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6004\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6014\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6026\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6035\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6036\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6045\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6052\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6062\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6065\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6071\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6080\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6087\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6092\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6095\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6103\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6111\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6118\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6124\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6132\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6140\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6146\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6150\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6159\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6168\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6175\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6182\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6193\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6199\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6211\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6218\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6221\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6229\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6238\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6244\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6253\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6259\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6268\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6276\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6283\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6290\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6294\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6307\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6316\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6321\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6324\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6328\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6334\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6340\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6346\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6349\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6357\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6365\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6375\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6385\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6390\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6397\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6400\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6411\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6418\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6425\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6429\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6435\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6443\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6458\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6473\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6481\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0570\tNumber of valid training triplets in epoch: 6488\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6495\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6505\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0571\tNumber of valid training triplets in epoch: 6513\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6517\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6525\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6529\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6538\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6541\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0572\tNumber of valid training triplets in epoch: 6546\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6552\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6560\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6567\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6570\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6575\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6585\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6595\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6600\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6606\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6615\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6623\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6630\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6631\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6642\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6649\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0573\tNumber of valid training triplets in epoch: 6655\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6658\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6665\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6673\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6682\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6692\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6699\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6709\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6715\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6722\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0575\tNumber of valid training triplets in epoch: 6726\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6734\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6740\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6746\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6755\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6764\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6771\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6782\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0574\tNumber of valid training triplets in epoch: 6791\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0575\tNumber of valid training triplets in epoch: 6794\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6798\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6802\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6807\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6812\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6824\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6832\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6840\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6850\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6855\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0575\tNumber of valid training triplets in epoch: 6871\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6874\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6880\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6887\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6896\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6903\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6908\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6916\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6925\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6927\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6937\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6944\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6956\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6962\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6972\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 6977\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6985\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 6993\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 7001\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 7006\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0576\tNumber of valid training triplets in epoch: 7016\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7025\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7035\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7050\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7055\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7061\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7067\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7076\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7082\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7085\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7089\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7097\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7099\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7113\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7122\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7130\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7139\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7146\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7155\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7160\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7169\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7174\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7182\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7191\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7198\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7211\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7218\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7222\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7228\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7233\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7241\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7247\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7252\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7260\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7266\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7272\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7275\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7283\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7290\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7296\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7298\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7310\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7316\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7324\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7336\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7340\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7347\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7354\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7368\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7374\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7392\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7400\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7404\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 7406\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7418\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7427\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7433\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7439\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7446\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7454\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 7459\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7470\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7477\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7486\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7492\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7505\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7514\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7519\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7526\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7532\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7539\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7546\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7553\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7560\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7569\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7571\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7581\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7587\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7591\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7598\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7605\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7612\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7619\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7625\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7632\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7640\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7650\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7656\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7663\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7672\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7679\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7691\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7700\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7708\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7715\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7722\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7729\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7731\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0577\tNumber of valid training triplets in epoch: 7742\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7746\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7750\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7754\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7759\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7763\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7772\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7777\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7786\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7797\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7800\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7807\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7814\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0578\tNumber of valid training triplets in epoch: 7819\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7823\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7828\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7834\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7843\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7852\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7858\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7862\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7866\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7873\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7879\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7882\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7890\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7893\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7898\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 7899\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7908\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7910\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7918\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7926\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7936\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7945\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7949\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7954\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7958\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 7966\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7976\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7983\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7989\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7995\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0579\tNumber of valid training triplets in epoch: 7999\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8007\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8012\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8018\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8029\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8032\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8051\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8060\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8068\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8076\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0580\tNumber of valid training triplets in epoch: 8082\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8085\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8087\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8097\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8106\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8110\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8114\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8123\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8133\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8142\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8150\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0581\tNumber of valid training triplets in epoch: 8152\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0582\tNumber of valid training triplets in epoch: 8153\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0582\tNumber of valid training triplets in epoch: 8158\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0582\tNumber of valid training triplets in epoch: 8166\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0582\tNumber of valid training triplets in epoch: 8171\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0583\tNumber of valid training triplets in epoch: 8175\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0583\tNumber of valid training triplets in epoch: 8182\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0583\tNumber of valid training triplets in epoch: 8188\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0584\tNumber of valid training triplets in epoch: 8191\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0583\tNumber of valid training triplets in epoch: 8201\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0584\tNumber of valid training triplets in epoch: 8207\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0584\tNumber of valid training triplets in epoch: 8213\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0584\tNumber of valid training triplets in epoch: 8220\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8225\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8232\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8241\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8248\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8256\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8271\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8275\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8283\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8289\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8298\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8303\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0585\tNumber of valid training triplets in epoch: 8308\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8309\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8317\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8325\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8330\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8340\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8348\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8356\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8371\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0586\tNumber of valid training triplets in epoch: 8376\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8378\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8388\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8394\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8399\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8404\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8410\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0587\tNumber of valid training triplets in epoch: 8417\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0588\tNumber of valid training triplets in epoch: 8421\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0588\tNumber of valid training triplets in epoch: 8425\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8431\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8436\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8441\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8445\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8446\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8454\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8460\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8475\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8483\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8493\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8496\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8503\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8516\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8525\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8538\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8543\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8549\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8556\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8566\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8577\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8584\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8591\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8595\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8599\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8604\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8616\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8624\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8632\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8641\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8646\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8652\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0589\tNumber of valid training triplets in epoch: 8655\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8659\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8669\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8674\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8684\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8688\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8695\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8702\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8709\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8710\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0590\tNumber of valid training triplets in epoch: 8725\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8728\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8736\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8743\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8746\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8754\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8759\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8766\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8774\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8778\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8785\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8788\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8794\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8800\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8808\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0591\tNumber of valid training triplets in epoch: 8818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8820\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8823\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8830\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8834\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8837\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8847\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8858\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8864\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8867\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8875\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8880\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8886\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8893\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8900\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8911\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8920\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8924\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8931\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8934\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8941\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8945\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0592\tNumber of valid training triplets in epoch: 8957\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8961\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8967\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8975\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8981\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8987\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0593\tNumber of valid training triplets in epoch: 8993\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 8997\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9006\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9013\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9023\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9026\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9036\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9045\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9052\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9059\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9063\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9068\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9075\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9087\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0594\tNumber of valid training triplets in epoch: 9095\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9099\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9112\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9113\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9116\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9127\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9131\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9137\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9145\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9151\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9154\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0595\tNumber of valid training triplets in epoch: 9157\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9161\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9168\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9174\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9180\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9187\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9193\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9198\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9210\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0596\tNumber of valid training triplets in epoch: 9215\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9222\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9228\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9234\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9239\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9246\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9251\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9265\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9272\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9280\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9284\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9291\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9297\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9299\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9307\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9316\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9318\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9325\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9330\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9338\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9342\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9351\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9356\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9363\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9371\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9374\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9377\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9384\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9393\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9402\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9407\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9417\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9429\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9440\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9446\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9452\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9459\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9465\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9478\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9485\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9489\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0597\tNumber of valid training triplets in epoch: 9494\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9499\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9502\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9507\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9510\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9517\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9525\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9535\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9537\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9543\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9553\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9558\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9562\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9566\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9578\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9586\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9595\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9600\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9611\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9615\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9621\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9628\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9634\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9641\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9651\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9661\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9668\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9671\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9676\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9685\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9691\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9697\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9706\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9714\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9721\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9725\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9733\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 9742\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9745\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9752\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9763\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9769\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9776\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9778\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9782\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9785\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9791\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9794\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9803\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9811\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9822\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9826\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9829\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9833\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9839\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9847\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9855\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9863\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9867\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9874\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9884\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9889\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9897\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9904\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9908\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9913\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9919\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9924\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9931\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9940\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9945\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9953\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9959\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9967\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9971\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9974\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9980\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 9986\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 9999\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10006\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10013\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10024\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10027\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10029\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10037\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10051\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10057\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10068\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10072\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10076\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10083\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10088\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10097\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10107\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10111\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10116\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10121\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10129\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10135\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10142\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10147\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10150\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10158\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10170\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10177\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10182\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10187\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10195\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10204\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10215\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10221\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10238\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10241\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10252\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10265\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10274\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10277\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10283\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10289\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10295\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10306\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10313\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10322\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10330\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10334\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10340\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10351\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10359\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10364\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10369\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10375\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10380\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10388\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10394\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10401\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10408\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10415\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10421\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10430\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10439\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10451\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10459\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10462\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10475\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10482\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10493\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10496\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10507\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10514\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10521\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10529\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10534\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10542\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10550\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0598\tNumber of valid training triplets in epoch: 10556\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10562\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10567\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10573\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10576\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10583\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10586\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10594\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10602\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10618\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10620\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10628\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10636\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10643\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10654\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10661\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10668\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10673\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10680\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10688\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10694\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10698\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10704\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10710\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10718\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10727\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10734\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10737\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10749\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10753\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10760\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10769\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10773\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10781\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10791\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10797\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10801\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 10808\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10815\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10822\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10833\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10838\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10846\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10854\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10862\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10870\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10873\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10877\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10885\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10889\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10895\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10900\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10911\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10919\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10928\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10934\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10941\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10948\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10954\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10962\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10968\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10975\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 10979\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10990\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 10997\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11005\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11013\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11022\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11030\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11036\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11042\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11052\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11062\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11068\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0599\tNumber of valid training triplets in epoch: 11080\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11084\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11088\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11094\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11102\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11117\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11123\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11129\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11137\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11150\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11157\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11164\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11169\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11174\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11177\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11184\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11193\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11202\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0600\tNumber of valid training triplets in epoch: 11205\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11210\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11215\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11219\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11227\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11234\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11237\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11246\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11252\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11257\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11264\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11268\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11272\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11278\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11285\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11293\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11300\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0601\tNumber of valid training triplets in epoch: 11308\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11312\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11321\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11324\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11327\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11331\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11336\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0602\tNumber of valid training triplets in epoch: 11342\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11345\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11347\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11352\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11369\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11376\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11380\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11386\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11391\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11401\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11411\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11414\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11422\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11433\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11436\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11437\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11444\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11449\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11453\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11457\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11466\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11477\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11482\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11488\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11498\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11506\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11512\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11517\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11523\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11529\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11540\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11542\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11551\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11559\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11568\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11575\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11582\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11587\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11596\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11604\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11615\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11621\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11626\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11631\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11638\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11644\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11649\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11660\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11671\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11676\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11683\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11691\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11702\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11707\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11714\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11725\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11734\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11742\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11747\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11755\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11764\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11771\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11775\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11779\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11785\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11789\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11796\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11798\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11804\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11808\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11812\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11824\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11830\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11837\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11842\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0603\tNumber of valid training triplets in epoch: 11852\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11856\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11862\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11866\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11873\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11882\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11887\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11893\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11898\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11906\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11917\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11924\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11928\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11936\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11941\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11946\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11950\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 11953\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11957\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11966\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11968\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11973\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11979\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11984\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11994\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 11999\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12008\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12018\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12021\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12030\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12037\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12041\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12046\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12052\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12058\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12065\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12073\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12079\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12088\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12093\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12098\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12103\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12112\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12122\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12128\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12138\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12142\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12148\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12154\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12160\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12168\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12177\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12185\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12195\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12200\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12206\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12212\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12218\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12227\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12234\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12239\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12243\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12248\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12255\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12261\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12267\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12273\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12279\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12287\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12298\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12307\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12316\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12324\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12328\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12332\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12342\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12347\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12355\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12361\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12369\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12374\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12380\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12386\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12392\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12397\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12407\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12412\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12418\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12424\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12428\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12434\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12440\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12448\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12455\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12457\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12466\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12472\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12477\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12486\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12490\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12493\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12499\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12504\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12513\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12516\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12526\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12534\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12540\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12547\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12553\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12567\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12576\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12585\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12591\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12603\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12606\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12610\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12614\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12623\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12628\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12631\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12638\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12644\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12654\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12660\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12667\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12675\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12683\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0604\tNumber of valid training triplets in epoch: 12688\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12692\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12696\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12702\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12708\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12712\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12720\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12728\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12732\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12736\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12742\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12747\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12752\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12759\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0605\tNumber of valid training triplets in epoch: 12764\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12767\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12773\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12776\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12786\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12792\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12798\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12805\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12813\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12819\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12825\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12832\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12839\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12841\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12846\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12850\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0606\tNumber of valid training triplets in epoch: 12857\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12858\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12861\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12869\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12880\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12882\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0607\tNumber of valid training triplets in epoch: 12886\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12894\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12901\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12905\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12913\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12916\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12921\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12925\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12929\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12932\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12941\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12949\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12952\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12954\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12961\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12971\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12974\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 12984\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 12989\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 12995\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 12999\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13006\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13015\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 13027\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13033\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13037\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13043\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13047\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13053\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13057\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13062\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13069\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13077\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13082\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13095\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13102\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13109\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13117\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13121\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13129\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13138\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13143\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 13156\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 13163\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 13169\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0608\tNumber of valid training triplets in epoch: 13173\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13179\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13186\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13190\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13198\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13202\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13212\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13221\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13224\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13229\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13232\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13238\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13243\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13245\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13249\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13258\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13263\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13270\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13278\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13283\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13289\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13296\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13299\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13305\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13312\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13326\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13337\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13342\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13346\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13358\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13362\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13368\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13372\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13377\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13381\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13386\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13392\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13399\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13402\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13407\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 13412\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 13417\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13430\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13436\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13448\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13453\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 13456\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13461\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13469\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13481\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13488\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13495\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13501\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13506\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13513\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13521\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13530\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13537\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13544\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13550\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13561\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13568\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13574\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13578\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13581\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13588\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13593\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13599\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13611\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13618\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13631\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13638\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13645\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13653\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13661\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13666\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13670\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13675\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13683\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13693\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13696\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13708\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13710\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13715\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13724\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13728\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13737\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13741\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13750\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13757\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0609\tNumber of valid training triplets in epoch: 13762\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13768\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13774\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13781\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13789\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13792\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13793\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13804\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13811\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13818\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13826\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13833\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13840\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13850\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13855\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13864\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13873\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13882\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 13884\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13894\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13903\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13908\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13912\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13920\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13930\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13936\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13938\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13945\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13953\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13961\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13969\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13980\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13984\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 13993\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 13999\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14003\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14010\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14020\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14029\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14034\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14046\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14053\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14058\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14063\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14068\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14075\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14079\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14085\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14092\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14101\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14105\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14109\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14116\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14122\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14128\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14135\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14140\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14145\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14152\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14163\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14170\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14176\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14187\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14193\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14203\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14205\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14216\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14222\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14225\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14231\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0611\tNumber of valid training triplets in epoch: 14237\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 14246\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 14249\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 14257\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 14264\n",
            "Epoch 5:\tAverage Triplet Loss: 0.0610\tNumber of valid training triplets in epoch: 14275\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 119, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/drive/My Drive/face_recognition/Triplet_trainer.py\", line 49, in train\n",
            "    self.train_epoch(epoch, 'train')\n",
            "  File \"/content/drive/My Drive/face_recognition/Triplet_trainer.py\", line 72, in train_epoch\n",
            "    anc_embedding, pos_embedding, neg_embedding = self.model(anc_img), self.model(pos_img), self.model(neg_img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/My Drive/face_recognition/SEResNet_IR.py\", line 161, in forward\n",
            "    x = self.layer4(x)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 117, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/My Drive/face_recognition/SEResNet_IR.py\", line 91, in forward\n",
            "    out = self.relu(out)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py\", line 988, in forward\n",
            "    return F.prelu(input, self.weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1319, in prelu\n",
            "    return torch.prelu(input, weight)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.97 GiB already allocated; 1.88 MiB free; 15.12 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXoJNzy2Yesf",
        "colab_type": "text"
      },
      "source": [
        "**Triplet（Scratch）CosFace+ResNet18-IR**\n",
        "\n",
        "Epoch 4:\n",
        "\n",
        "Eval Epoch Average Acc: 0.7968, Average Threshold: 0.2445\n",
        "\n",
        "Fail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calMP6IQiC4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa0a20ce-c713-4dd3-9ddb-4636e8026b78"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "ArcFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:22.821655\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:22.069271\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:21.486652\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:21.248180\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:20.562748\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:20.032614\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:19.383581\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:19.549032\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:18.844700\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:18.467501\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:17.670378\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:17.728683\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:16.096275\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch Loss: 19.657280 Accuracy: 0.000526\n",
            "Eval Epoch Average Acc: 0.7550, Average Threshold: 0.2394\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:16.828857\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:16.072170\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:14.912889\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:13.903936\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:12.341366\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:15.675777\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:17.854858\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:17.710951\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:18.832958\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:16.525255\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:17.801237\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:20.706711\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:19.624571\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 17.301800 Accuracy: 0.000588\n",
            "Eval Epoch Average Acc: 0.5938, Average Threshold: 0.3982\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:19.548069\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:19.469786\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:19.850937\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:19.474070\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:19.553995\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:17.942566\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:19.723785\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:20.060266\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:20.088612\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:19.049387\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:20.918976\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:20.403591\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:20.040657\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 20.065969 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7225, Average Threshold: 0.2629\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:20.503252\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:20.786518\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:20.903034\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:20.824837\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:21.166527\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:19.379396\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:20.260071\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:20.593233\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:20.225609\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:20.523268\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:20.018267\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:18.933178\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:19.133707\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 19.556225 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7293, Average Threshold: 0.2846\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:19.145527\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:18.761070\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:18.732285\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:18.378777\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:18.286257\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:18.976385\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:17.323881\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:20.250843\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:18.724388\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:18.694029\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:19.714109\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:18.032171\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:20.275318\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 19.088806 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7193, Average Threshold: 0.3348\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:20.061687\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:19.513315\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:19.401516\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:18.897043\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:18.777208\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:18.868811\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:18.777298\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:17.910017\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:19.547550\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:19.658993\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:18.468866\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:19.380543\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:18.395273\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.983196 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7207, Average Threshold: 0.3151\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:18.599152\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:19.363762\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:19.377472\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:17.809256\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:18.712379\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:18.137589\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:18.862747\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:18.966732\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:19.886808\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:19.605810\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:19.316452\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:18.914051\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:18.856766\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.904734 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7160, Average Threshold: 0.3618\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:19.135479\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:18.671453\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:18.992674\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:18.657946\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:18.340492\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:19.090515\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:19.846420\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:18.682346\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:19.106382\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:18.396423\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:18.176214\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:15.916224\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:19.620729\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.889780 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7223, Average Threshold: 0.3065\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:18.473410\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:19.613951\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:19.974998\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:18.135443\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:18.639179\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:19.587391\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:18.783575\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:18.234495\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:18.402134\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:19.669697\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:18.375879\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:18.138250\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:19.759371\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.821995 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7275, Average Threshold: 0.3337\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:19.412247\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:17.891659\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:18.307133\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:19.562382\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:17.944111\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:19.443859\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:19.622414\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:19.128811\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:17.785563\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:19.678654\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:18.815849\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:18.058994\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:18.517488\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.734726 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7202, Average Threshold: 0.3625\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:18.965605\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:19.209736\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:18.988886\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:19.195736\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:18.773600\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:17.891460\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:19.135098\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:17.972513\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:17.699728\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:19.474218\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:19.050743\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:19.102407\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:18.111784\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.693916 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7292, Average Threshold: 0.3052\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:18.603922\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:19.240486\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:17.899174\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:19.236189\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:18.972372\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:19.456282\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:17.968164\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:17.969065\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:20.352440\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:17.647236\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:19.224941\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:18.800976\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:19.012962\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.661469 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7315, Average Threshold: 0.3466\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:18.763901\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:18.117756\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:18.884161\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:19.120712\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:18.426250\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:17.696133\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:18.141745\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:18.886721\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:19.225721\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:18.584093\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:19.737419\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:17.993319\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:18.061100\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.643730 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7280, Average Threshold: 0.3110\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:19.232323\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:19.382454\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:19.123636\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:18.108639\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:18.840908\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:19.358250\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:17.961655\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:19.197004\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:18.903873\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:19.053799\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:18.673944\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:19.018793\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:19.285564\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.638235 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7290, Average Threshold: 0.3102\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:18.241856\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:19.458733\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:18.245380\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:18.119297\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:19.310343\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:18.484716\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:18.493263\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:19.062046\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:18.599760\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:17.804749\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:18.691422\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:18.075996\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:18.374056\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.642004 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7280, Average Threshold: 0.3077\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:18.266949\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:18.650625\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:18.334993\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:18.538141\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:17.908653\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:18.700741\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:18.045101\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:18.475428\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:18.896872\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:18.673506\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:18.349009\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:17.748411\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:19.540833\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.641548 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7150, Average Threshold: 0.3227\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:18.039415\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:18.351152\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:19.089710\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:17.637783\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:19.057329\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:18.426437\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:18.153057\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:19.205193\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:18.085648\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:18.678045\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:19.370598\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:17.680780\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:18.318697\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.612225 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7283, Average Threshold: 0.3033\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:18.924198\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:18.256474\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:19.367422\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:19.583435\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:18.579731\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:19.104109\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:19.811316\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:18.182596\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:18.405596\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:18.196478\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:18.505924\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:17.913363\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:18.982229\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.616838 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7318, Average Threshold: 0.2796\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:17.049442\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:17.528349\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:18.703087\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:19.552643\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:17.964548\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:19.648350\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:17.601053\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:18.758226\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:17.935726\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:18.468700\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:18.603264\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:19.490503\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:18.988762\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 18.590660 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7185, Average Threshold: 0.3211\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:19.351292\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:18.088261\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:18.889051\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:18.785912\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:18.749590\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:18.518280\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:18.994432\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:18.517582\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:18.238264\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:19.619158\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:18.136297\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:19.321804\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:18.980453\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 18.602003 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7270, Average Threshold: 0.3615\n",
            "Best acc on LFW: 0.755, best threshold: 0.239405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT-xYVESZzAJ",
        "colab_type": "text"
      },
      "source": [
        "**ArcFace+ResNet34-IR:**\n",
        "\n",
        "Train Epoch Loss: 18.602003 \n",
        "\n",
        "Accuracy: 0.000000\n",
        "\n",
        "Eval Epoch Average Acc: 0.7270\n",
        "\n",
        "Average Threshold: 0.3615\n",
        "\n",
        "Best acc on LFW: **0.755**\n",
        "\n",
        "best threshold: 0.239405"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnyyauFPc8iQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dadcbab0-6c2c-49a6-8d8d-aa47bd36c566"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "SphereFace\n",
            "/content/drive/My Drive/face_recognition/loss_function.py:119: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(self.weight)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:7.395148\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:6.227127\tAcc:0.046875 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:6.073266\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:5.637140\tAcc:0.031250 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:4.746840\tAcc:0.125000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:4.928899\tAcc:0.171875 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:5.055028\tAcc:0.117188 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:4.708014\tAcc:0.195312 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:3.934722\tAcc:0.226562 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:4.053627\tAcc:0.250000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:4.017606\tAcc:0.203125 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:3.690134\tAcc:0.296875 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:3.238419\tAcc:0.375000 LR:0.0010000\n",
            "Train Epoch Loss: 4.596758 Accuracy: 0.197105\n",
            "Eval Epoch Average Acc: 0.8137, Average Threshold: 0.2206\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:2.881994\tAcc:0.484375 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:3.081132\tAcc:0.445312 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:2.659510\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:2.744286\tAcc:0.460938 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:2.595872\tAcc:0.484375 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:2.643078\tAcc:0.507812 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:2.594635\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:2.573006\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:2.688310\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:2.505168\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:2.054292\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:1.978743\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:2.519194\tAcc:0.453125 LR:0.0010000\n",
            "Train Epoch Loss: 2.630500 Accuracy: 0.486646\n",
            "Eval Epoch Average Acc: 0.8212, Average Threshold: 0.2065\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:2.078467\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:1.872687\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.673302\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:1.771439\tAcc:0.625000 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.990718\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:2.342917\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:1.962771\tAcc:0.609375 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:2.257264\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:2.364614\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:2.354766\tAcc:0.554688 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:1.682845\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:1.978007\tAcc:0.570312 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:1.782545\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch Loss: 1.932773 Accuracy: 0.612960\n",
            "Eval Epoch Average Acc: 0.8297, Average Threshold: 0.2104\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:1.605283\tAcc:0.664062 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:1.426243\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:1.865239\tAcc:0.640625 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:1.450480\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.626586\tAcc:0.656250 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:1.623069\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:1.621444\tAcc:0.656250 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:1.354843\tAcc:0.718750 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:1.725587\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:1.718630\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.410571\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:1.556143\tAcc:0.710938 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.959272\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch Loss: 1.539053 Accuracy: 0.683533\n",
            "Eval Epoch Average Acc: 0.8548, Average Threshold: 0.1838\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:1.198700\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:0.911780\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.755695\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:1.030098\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.862869\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:0.980453\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:0.963898\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:0.856283\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.985316\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:0.989969\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.689993\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:0.777391\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:1.032840\tAcc:0.781250 LR:0.0001000\n",
            "Train Epoch Loss: 0.943516 Accuracy: 0.811829\n",
            "Eval Epoch Average Acc: 0.8552, Average Threshold: 0.1613\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.733659\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:0.956451\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.968133\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:0.859627\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.987902\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:0.989027\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.934932\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:0.763529\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.668140\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:0.882129\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.897540\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:0.924688\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.907841\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch Loss: 0.845179 Accuracy: 0.834977\n",
            "Eval Epoch Average Acc: 0.8577, Average Threshold: 0.1768\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.794960\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.745762\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.597416\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:1.035292\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.580757\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.766339\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.669679\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:0.847536\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:1.084429\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:0.937642\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.555469\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.695612\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.669048\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch Loss: 0.795120 Accuracy: 0.847023\n",
            "Eval Epoch Average Acc: 0.8603, Average Threshold: 0.1674\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.687673\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.435297\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.745150\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.620506\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.696150\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.822551\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.515765\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.940168\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.790618\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.972990\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.893352\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.885438\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.762302\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch Loss: 0.764112 Accuracy: 0.854316\n",
            "Eval Epoch Average Acc: 0.8608, Average Threshold: 0.1648\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.710407\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.792393\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.623392\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.632615\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.890956\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.841372\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.608111\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.742686\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.675396\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.812320\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.779865\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.880055\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.859110\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch Loss: 0.734710 Accuracy: 0.859735\n",
            "Eval Epoch Average Acc: 0.8652, Average Threshold: 0.1675\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.641912\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.552296\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.682952\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.651432\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.497127\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.479864\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.639934\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.543069\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.664876\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.668290\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.743956\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.912268\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.623134\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch Loss: 0.707022 Accuracy: 0.864133\n",
            "Eval Epoch Average Acc: 0.8630, Average Threshold: 0.1457\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.779019\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.565336\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.726579\tAcc:0.867188 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.459864\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.606759\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.760894\tAcc:0.882812 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.592466\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.528184\tAcc:0.867188 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.612364\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.603146\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.859502\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.553537\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.806169\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch Loss: 0.637213 Accuracy: 0.883115\n",
            "Eval Epoch Average Acc: 0.8635, Average Threshold: 0.1596\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.500836\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.717099\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.671526\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.801176\tAcc:0.843750 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.668108\tAcc:0.835938 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.631701\tAcc:0.882812 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.412897\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.698944\tAcc:0.867188 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.883183\tAcc:0.804688 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.943728\tAcc:0.804688 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.970378\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.924354\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.694472\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch Loss: 0.721951 Accuracy: 0.863622\n",
            "Eval Epoch Average Acc: 0.8613, Average Threshold: 0.1628\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.669805\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.477217\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.987947\tAcc:0.820312 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.943708\tAcc:0.804688 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.803295\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.772579\tAcc:0.843750 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.746412\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.950378\tAcc:0.789062 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.797655\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:1.183879\tAcc:0.765625 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.871548\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.743644\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.745343\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch Loss: 0.835122 Accuracy: 0.835705\n",
            "Eval Epoch Average Acc: 0.8645, Average Threshold: 0.1685\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.963605\tAcc:0.781250 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.792560\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:1.021987\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:1.260620\tAcc:0.750000 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.787532\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.857854\tAcc:0.835938 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.848747\tAcc:0.828125 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.957309\tAcc:0.789062 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.943911\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.998264\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.910887\tAcc:0.789062 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:1.012014\tAcc:0.742188 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.798351\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch Loss: 0.970588 Accuracy: 0.800526\n",
            "Eval Epoch Average Acc: 0.8637, Average Threshold: 0.1652\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:1.340932\tAcc:0.710938 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:1.044118\tAcc:0.812500 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.990296\tAcc:0.835938 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:1.012601\tAcc:0.789062 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:1.051003\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:1.114713\tAcc:0.773438 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.926412\tAcc:0.820312 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:1.270526\tAcc:0.726562 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:1.066242\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:1.254237\tAcc:0.750000 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:1.079060\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:1.226435\tAcc:0.726562 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:1.202439\tAcc:0.726562 LR:0.0000100\n",
            "Train Epoch Loss: 1.118635 Accuracy: 0.760331\n",
            "Eval Epoch Average Acc: 0.8638, Average Threshold: 0.1730\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:1.120726\tAcc:0.757812 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:1.180322\tAcc:0.750000 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:1.144845\tAcc:0.750000 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:1.240573\tAcc:0.703125 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:1.488282\tAcc:0.710938 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:1.300101\tAcc:0.679688 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:1.201090\tAcc:0.710938 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:1.393959\tAcc:0.734375 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:1.536557\tAcc:0.718750 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:1.280316\tAcc:0.679688 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:1.359644\tAcc:0.710938 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:1.504416\tAcc:0.664062 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:1.694608\tAcc:0.632812 LR:0.0000100\n",
            "Train Epoch Loss: 1.308046 Accuracy: 0.709128\n",
            "Eval Epoch Average Acc: 0.8637, Average Threshold: 0.1709\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:1.132732\tAcc:0.765625 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:1.310202\tAcc:0.664062 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:1.377250\tAcc:0.695312 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:1.347851\tAcc:0.687500 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:1.511331\tAcc:0.679688 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:1.645998\tAcc:0.601562 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:1.660161\tAcc:0.585938 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:1.552680\tAcc:0.656250 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:1.515402\tAcc:0.601562 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:1.479378\tAcc:0.679688 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:1.733830\tAcc:0.648438 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:1.589798\tAcc:0.617188 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:1.612846\tAcc:0.578125 LR:0.0000100\n",
            "Train Epoch Loss: 1.503126 Accuracy: 0.655880\n",
            "Eval Epoch Average Acc: 0.8637, Average Threshold: 0.1676\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:1.707685\tAcc:0.656250 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:1.702314\tAcc:0.617188 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:1.518937\tAcc:0.687500 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:1.624307\tAcc:0.609375 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:1.831687\tAcc:0.570312 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:1.589952\tAcc:0.648438 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:1.977636\tAcc:0.546875 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:2.115520\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:1.799112\tAcc:0.625000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:1.737279\tAcc:0.578125 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:1.795456\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:1.897960\tAcc:0.585938 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:1.743540\tAcc:0.632812 LR:0.0000100\n",
            "Train Epoch Loss: 1.726736 Accuracy: 0.601781\n",
            "Eval Epoch Average Acc: 0.8637, Average Threshold: 0.1687\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:1.811692\tAcc:0.601562 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:1.908138\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:2.045578\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:2.141405\tAcc:0.523438 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:1.969065\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:1.993387\tAcc:0.585938 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:1.696478\tAcc:0.562500 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:2.170134\tAcc:0.531250 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:1.755331\tAcc:0.562500 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:1.967263\tAcc:0.593750 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:2.480490\tAcc:0.445312 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:1.903421\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:1.925990\tAcc:0.539062 LR:0.0000010\n",
            "Train Epoch Loss: 1.971066 Accuracy: 0.541457\n",
            "Eval Epoch Average Acc: 0.8625, Average Threshold: 0.1664\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:2.038523\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:2.341247\tAcc:0.468750 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:2.058023\tAcc:0.507812 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:2.461996\tAcc:0.460938 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:2.440514\tAcc:0.468750 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:2.570303\tAcc:0.437500 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:2.557097\tAcc:0.421875 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:2.391788\tAcc:0.484375 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:2.632368\tAcc:0.437500 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:2.289936\tAcc:0.460938 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:2.911615\tAcc:0.367188 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:2.665800\tAcc:0.398438 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:2.179828\tAcc:0.523438 LR:0.0000010\n",
            "Train Epoch Loss: 2.294474 Accuracy: 0.472107\n",
            "Eval Epoch Average Acc: 0.8628, Average Threshold: 0.1663\n",
            "Best acc on LFW: 0.8651666666666665, best threshold: 0.16747499999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "watq3PFAOWTZ",
        "colab_type": "text"
      },
      "source": [
        "**SphereFace+ResNet34-IR:**\n",
        "\n",
        "Train Epoch Loss: 2.294474\n",
        "\n",
        "Accuracy: 0.472107\n",
        "\n",
        "Eval Epoch Average Acc: 0.8628\n",
        "\n",
        "Average Threshold: 0.1663\n",
        "\n",
        "Best acc on LFW: **0.8651666666666665**\n",
        "\n",
        "best threshold: 0.16747499999999998"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmWid6MzizEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37ea78f0-034f-4fd6-bcc8-1a2b4201141f"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "OHEM(normface基础上)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:7.327922\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:6.734725\tAcc:0.031250 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:6.691010\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:6.498085\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:6.160120\tAcc:0.078125 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:6.175959\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:6.019506\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:5.853946\tAcc:0.125000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:5.827513\tAcc:0.101562 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:5.765893\tAcc:0.085938 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:5.765883\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:5.600215\tAcc:0.164062 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:5.574379\tAcc:0.187500 LR:0.0010000\n",
            "Train Epoch Loss: 6.080790 Accuracy: 0.082620\n",
            "Eval Epoch Average Acc: 0.7718, Average Threshold: 0.2858\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:5.272663\tAcc:0.250000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:5.409843\tAcc:0.187500 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:5.534387\tAcc:0.148438 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:5.190042\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:5.419510\tAcc:0.164062 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:5.381866\tAcc:0.179688 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:4.893280\tAcc:0.218750 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:5.131374\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:5.127600\tAcc:0.296875 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:4.735686\tAcc:0.335938 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:4.821205\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:4.783147\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:5.062700\tAcc:0.296875 LR:0.0010000\n",
            "Train Epoch Loss: 5.064494 Accuracy: 0.258373\n",
            "Eval Epoch Average Acc: 0.8008, Average Threshold: 0.2573\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:4.470079\tAcc:0.390625 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:4.331242\tAcc:0.382812 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:4.684299\tAcc:0.359375 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:4.439575\tAcc:0.304688 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:4.248855\tAcc:0.406250 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:4.396227\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:4.151225\tAcc:0.460938 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:4.246331\tAcc:0.382812 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:4.302963\tAcc:0.375000 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:4.062414\tAcc:0.421875 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:4.046739\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:4.216164\tAcc:0.359375 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:3.692370\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch Loss: 4.195911 Accuracy: 0.431029\n",
            "Eval Epoch Average Acc: 0.8267, Average Threshold: 0.2159\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:3.510868\tAcc:0.609375 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:3.648091\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:3.207556\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:3.274328\tAcc:0.570312 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:3.557271\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:3.427562\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:3.437897\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:3.339237\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:3.264746\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:3.002442\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:3.119895\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:3.201521\tAcc:0.554688 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:3.061007\tAcc:0.640625 LR:0.0010000\n",
            "Train Epoch Loss: 3.403814 Accuracy: 0.566122\n",
            "Eval Epoch Average Acc: 0.8375, Average Threshold: 0.2154\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:2.960639\tAcc:0.601562 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:2.903662\tAcc:0.687500 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:2.413906\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:2.829045\tAcc:0.664062 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:2.263035\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:2.469449\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:2.367880\tAcc:0.742188 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:2.467216\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:2.811019\tAcc:0.617188 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:2.625060\tAcc:0.703125 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:2.608057\tAcc:0.664062 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:2.305605\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:2.567586\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch Loss: 2.496313 Accuracy: 0.719579\n",
            "Eval Epoch Average Acc: 0.8455, Average Threshold: 0.1922\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:2.315440\tAcc:0.710938 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:2.292388\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:2.143558\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:2.227414\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:2.031048\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:2.201214\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:2.370802\tAcc:0.710938 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:2.051371\tAcc:0.765625 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:2.114217\tAcc:0.742188 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:2.294775\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:2.176541\tAcc:0.750000 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:2.186651\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:2.399250\tAcc:0.742188 LR:0.0001000\n",
            "Train Epoch Loss: 2.242110 Accuracy: 0.760687\n",
            "Eval Epoch Average Acc: 0.8432, Average Threshold: 0.1862\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:2.104319\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:1.995608\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:2.016592\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:2.454326\tAcc:0.671875 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:2.086780\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:1.923692\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:2.343153\tAcc:0.726562 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:2.197855\tAcc:0.726562 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:2.271423\tAcc:0.710938 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:1.837409\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:2.009426\tAcc:0.781250 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:2.039058\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:1.850789\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch Loss: 2.048838 Accuracy: 0.790725\n",
            "Eval Epoch Average Acc: 0.8450, Average Threshold: 0.1822\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:1.800455\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:1.746252\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:2.051246\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:1.725574\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:1.867264\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:1.955243\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:1.593989\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:1.691572\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:1.984687\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:1.801311\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:1.957169\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:1.831385\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:2.008065\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch Loss: 1.868148 Accuracy: 0.818162\n",
            "Eval Epoch Average Acc: 0.8445, Average Threshold: 0.1729\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:1.557133\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:1.611605\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:1.599190\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:1.976726\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:1.477236\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:1.738781\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:1.716410\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:1.666589\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:1.837255\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:1.692870\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:1.578140\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:1.747609\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:2.122620\tAcc:0.781250 LR:0.0001000\n",
            "Train Epoch Loss: 1.693394 Accuracy: 0.845583\n",
            "Eval Epoch Average Acc: 0.8442, Average Threshold: 0.1631\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:1.617834\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:1.524941\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:1.452690\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:1.350523\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:1.841225\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:1.558225\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:1.818230\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:1.470297\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:1.624776\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:1.462025\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:1.453081\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:1.639609\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:1.495131\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch Loss: 1.525571 Accuracy: 0.873779\n",
            "Eval Epoch Average Acc: 0.8473, Average Threshold: 0.1664\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:1.099572\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:1.396181\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:1.150747\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:1.270119\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:1.094956\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:1.612160\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:1.326136\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:1.470068\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:1.671697\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:1.436179\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:1.537573\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:1.310817\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:1.348604\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch Loss: 1.334445 Accuracy: 0.902779\n",
            "Eval Epoch Average Acc: 0.8458, Average Threshold: 0.1551\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:1.178909\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:1.131826\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:1.401837\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:1.386961\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:1.511716\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:1.257365\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:1.408799\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:1.098931\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:1.039940\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:1.179699\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:1.201703\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:1.519715\tAcc:0.851562 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:1.182544\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch Loss: 1.307361 Accuracy: 0.908864\n",
            "Eval Epoch Average Acc: 0.8445, Average Threshold: 0.1534\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:1.375910\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:1.404433\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:1.551616\tAcc:0.867188 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:1.390339\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:1.520687\tAcc:0.859375 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:1.255601\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:1.334474\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:1.235427\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:1.418687\tAcc:0.843750 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:1.269103\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:1.266573\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:1.444769\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:1.248102\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch Loss: 1.283403 Accuracy: 0.912147\n",
            "Eval Epoch Average Acc: 0.8473, Average Threshold: 0.1552\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:1.452025\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:1.219866\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:1.353163\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:1.116859\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:1.225340\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:1.489687\tAcc:0.882812 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.968705\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:1.161229\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:1.372945\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:1.224634\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:1.285632\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:1.109649\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:1.168772\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch Loss: 1.264993 Accuracy: 0.914702\n",
            "Eval Epoch Average Acc: 0.8458, Average Threshold: 0.1485\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:1.338288\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:1.385070\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:1.386079\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:1.428427\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:1.304910\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:1.104194\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:1.143281\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:1.181567\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:1.142570\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:1.339308\tAcc:0.890625 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:1.177245\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:1.189097\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:1.263799\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch Loss: 1.249076 Accuracy: 0.916111\n",
            "Eval Epoch Average Acc: 0.8472, Average Threshold: 0.1471\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:1.135301\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:1.274498\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:1.073367\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.971705\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:1.258576\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:1.187635\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:1.145961\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:1.359049\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:1.064410\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:1.114155\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:1.273903\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:1.230328\tAcc:0.898438 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:1.147847\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 1.231148 Accuracy: 0.919439\n",
            "Eval Epoch Average Acc: 0.8458, Average Threshold: 0.1495\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:1.191252\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:1.154638\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:1.256568\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:1.177259\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:1.103813\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:1.068776\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:1.096228\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:1.119222\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:1.341113\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.885476\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:1.173254\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:1.244681\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:1.109620\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch Loss: 1.209556 Accuracy: 0.921096\n",
            "Eval Epoch Average Acc: 0.8485, Average Threshold: 0.1523\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:1.260610\tAcc:0.906250 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:1.267908\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:1.149804\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:1.224041\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:1.153106\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:1.128271\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:1.140857\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:1.223600\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:1.338560\tAcc:0.875000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:1.344358\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:1.054929\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.994279\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.923109\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch Loss: 1.195535 Accuracy: 0.924472\n",
            "Eval Epoch Average Acc: 0.8458, Average Threshold: 0.1512\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:1.163533\tAcc:0.898438 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:1.282894\tAcc:0.882812 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:1.052514\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:1.206841\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:1.433018\tAcc:0.875000 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:1.251446\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.927723\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:1.166301\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:1.194350\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:1.131669\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:1.309918\tAcc:0.875000 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:1.218677\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:1.132889\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch Loss: 1.172686 Accuracy: 0.928281\n",
            "Eval Epoch Average Acc: 0.8475, Average Threshold: 0.1472\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:1.311137\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:1.465919\tAcc:0.890625 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:1.202131\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:1.356643\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:1.208385\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:1.063877\tAcc:0.937500 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:1.204246\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:1.140138\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:1.085907\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:1.177564\tAcc:0.906250 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:1.246196\tAcc:0.921875 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:1.154875\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:1.200535\tAcc:0.914062 LR:0.0000010\n",
            "Train Epoch Loss: 1.172904 Accuracy: 0.927212\n",
            "Eval Epoch Average Acc: 0.8472, Average Threshold: 0.1520\n",
            "Best acc on LFW: 0.8485000000000001, best threshold: 0.1523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPZvtMbWjeWN",
        "colab_type": "text"
      },
      "source": [
        "**OHEM(normface基础上)+ResNet34-IR**\n",
        "\n",
        "Train Epoch Loss: 1.172904\n",
        "\n",
        "Accuracy: 0.927212\n",
        "\n",
        "Eval Epoch Average Acc: 0.8472\n",
        "\n",
        "Average Threshold: 0.1520\n",
        "\n",
        "Best acc on LFW: **0.8485000000000001**\n",
        "\n",
        "best threshold: 0.1523"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTDutNNIfp_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a62fb89-b25b-496e-b749-2a2091b18328"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "FocalLoss(normface基础上)\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:3.439764\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:3.057897\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:2.954415\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:2.691144\tAcc:0.109375 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:2.746006\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:2.463144\tAcc:0.171875 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:2.415074\tAcc:0.156250 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:2.304783\tAcc:0.218750 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:2.405860\tAcc:0.179688 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:2.075864\tAcc:0.289062 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:2.106447\tAcc:0.226562 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:1.897774\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:1.706487\tAcc:0.359375 LR:0.0010000\n",
            "Train Epoch Loss: 2.472113 Accuracy: 0.164311\n",
            "Eval Epoch Average Acc: 0.8015, Average Threshold: 0.2128\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:1.757189\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:1.543630\tAcc:0.437500 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:1.743593\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:1.696849\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:1.900187\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:1.804700\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:1.568992\tAcc:0.398438 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:1.494499\tAcc:0.445312 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:1.304134\tAcc:0.484375 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:1.309041\tAcc:0.468750 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:1.561873\tAcc:0.390625 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:1.437074\tAcc:0.398438 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:1.464065\tAcc:0.484375 LR:0.0010000\n",
            "Train Epoch Loss: 1.584498 Accuracy: 0.406859\n",
            "Eval Epoch Average Acc: 0.8325, Average Threshold: 0.1969\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:1.143581\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:1.091002\tAcc:0.609375 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.086131\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:1.068104\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.234097\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:1.258566\tAcc:0.500000 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:1.144990\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:1.173577\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:1.194677\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:1.157024\tAcc:0.578125 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:0.999810\tAcc:0.609375 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:1.067460\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:0.902138\tAcc:0.664062 LR:0.0010000\n",
            "Train Epoch Loss: 1.124832 Accuracy: 0.559263\n",
            "Eval Epoch Average Acc: 0.8453, Average Threshold: 0.1886\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:0.862802\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:0.829661\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:1.056263\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:0.917693\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:0.796619\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:0.743404\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:0.821783\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:0.975141\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:0.751106\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:0.902070\tAcc:0.632812 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:0.830931\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:0.887349\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:0.918967\tAcc:0.617188 LR:0.0010000\n",
            "Train Epoch Loss: 0.821428 Accuracy: 0.675730\n",
            "Eval Epoch Average Acc: 0.8470, Average Threshold: 0.1778\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.654334\tAcc:0.734375 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:0.583355\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.500067\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:0.553539\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.389233\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:0.487132\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:0.542714\tAcc:0.789062 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:0.505997\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.574089\tAcc:0.773438 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:0.421590\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.459896\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:0.447352\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.520897\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch Loss: 0.498459 Accuracy: 0.811891\n",
            "Eval Epoch Average Acc: 0.8497, Average Threshold: 0.1396\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.411830\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:0.407349\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.337674\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:0.372619\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.325024\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:0.444821\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.438593\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:0.470834\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.432434\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:0.459469\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.483303\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:0.519472\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.428777\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch Loss: 0.422469 Accuracy: 0.849005\n",
            "Eval Epoch Average Acc: 0.8505, Average Threshold: 0.1394\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.262829\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.381164\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.328306\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:0.453284\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.310748\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.328844\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.392867\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:0.359268\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.358238\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:0.531691\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.283240\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.361044\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.331583\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch Loss: 0.364697 Accuracy: 0.875823\n",
            "Eval Epoch Average Acc: 0.8495, Average Threshold: 0.1372\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.315928\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.323737\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.299516\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.254702\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.212110\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.273703\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.288859\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.262602\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.408179\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.298037\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.363775\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.331665\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.308103\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch Loss: 0.314346 Accuracy: 0.900008\n",
            "Eval Epoch Average Acc: 0.8487, Average Threshold: 0.1337\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.240614\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.175104\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.322336\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.232285\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.215257\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.335688\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.292705\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.276785\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.306531\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.289905\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.295260\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.257414\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.337434\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch Loss: 0.270235 Accuracy: 0.920585\n",
            "Eval Epoch Average Acc: 0.8455, Average Threshold: 0.1236\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.188980\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.239643\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.194897\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.213883\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.213740\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.328214\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.181799\tAcc:0.968750 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.219264\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.189257\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.247274\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.237274\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.208736\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.226936\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch Loss: 0.226192 Accuracy: 0.938980\n",
            "Eval Epoch Average Acc: 0.8467, Average Threshold: 0.1404\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.170312\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.173653\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.180112\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.170993\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.161462\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.227795\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.157739\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.205366\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.185746\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.164087\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.190427\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.177632\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.204183\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.173608 Accuracy: 0.959418\n",
            "Eval Epoch Average Acc: 0.8442, Average Threshold: 0.1394\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.163740\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.142307\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.172434\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.154914\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.128384\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.258986\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.166448\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.138533\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.178742\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.140451\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.232977\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.195301\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.308101\tAcc:0.914062 LR:0.0000100\n",
            "Train Epoch Loss: 0.168322 Accuracy: 0.961988\n",
            "Eval Epoch Average Acc: 0.8422, Average Threshold: 0.1428\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.173811\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.172410\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.171138\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.146641\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.193464\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.133619\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.164398\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.142672\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.104506\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:0.189471\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.183146\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.231864\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.174446\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch Loss: 0.161877 Accuracy: 0.964512\n",
            "Eval Epoch Average Acc: 0.8435, Average Threshold: 0.1456\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.138122\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.145154\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.212856\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:0.187140\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.120085\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.110440\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.133614\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.147125\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.126009\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.153948\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.194912\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:0.170233\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.140286\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.156688 Accuracy: 0.967469\n",
            "Eval Epoch Average Acc: 0.8415, Average Threshold: 0.1337\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.111350\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:0.183702\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.198421\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:0.180581\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.136061\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:0.131436\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.139453\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:0.139471\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.145741\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:0.225785\tAcc:0.921875 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.120460\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:0.132432\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.131238\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch Loss: 0.151138 Accuracy: 0.968754\n",
            "Eval Epoch Average Acc: 0.8407, Average Threshold: 0.1384\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.118074\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:0.172497\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.151255\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.135425\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.154117\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:0.145320\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:0.126637\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:0.206480\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.170227\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:0.172954\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.141676\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:0.127158\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.187504\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch Loss: 0.147867 Accuracy: 0.970380\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1436\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.128160\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:0.078823\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.151444\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:0.135714\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.176642\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:0.127181\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:0.113413\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:0.116420\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.132830\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.161146\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.190845\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:0.096196\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.202078\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.143344 Accuracy: 0.971294\n",
            "Eval Epoch Average Acc: 0.8408, Average Threshold: 0.1464\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.167898\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:0.164390\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.136918\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:0.158554\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.091709\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:0.180246\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:0.127930\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:0.098872\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.149800\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:0.106001\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.125906\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.164426\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.153221\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.140012 Accuracy: 0.973043\n",
            "Eval Epoch Average Acc: 0.8392, Average Threshold: 0.1413\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.133872\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:0.107045\tAcc:1.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.140138\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:0.176900\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.173886\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:0.142844\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.110190\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:0.192651\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.115926\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:0.121961\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.133595\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:0.108392\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.102429\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch Loss: 0.134123 Accuracy: 0.975056\n",
            "Eval Epoch Average Acc: 0.8408, Average Threshold: 0.1431\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.109891\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:0.118916\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.164716\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:0.169452\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.097886\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:0.137526\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:0.143069\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:0.135939\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.156392\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:0.118824\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.183181\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:0.148782\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.143637\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch Loss: 0.134180 Accuracy: 0.975134\n",
            "Eval Epoch Average Acc: 0.8413, Average Threshold: 0.1447\n",
            "Best acc on LFW: 0.8504999999999999, best threshold: 0.139395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nss4pcwl4Kxi",
        "colab_type": "text"
      },
      "source": [
        "**FocalLoss(normface基础上)+ResNet34-IR**\n",
        "\n",
        "Train Epoch Loss: 0.134180\n",
        "\n",
        "Accuracy: 0.975134\n",
        "\n",
        "Eval Epoch Average Acc: 0.8413\n",
        "\n",
        "Average Threshold: 0.1447\n",
        "\n",
        "Best acc on LFW: **0.8504999999999999**\n",
        "\n",
        "best threshold: 0.139395"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogRYSS1-4KLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6f9315b-adfc-4f15-b6b4-c57301713361"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "CosFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:18.271578\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:17.043428\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:16.167536\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:15.916327\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:15.500957\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:15.240651\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:15.020101\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:14.537114\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:14.092602\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:14.263521\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:13.501030\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:13.416240\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:13.190861\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 15.047360 Accuracy: 0.001564\n",
            "Eval Epoch Average Acc: 0.8067, Average Threshold: 0.2012\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:13.238172\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:12.252065\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:11.645258\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:12.218580\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:11.936696\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:11.304377\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:11.598321\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:11.102586\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:11.930393\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:11.057278\tAcc:0.070312 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:10.913269\tAcc:0.039062 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:10.897180\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:10.749532\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch Loss: 11.581541 Accuracy: 0.028397\n",
            "Eval Epoch Average Acc: 0.8243, Average Threshold: 0.1902\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:9.929600\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:10.470213\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:9.868497\tAcc:0.085938 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:10.085711\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:9.898612\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:9.713832\tAcc:0.085938 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:10.258011\tAcc:0.101562 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:8.834167\tAcc:0.125000 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:8.471071\tAcc:0.078125 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:9.813988\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:9.222429\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:9.092599\tAcc:0.109375 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:8.675280\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch Loss: 9.477571 Accuracy: 0.083828\n",
            "Eval Epoch Average Acc: 0.8435, Average Threshold: 0.1694\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:9.106104\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:8.842471\tAcc:0.101562 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:7.606178\tAcc:0.148438 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:8.068146\tAcc:0.125000 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:7.693328\tAcc:0.164062 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:7.861096\tAcc:0.156250 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:8.395299\tAcc:0.140625 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:8.645379\tAcc:0.140625 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:8.065695\tAcc:0.171875 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:8.388397\tAcc:0.164062 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:9.138765\tAcc:0.148438 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:7.514204\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:7.709826\tAcc:0.156250 LR:0.0010000\n",
            "Train Epoch Loss: 8.088161 Accuracy: 0.152667\n",
            "Eval Epoch Average Acc: 0.8452, Average Threshold: 0.1917\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:7.083922\tAcc:0.218750 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:6.781193\tAcc:0.242188 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:5.815353\tAcc:0.250000 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:6.593377\tAcc:0.257812 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:6.166054\tAcc:0.296875 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:7.366401\tAcc:0.242188 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:6.446144\tAcc:0.250000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:6.157387\tAcc:0.250000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:5.006386\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:5.930398\tAcc:0.234375 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:6.242858\tAcc:0.257812 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:6.304611\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:5.643781\tAcc:0.257812 LR:0.0001000\n",
            "Train Epoch Loss: 6.190781 Accuracy: 0.274042\n",
            "Eval Epoch Average Acc: 0.8505, Average Threshold: 0.1818\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:5.068519\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:5.461656\tAcc:0.328125 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:5.705826\tAcc:0.328125 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:5.765669\tAcc:0.328125 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:5.214467\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:5.245245\tAcc:0.328125 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:5.760334\tAcc:0.312500 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:5.670261\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:4.820848\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:5.634490\tAcc:0.304688 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:5.387932\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:5.962692\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:5.142972\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch Loss: 5.623226 Accuracy: 0.319347\n",
            "Eval Epoch Average Acc: 0.8477, Average Threshold: 0.1667\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:5.567431\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:5.070159\tAcc:0.375000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:5.813207\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:4.514668\tAcc:0.414062 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:4.975732\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:5.413113\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:4.732139\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:5.143925\tAcc:0.335938 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:5.057676\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:5.495253\tAcc:0.343750 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:5.103181\tAcc:0.375000 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:5.254399\tAcc:0.359375 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:5.232544\tAcc:0.328125 LR:0.0001000\n",
            "Train Epoch Loss: 5.206657 Accuracy: 0.352094\n",
            "Eval Epoch Average Acc: 0.8470, Average Threshold: 0.1760\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:4.494314\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:5.476550\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:5.304607\tAcc:0.335938 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:4.785423\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:5.581650\tAcc:0.320312 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:4.380722\tAcc:0.437500 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:4.236700\tAcc:0.382812 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:4.396254\tAcc:0.406250 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:4.685163\tAcc:0.406250 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:4.443720\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:4.427358\tAcc:0.476562 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:5.616218\tAcc:0.312500 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:4.208154\tAcc:0.492188 LR:0.0001000\n",
            "Train Epoch Loss: 4.814342 Accuracy: 0.382782\n",
            "Eval Epoch Average Acc: 0.8492, Average Threshold: 0.1704\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:4.055530\tAcc:0.468750 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:4.604377\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:4.884300\tAcc:0.398438 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:3.942458\tAcc:0.460938 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:4.780541\tAcc:0.421875 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:4.370684\tAcc:0.421875 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:4.647677\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:4.859197\tAcc:0.367188 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:4.201957\tAcc:0.476562 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:5.173867\tAcc:0.406250 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:4.363104\tAcc:0.437500 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:4.544245\tAcc:0.437500 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:4.049595\tAcc:0.507812 LR:0.0001000\n",
            "Train Epoch Loss: 4.446951 Accuracy: 0.414523\n",
            "Eval Epoch Average Acc: 0.8470, Average Threshold: 0.1762\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:4.321029\tAcc:0.390625 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:4.663819\tAcc:0.429688 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:4.576714\tAcc:0.437500 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:4.097308\tAcc:0.500000 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:4.890700\tAcc:0.351562 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:3.651235\tAcc:0.445312 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:3.802144\tAcc:0.468750 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:4.881421\tAcc:0.398438 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:3.813450\tAcc:0.468750 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:4.116342\tAcc:0.398438 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:4.697461\tAcc:0.406250 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:4.410817\tAcc:0.398438 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:3.636683\tAcc:0.531250 LR:0.0001000\n",
            "Train Epoch Loss: 4.077335 Accuracy: 0.445429\n",
            "Eval Epoch Average Acc: 0.8473, Average Threshold: 0.1669\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:3.720883\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:3.642494\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:3.787974\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:3.575111\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:3.942778\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:3.285920\tAcc:0.539062 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:3.844330\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:3.529571\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:3.764429\tAcc:0.445312 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:3.765085\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:3.216070\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:3.523105\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:3.701354\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch Loss: 3.633945 Accuracy: 0.485778\n",
            "Eval Epoch Average Acc: 0.8455, Average Threshold: 0.1575\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:2.946229\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:3.279356\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:3.014679\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:4.036433\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:3.526399\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:3.399316\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:3.626141\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:3.785881\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:3.613074\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:2.905276\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:3.979519\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:3.208002\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:3.940120\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch Loss: 3.558474 Accuracy: 0.493210\n",
            "Eval Epoch Average Acc: 0.8437, Average Threshold: 0.1577\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:4.116860\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:3.114214\tAcc:0.562500 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:3.333630\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:3.308777\tAcc:0.492188 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:3.174870\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:3.561288\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:3.532130\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:3.482056\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:3.691337\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:3.574578\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:3.696291\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:4.092568\tAcc:0.453125 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:3.350302\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch Loss: 3.506205 Accuracy: 0.499388\n",
            "Eval Epoch Average Acc: 0.8462, Average Threshold: 0.1691\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:3.221080\tAcc:0.546875 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:3.212177\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:3.238233\tAcc:0.546875 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:3.066243\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:3.467535\tAcc:0.539062 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:3.526554\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:3.963842\tAcc:0.453125 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:3.910235\tAcc:0.429688 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:4.044589\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:3.611061\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:4.178598\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:3.560470\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:3.098424\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch Loss: 3.459492 Accuracy: 0.501076\n",
            "Eval Epoch Average Acc: 0.8448, Average Threshold: 0.1700\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:3.078978\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:2.767483\tAcc:0.570312 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:4.110322\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:3.200691\tAcc:0.539062 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:3.456198\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:3.442607\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:3.252411\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:3.471857\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:3.868335\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:2.639011\tAcc:0.609375 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:3.504145\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:2.620986\tAcc:0.562500 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:3.861503\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch Loss: 3.419129 Accuracy: 0.505365\n",
            "Eval Epoch Average Acc: 0.8442, Average Threshold: 0.1608\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:3.432445\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:4.205987\tAcc:0.414062 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:3.406910\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:2.927130\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:3.660128\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:3.639775\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:3.194884\tAcc:0.476562 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:3.242756\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:2.894175\tAcc:0.578125 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:3.005484\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:3.017068\tAcc:0.585938 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:3.652280\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:3.278217\tAcc:0.507812 LR:0.0000100\n",
            "Train Epoch Loss: 3.376518 Accuracy: 0.509561\n",
            "Eval Epoch Average Acc: 0.8463, Average Threshold: 0.1724\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:3.688036\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:3.608871\tAcc:0.515625 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:2.751977\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:3.045738\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:3.250290\tAcc:0.570312 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:2.861032\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:3.369627\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:3.157134\tAcc:0.554688 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:3.516541\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:2.991658\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:3.834913\tAcc:0.460938 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:3.339849\tAcc:0.546875 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:3.413984\tAcc:0.546875 LR:0.0000100\n",
            "Train Epoch Loss: 3.345881 Accuracy: 0.511450\n",
            "Eval Epoch Average Acc: 0.8462, Average Threshold: 0.1596\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:3.463518\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:2.582125\tAcc:0.617188 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:2.909202\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:3.798947\tAcc:0.437500 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:4.275521\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:3.125248\tAcc:0.539062 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:3.027775\tAcc:0.523438 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:3.274907\tAcc:0.468750 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:3.354816\tAcc:0.500000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:2.795464\tAcc:0.562500 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:3.352817\tAcc:0.484375 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:3.481498\tAcc:0.406250 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:3.413551\tAcc:0.531250 LR:0.0000100\n",
            "Train Epoch Loss: 3.302878 Accuracy: 0.516792\n",
            "Eval Epoch Average Acc: 0.8447, Average Threshold: 0.1586\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:3.397064\tAcc:0.484375 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:3.515317\tAcc:0.492188 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:3.194587\tAcc:0.539062 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:3.545904\tAcc:0.570312 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:3.061646\tAcc:0.546875 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:3.338483\tAcc:0.492188 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:3.271765\tAcc:0.484375 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:4.110966\tAcc:0.453125 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:2.788411\tAcc:0.585938 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:3.854964\tAcc:0.437500 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:3.597593\tAcc:0.468750 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:3.374495\tAcc:0.437500 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:3.007336\tAcc:0.578125 LR:0.0000010\n",
            "Train Epoch Loss: 3.252504 Accuracy: 0.520941\n",
            "Eval Epoch Average Acc: 0.8467, Average Threshold: 0.1609\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:3.210270\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:2.885571\tAcc:0.578125 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:3.366435\tAcc:0.515625 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:3.119947\tAcc:0.507812 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:3.118682\tAcc:0.546875 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:3.725082\tAcc:0.445312 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:3.595289\tAcc:0.453125 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:3.193551\tAcc:0.531250 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:3.650314\tAcc:0.476562 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:2.767681\tAcc:0.554688 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:3.609647\tAcc:0.507812 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:2.825589\tAcc:0.570312 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:3.309279\tAcc:0.492188 LR:0.0000010\n",
            "Train Epoch Loss: 3.251487 Accuracy: 0.521359\n",
            "Eval Epoch Average Acc: 0.8472, Average Threshold: 0.1631\n",
            "Best acc on LFW: 0.8504999999999999, best threshold: 0.18183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iDQvGjOOOEt",
        "colab_type": "text"
      },
      "source": [
        "**CosFace+ResNet34-IR**\n",
        "\n",
        "Train Epoch Loss: 3.251487 \n",
        "\n",
        "Accuracy: 0.521359\n",
        "\n",
        "Eval Epoch Average Acc: 0.8472\n",
        "\n",
        "Average Threshold: 0.1631\n",
        "\n",
        "Best acc on LFW: **0.8504999999999999**\n",
        "\n",
        "best threshold: 0.18183"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAkQKW8exZog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08e9d662-c946-402f-e9d6-dcdfc49388c1"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "NormFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:6.861026\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:6.423325\tAcc:0.031250 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:5.813034\tAcc:0.070312 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:5.733257\tAcc:0.062500 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:5.452716\tAcc:0.125000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:5.193485\tAcc:0.187500 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:5.044561\tAcc:0.132812 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:4.751154\tAcc:0.226562 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:4.569767\tAcc:0.203125 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:4.563012\tAcc:0.218750 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:4.551109\tAcc:0.234375 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:4.317073\tAcc:0.257812 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:4.631595\tAcc:0.242188 LR:0.0010000\n",
            "Train Epoch Loss: 5.179455 Accuracy: 0.152357\n",
            "Eval Epoch Average Acc: 0.8022, Average Threshold: 0.2001\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:4.131823\tAcc:0.320312 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:4.102386\tAcc:0.257812 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:3.912388\tAcc:0.328125 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:3.944078\tAcc:0.250000 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:3.826577\tAcc:0.367188 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:3.878214\tAcc:0.304688 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:3.520561\tAcc:0.367188 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:3.586242\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:3.434644\tAcc:0.390625 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:3.377884\tAcc:0.429688 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:3.327487\tAcc:0.421875 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:3.116771\tAcc:0.468750 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:3.186867\tAcc:0.398438 LR:0.0010000\n",
            "Train Epoch Loss: 3.581628 Accuracy: 0.382395\n",
            "Eval Epoch Average Acc: 0.8287, Average Threshold: 0.1839\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:2.885157\tAcc:0.468750 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:2.739470\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:2.657527\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:2.567304\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:2.671469\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:2.798433\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:2.615467\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:2.502057\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:2.698123\tAcc:0.546875 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:2.165719\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:2.422701\tAcc:0.593750 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:2.677935\tAcc:0.531250 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:2.298190\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch Loss: 2.629210 Accuracy: 0.545792\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1915\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:1.936117\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:2.123501\tAcc:0.625000 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:2.018613\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:2.091268\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.977046\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:1.819842\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:2.037287\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:1.926486\tAcc:0.679688 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:1.973938\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:2.010200\tAcc:0.656250 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.984221\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:1.903016\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.917494\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch Loss: 1.984459 Accuracy: 0.664814\n",
            "Eval Epoch Average Acc: 0.8398, Average Threshold: 0.1520\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:1.768701\tAcc:0.718750 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:1.515311\tAcc:0.757812 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:1.250353\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:1.189292\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:1.210475\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:1.361848\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:1.208832\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:0.996846\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:1.393571\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:1.250062\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:1.338853\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:1.173978\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:1.053588\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch Loss: 1.280588 Accuracy: 0.809662\n",
            "Eval Epoch Average Acc: 0.8462, Average Threshold: 0.1580\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:1.227555\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:1.035887\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.946895\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:1.147626\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:1.151040\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:1.117251\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.999687\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:1.289816\tAcc:0.828125 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:1.017338\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:1.063370\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:1.120809\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:1.275188\tAcc:0.804688 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.976907\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch Loss: 1.102608 Accuracy: 0.849485\n",
            "Eval Epoch Average Acc: 0.8463, Average Threshold: 0.1603\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.892251\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.985163\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:1.018979\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:0.926486\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:1.128883\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.906430\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.983885\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:1.122683\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.954901\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:1.083311\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.893518\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.889911\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.850274\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch Loss: 0.976132 Accuracy: 0.877804\n",
            "Eval Epoch Average Acc: 0.8470, Average Threshold: 0.1287\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.895013\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.809482\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.901547\tAcc:0.843750 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.811056\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.992088\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.947967\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.806054\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.869035\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.857796\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.890771\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.856626\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.875448\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.894579\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch Loss: 0.867196 Accuracy: 0.900488\n",
            "Eval Epoch Average Acc: 0.8412, Average Threshold: 0.1431\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:1.058658\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.903930\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.805823\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.664331\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.779720\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.779210\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.739498\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.865632\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.767643\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.728838\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.791787\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.679631\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.782848\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch Loss: 0.763993 Accuracy: 0.922149\n",
            "Eval Epoch Average Acc: 0.8437, Average Threshold: 0.1537\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.583839\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.717920\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.680370\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.577811\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.585966\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.564298\tAcc:0.976562 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.603353\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.572263\tAcc:0.976562 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.585494\tAcc:0.968750 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.644598\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.714196\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.794932\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.600597\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch Loss: 0.665267 Accuracy: 0.944136\n",
            "Eval Epoch Average Acc: 0.8433, Average Threshold: 0.1551\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.582950\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.552267\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.600671\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.553923\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.508146\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.604483\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.537938\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.527923\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.596226\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.524860\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.539647\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.517940\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.669019\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch Loss: 0.550641 Accuracy: 0.963366\n",
            "Eval Epoch Average Acc: 0.8408, Average Threshold: 0.1529\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.496119\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.513772\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.610438\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.600486\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.509970\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.557574\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.493938\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.471238\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.759684\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.542011\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.515582\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.476611\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.497867\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.535737 Accuracy: 0.966912\n",
            "Eval Epoch Average Acc: 0.8412, Average Threshold: 0.1370\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.528234\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.506698\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.495707\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.429705\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.448526\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.395280\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.505886\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.470593\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.631525\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:0.544292\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.482310\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.433967\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.584633\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.521083 Accuracy: 0.969080\n",
            "Eval Epoch Average Acc: 0.8418, Average Threshold: 0.1545\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.540581\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.460561\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.523309\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:0.505561\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.465046\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.530507\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.427689\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.439817\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.617280\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.533604\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.488610\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:0.505145\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.551292\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.509682 Accuracy: 0.971092\n",
            "Eval Epoch Average Acc: 0.8398, Average Threshold: 0.1387\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.412831\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:0.487677\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.465454\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:0.473692\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.573883\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:0.534357\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.657359\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:0.485869\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.421208\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:0.535422\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.511360\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:0.423837\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.490185\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch Loss: 0.505987 Accuracy: 0.971123\n",
            "Eval Epoch Average Acc: 0.8413, Average Threshold: 0.1519\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.546974\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:0.512067\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.483707\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.454836\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.514904\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:0.504602\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:0.451728\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:0.427348\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.473580\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:0.431843\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.470558\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:0.489893\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.433124\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch Loss: 0.492272 Accuracy: 0.973585\n",
            "Eval Epoch Average Acc: 0.8380, Average Threshold: 0.1317\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.482758\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:0.486296\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.440193\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:0.375332\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.576423\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:0.445676\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:0.408897\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:0.537719\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.396710\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.471094\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.458026\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:0.463055\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.441367\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch Loss: 0.480076 Accuracy: 0.975583\n",
            "Eval Epoch Average Acc: 0.8372, Average Threshold: 0.1434\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.609383\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:0.451755\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.458658\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:0.472697\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.495360\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:0.464090\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:0.451270\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:0.321334\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.397921\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:0.485315\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.501795\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.537840\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.582364\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch Loss: 0.472756 Accuracy: 0.976155\n",
            "Eval Epoch Average Acc: 0.8380, Average Threshold: 0.1530\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.464487\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:0.485714\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.442719\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:0.464613\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.403754\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:0.486314\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.428698\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:0.411983\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.473160\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:0.383008\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.359238\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:0.570172\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.551287\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch Loss: 0.462021 Accuracy: 0.978323\n",
            "Eval Epoch Average Acc: 0.8392, Average Threshold: 0.1409\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.373506\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:0.484873\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.455372\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:0.465171\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.505779\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:0.592725\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:0.403386\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:0.429084\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.371166\tAcc:1.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:0.488786\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.529143\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:0.477634\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.443181\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch Loss: 0.462395 Accuracy: 0.977719\n",
            "Eval Epoch Average Acc: 0.8370, Average Threshold: 0.1419\n",
            "Best acc on LFW: 0.8470000000000001, best threshold: 0.128695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ziB1boWxuep",
        "colab_type": "text"
      },
      "source": [
        "**NormFace+ResNet34-IR**\n",
        "\n",
        "Train Epoch Loss: 0.462395\n",
        "\n",
        "Accuracy: 0.977719\n",
        "\n",
        "Eval Epoch Average Acc: 0.8370\n",
        "\n",
        "Average Threshold: 0.1419\n",
        "\n",
        "Best acc on LFW: **0.8470000000000001**\n",
        "\n",
        "best threshold: 0.128695"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-C7N2AhGXUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38e401fb-232e-4659-a5fa-d3e5388289aa"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "Softmax\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:6.777514\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:5.938496\tAcc:0.054688 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:5.647607\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:4.984230\tAcc:0.109375 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:4.967398\tAcc:0.093750 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:4.490786\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:4.141701\tAcc:0.210938 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:3.950588\tAcc:0.250000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:3.666833\tAcc:0.289062 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:3.754659\tAcc:0.242188 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:3.393238\tAcc:0.367188 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:3.281801\tAcc:0.351562 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:3.729729\tAcc:0.312500 LR:0.0010000\n",
            "Train Epoch Loss: 4.401125 Accuracy: 0.212511\n",
            "Eval Epoch Average Acc: 0.8073, Average Threshold: 0.2050\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:3.002239\tAcc:0.375000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:2.475428\tAcc:0.523438 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:2.428213\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:2.774749\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:2.402091\tAcc:0.523438 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:2.744825\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:2.192066\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:2.360144\tAcc:0.539062 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:2.614379\tAcc:0.492188 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:2.356125\tAcc:0.523438 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:2.385340\tAcc:0.515625 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:2.446411\tAcc:0.523438 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:2.614960\tAcc:0.476562 LR:0.0010000\n",
            "Train Epoch Loss: 2.521461 Accuracy: 0.496447\n",
            "Eval Epoch Average Acc: 0.8327, Average Threshold: 0.1904\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:1.872779\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:1.470432\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:1.426713\tAcc:0.710938 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:1.977390\tAcc:0.601562 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:1.612576\tAcc:0.664062 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:1.636430\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:1.971413\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:1.873646\tAcc:0.703125 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:1.829096\tAcc:0.578125 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:2.108695\tAcc:0.585938 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:1.907653\tAcc:0.570312 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:1.770975\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:2.120017\tAcc:0.562500 LR:0.0010000\n",
            "Train Epoch Loss: 1.817911 Accuracy: 0.623860\n",
            "Eval Epoch Average Acc: 0.8355, Average Threshold: 0.2026\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:1.330664\tAcc:0.718750 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:1.575796\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:1.478126\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:1.555113\tAcc:0.648438 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:1.272579\tAcc:0.710938 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:1.564500\tAcc:0.671875 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:1.107198\tAcc:0.757812 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:1.484561\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:1.503556\tAcc:0.664062 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:1.592015\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:1.349780\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:1.333359\tAcc:0.687500 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:1.490810\tAcc:0.726562 LR:0.0010000\n",
            "Train Epoch Loss: 1.390236 Accuracy: 0.703693\n",
            "Eval Epoch Average Acc: 0.8433, Average Threshold: 0.1869\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.940228\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:1.075750\tAcc:0.820312 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.924381\tAcc:0.796875 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:0.716734\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.815075\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:0.695275\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:0.627306\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:0.812305\tAcc:0.812500 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.725614\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:0.651279\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.692956\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:0.744779\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.577462\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch Loss: 0.761655 Accuracy: 0.839235\n",
            "Eval Epoch Average Acc: 0.8532, Average Threshold: 0.1807\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.733509\tAcc:0.835938 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:0.555780\tAcc:0.867188 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.543892\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:0.547028\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.542708\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:0.681918\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.588999\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:0.277973\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.605561\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:0.346281\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.345812\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:0.545564\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.620201\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch Loss: 0.607266 Accuracy: 0.875126\n",
            "Eval Epoch Average Acc: 0.8578, Average Threshold: 0.1698\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.491316\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.396775\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.496552\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:0.561996\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.390555\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.460084\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.496852\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:0.555227\tAcc:0.851562 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.696130\tAcc:0.859375 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:0.449794\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.463549\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.339750\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.517419\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch Loss: 0.514995 Accuracy: 0.895502\n",
            "Eval Epoch Average Acc: 0.8575, Average Threshold: 0.1595\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.567319\tAcc:0.875000 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.261751\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.613678\tAcc:0.882812 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.294185\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.349910\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.381649\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.335083\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.389795\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.343512\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.426037\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.554012\tAcc:0.898438 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.621286\tAcc:0.890625 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.430664\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch Loss: 0.428439 Accuracy: 0.913865\n",
            "Eval Epoch Average Acc: 0.8543, Average Threshold: 0.1608\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.347422\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.421105\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.402826\tAcc:0.914062 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.419277\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.307878\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.254444\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.355064\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.463738\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.357768\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.398823\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.278613\tAcc:0.937500 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.287174\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.308533\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch Loss: 0.356415 Accuracy: 0.930773\n",
            "Eval Epoch Average Acc: 0.8563, Average Threshold: 0.1600\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.265708\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.364914\tAcc:0.929688 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.231283\tAcc:0.968750 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.249258\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.319125\tAcc:0.945312 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.197353\tAcc:0.953125 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.408576\tAcc:0.906250 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.312854\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.159573\tAcc:0.984375 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.243585\tAcc:0.968750 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.207810\tAcc:0.960938 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.393364\tAcc:0.921875 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.136647\tAcc:0.976562 LR:0.0001000\n",
            "Train Epoch Loss: 0.288496 Accuracy: 0.946056\n",
            "Eval Epoch Average Acc: 0.8550, Average Threshold: 0.1587\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.187477\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.219745\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.178588\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.177216\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.211250\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.249775\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.196351\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.139560\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.296749\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.266006\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.228254\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.282298\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.223322\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch Loss: 0.218116 Accuracy: 0.962499\n",
            "Eval Epoch Average Acc: 0.8528, Average Threshold: 0.1628\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.267223\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.289663\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.220890\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.161235\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.245905\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.190489\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.349090\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.269170\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.135381\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.203382\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.104098\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.272583\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.129384\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch Loss: 0.207286 Accuracy: 0.965255\n",
            "Eval Epoch Average Acc: 0.8542, Average Threshold: 0.1600\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.286142\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.201246\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.251822\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.260243\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.225753\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.149286\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.286295\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.086737\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.153355\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:0.225508\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.117403\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.203635\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.148169\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch Loss: 0.196362 Accuracy: 0.967918\n",
            "Eval Epoch Average Acc: 0.8493, Average Threshold: 0.1487\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.198813\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.129796\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.154956\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:0.117464\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.265664\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.148575\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.249110\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.232152\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.150646\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.115310\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.185529\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:0.140224\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.157977\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch Loss: 0.189878 Accuracy: 0.968987\n",
            "Eval Epoch Average Acc: 0.8530, Average Threshold: 0.1562\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.108928\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:0.211007\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.111976\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:0.161890\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.207549\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:0.249196\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.154283\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:0.272108\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.110626\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:0.172865\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.256208\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:0.202162\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.108677\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch Loss: 0.183127 Accuracy: 0.970055\n",
            "Eval Epoch Average Acc: 0.8538, Average Threshold: 0.1513\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.212310\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:0.172669\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.150931\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.174410\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.115984\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:0.184706\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:0.152658\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:0.190371\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.298895\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:0.165414\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.173894\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:0.144884\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.069573\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.174174 Accuracy: 0.971603\n",
            "Eval Epoch Average Acc: 0.8532, Average Threshold: 0.1594\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.129493\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:0.138886\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.137164\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:0.181897\tAcc:0.984375 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.207977\tAcc:0.945312 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:0.192672\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:0.176943\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:0.151297\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.200469\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.195145\tAcc:0.953125 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.236491\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:0.299376\tAcc:0.929688 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.219610\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch Loss: 0.167880 Accuracy: 0.973508\n",
            "Eval Epoch Average Acc: 0.8510, Average Threshold: 0.1533\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.148282\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:0.154251\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.088488\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:0.133666\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.131114\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:0.198086\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:0.239584\tAcc:0.960938 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:0.217743\tAcc:0.937500 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.195173\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:0.142654\tAcc:0.968750 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.229542\tAcc:0.976562 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.089515\tAcc:0.992188 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.121352\tAcc:1.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.159539 Accuracy: 0.975103\n",
            "Eval Epoch Average Acc: 0.8510, Average Threshold: 0.1540\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.126843\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:0.159509\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.243950\tAcc:0.929688 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:0.170808\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.294999\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:0.183377\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.180874\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:0.142482\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.160003\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:0.154933\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.170386\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:0.112681\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.090025\tAcc:0.992188 LR:0.0000010\n",
            "Train Epoch Loss: 0.152472 Accuracy: 0.976961\n",
            "Eval Epoch Average Acc: 0.8533, Average Threshold: 0.1541\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.129387\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:0.172666\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.112361\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:0.201084\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.178324\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:0.197118\tAcc:0.953125 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:0.248779\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:0.119975\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.192032\tAcc:0.960938 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:0.111797\tAcc:0.976562 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.126482\tAcc:0.984375 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:0.209054\tAcc:0.968750 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.228719\tAcc:0.945312 LR:0.0000010\n",
            "Train Epoch Loss: 0.154187 Accuracy: 0.976403\n",
            "Eval Epoch Average Acc: 0.8518, Average Threshold: 0.1545\n",
            "Best acc on LFW: 0.8578333333333333, best threshold: 0.16982499999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlcb9Y3GZaY",
        "colab_type": "text"
      },
      "source": [
        "**Softmax+ResNet34-IR**\n",
        "\n",
        "Train Epoch Loss: 0.154187 \n",
        "\n",
        "Accuracy: 0.976403\n",
        "\n",
        "Eval Epoch Average Acc: 0.8518\n",
        "\n",
        "Average Threshold: 0.1545\n",
        "\n",
        "Best acc on LFW: **0.8578333333333333**\n",
        "\n",
        "best threshold: 0.16982499999999998"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ4tGZymxbwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "495acf5d-96a4-402d-8b4c-9dbb0e7e2db4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average_meter.py\tdatasets\t\t     __MACOSX\n",
            "CASIA_anno.txt\t\tgenerate_CASIA_WebFace.py    __pycache__\n",
            "casia-maxpy-clean\tgenerate_csv_files.py\t     ResNet.py\n",
            "casia-maxpy-clean.zip\tLFW\t\t\t     SEResNet_IR.py\n",
            "CASIAWebFace.csv\tLFW_CASIAWebFace_Dataset.py  trainer.py\n",
            "checkpoints\t\tLFW.tar.gz\t\t     train.py\n",
            "Contrastive_trainer.py\tloss_function.py\t     Triplet_trainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiQ8m5FfwJIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d6f9887-d3b4-4ce8-d108-644ab66e803f"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
            "            PReLU-36          [-1, 128, 56, 56]             128\n",
            "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-39            [-1, 128, 1, 1]               0\n",
            "           Linear-40                    [-1, 8]           1,032\n",
            "             ReLU-41                    [-1, 8]               0\n",
            "           Linear-42                  [-1, 128]           1,152\n",
            "          Sigmoid-43                  [-1, 128]               0\n",
            "         SEModule-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "           Conv2d-50          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
            "            PReLU-52          [-1, 128, 28, 28]             128\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-55            [-1, 128, 1, 1]               0\n",
            "           Linear-56                    [-1, 8]           1,032\n",
            "             ReLU-57                    [-1, 8]               0\n",
            "           Linear-58                  [-1, 128]           1,152\n",
            "          Sigmoid-59                  [-1, 128]               0\n",
            "         SEModule-60          [-1, 128, 28, 28]               0\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "            PReLU-66          [-1, 256, 28, 28]             256\n",
            "           Conv2d-67          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-69            [-1, 256, 1, 1]               0\n",
            "           Linear-70                   [-1, 16]           4,112\n",
            "             ReLU-71                   [-1, 16]               0\n",
            "           Linear-72                  [-1, 256]           4,352\n",
            "          Sigmoid-73                  [-1, 256]               0\n",
            "         SEModule-74          [-1, 256, 14, 14]               0\n",
            "           Conv2d-75          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
            "            PReLU-77          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
            "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
            "            PReLU-82          [-1, 256, 14, 14]             256\n",
            "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 16]           4,112\n",
            "             ReLU-87                   [-1, 16]               0\n",
            "           Linear-88                  [-1, 256]           4,352\n",
            "          Sigmoid-89                  [-1, 256]               0\n",
            "         SEModule-90          [-1, 256, 14, 14]               0\n",
            "            PReLU-91          [-1, 256, 14, 14]             256\n",
            "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
            "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
            "           Conv2d-94          [-1, 512, 14, 14]       1,179,648\n",
            "      BatchNorm2d-95          [-1, 512, 14, 14]           1,024\n",
            "            PReLU-96          [-1, 512, 14, 14]             512\n",
            "           Conv2d-97            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-98            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-99            [-1, 512, 1, 1]               0\n",
            "          Linear-100                   [-1, 32]          16,416\n",
            "            ReLU-101                   [-1, 32]               0\n",
            "          Linear-102                  [-1, 512]          16,896\n",
            "         Sigmoid-103                  [-1, 512]               0\n",
            "        SEModule-104            [-1, 512, 7, 7]               0\n",
            "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-107            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-110            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-111            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-112            [-1, 512, 7, 7]             512\n",
            "          Conv2d-113            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "            ReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "        SEModule-120            [-1, 512, 7, 7]               0\n",
            "           PReLU-121            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-123            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-124            [-1, 512, 7, 7]               0\n",
            "         Flatten-125                [-1, 25088]               0\n",
            "          Linear-126                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 24,112,312\n",
            "Trainable params: 24,112,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 88.29\n",
            "Params size (MB): 91.98\n",
            "Estimated Total Size (MB): 180.42\n",
            "----------------------------------------------------------------\n",
            "CosFace\n",
            "Contrastive（Finetune）\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:66.098160\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:42.516418\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:56.568455\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:28.664070\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:23.365868\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:20.733856\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:17.350660\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:15.336240\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:11.034109\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:11.348654\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:7.730385\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:7.524926\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:5.033806\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 20.418177 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5547, Average Threshold: -0.9806\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:5.975082\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:3.738591\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:4.567654\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:1.821437\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:2.880652\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:2.017102\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:2.785783\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:2.371714\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:1.404935\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:0.746064\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:0.995567\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:0.710553\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:0.723783\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 2.115342 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5738, Average Threshold: 0.8001\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:0.697830\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:0.482664\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:0.587119\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:0.422689\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:0.532770\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:0.400879\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:0.341898\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:0.418703\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:0.246131\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:0.316709\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:0.171683\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:0.158171\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:0.268145\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 0.387962 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5850, Average Threshold: -0.6383\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:0.252402\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:0.125640\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:0.135752\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:0.112880\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:0.114374\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:0.111186\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:0.109731\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:0.105122\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:0.101054\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:0.097411\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:0.100860\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:0.074728\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:0.083669\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 0.114851 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5908, Average Threshold: 0.9957\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:0.078522\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:0.075152\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:0.076588\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:0.080932\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:0.046979\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:0.075572\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:0.068068\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:0.065017\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:0.070416\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:0.067177\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:0.071398\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:0.084692\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:0.061333\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.075305 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5933, Average Threshold: 0.9936\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:0.046912\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:0.081040\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:0.065539\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:0.069747\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:0.059980\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:0.082625\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:0.117157\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:0.060564\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:0.066869\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:0.062016\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:0.075147\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:0.091358\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:0.080142\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.071593 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5950, Average Threshold: 0.9934\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:0.056653\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:0.064944\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:0.063325\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:0.102469\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:0.075893\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:0.068480\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:0.070021\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:0.068718\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:0.058258\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:0.049255\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:0.063310\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:0.056232\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:0.068021\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.068130 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5962, Average Threshold: 0.9911\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:0.074237\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:0.069870\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:0.074474\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:0.067624\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:0.074648\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:0.054346\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:0.051694\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:0.073033\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:0.066813\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:0.063697\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:0.077117\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:0.068734\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:0.077195\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.065698 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5938, Average Threshold: 0.9672\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:0.067380\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:0.077023\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:0.065710\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:0.048808\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:0.074561\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:0.052036\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:0.060548\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:0.050003\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:0.061756\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:0.057678\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:0.060831\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:0.063703\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:0.063740\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.063077 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.5957, Average Threshold: 0.8995\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:0.052879\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:0.057388\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:0.065907\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:0.067234\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:0.059787\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:0.072966\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:0.059165\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:0.057438\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:0.057592\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:0.062043\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:0.060980\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:0.069111\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:0.059300\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 0.061188 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6157, Average Threshold: 0.8233\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:0.059011\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:0.062278\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:0.053529\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:0.064588\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:0.066814\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:0.057427\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:0.057670\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:0.053435\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:0.067536\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:0.043115\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:0.065608\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:0.048171\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:0.065604\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.059536 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6138, Average Threshold: 0.8695\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:0.068136\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:0.058222\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:0.074884\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:0.058559\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:0.055707\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:0.050816\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:0.057204\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:0.068576\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:0.042378\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:0.049571\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:0.057562\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:0.067309\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:0.063915\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.059018 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6160, Average Threshold: 0.8325\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:0.065634\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:0.056397\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:0.062675\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:0.059077\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:0.049245\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:0.051674\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:0.059617\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:0.059133\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:0.059967\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:0.057979\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:0.052052\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:0.065029\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:0.047287\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.059452 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6192, Average Threshold: 0.8116\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:0.062370\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:0.070482\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:0.053199\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:0.060092\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:0.064741\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:0.050040\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:0.056842\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:0.061834\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:0.077788\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:0.054170\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:0.073256\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:0.060765\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:0.062932\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.058850 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6233, Average Threshold: 0.7944\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:0.056156\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:0.069074\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:0.052709\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:0.054540\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:0.051049\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:0.055113\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:0.064702\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:0.063477\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:0.049977\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:0.056140\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:0.063916\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:0.062821\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:0.061158\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.058313 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6275, Average Threshold: 0.7919\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:0.060577\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:0.057250\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:0.060502\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:0.063971\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:0.053902\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:0.050425\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:0.051738\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:0.050848\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:0.057007\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:0.061006\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:0.059697\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:0.057889\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:0.058910\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.058114 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6285, Average Threshold: 0.7826\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:0.056587\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:0.052067\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:0.050396\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:0.061354\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:0.059763\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:0.055394\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:0.051777\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:0.059161\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:0.059870\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:0.055003\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:0.051609\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:0.057662\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:0.057299\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.057474 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6282, Average Threshold: 0.7134\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:0.062972\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:0.057401\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:0.052871\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:0.063899\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:0.064508\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:0.067346\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:0.066523\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:0.054392\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:0.059789\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:0.051505\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:0.056882\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:0.055196\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:0.063835\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 0.057389 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6355, Average Threshold: 0.7134\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:0.053097\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:0.054091\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:0.050939\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:0.051475\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:0.045728\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:0.063155\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:0.057670\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:0.057633\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:0.058915\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:0.058794\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:0.058989\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:0.053104\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:0.058447\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 0.056839 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6308, Average Threshold: 0.6942\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:0.062038\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:0.064599\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:0.060679\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:0.055990\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:0.061987\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:0.053153\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:0.056676\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:0.054913\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:0.060512\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:0.049671\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:0.062381\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:0.063000\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:0.058399\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 0.056769 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.6372, Average Threshold: 0.7041\n",
            "Best acc on LFW: 0.6371666666666667, best threshold: 0.7041449999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz57mHQ5IoB_",
        "colab_type": "text"
      },
      "source": [
        "**Contrastive（Finetune）CosFace+ResNet18-IR**\n",
        "\n",
        "Train Epoch Loss: 0.056769 \n",
        "\n",
        "Accuracy: 0.000000\n",
        "Eval Epoch Average Acc: 0.6372\n",
        "\n",
        "Average Threshold: 0.7041\n",
        "\n",
        "Best acc on LFW: **0.6371666666666667**\n",
        "\n",
        "best threshold: 0.7041449999999999"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8YhzHMgVHPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0635961-ee0a-48ae-e6ca-69074c1c47fe"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "CASIA dataset size: 64585 / 775\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
            "             PReLU-8           [-1, 64, 56, 56]              64\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-11             [-1, 64, 1, 1]               0\n",
            "           Linear-12                    [-1, 4]             260\n",
            "             ReLU-13                    [-1, 4]               0\n",
            "           Linear-14                   [-1, 64]             320\n",
            "          Sigmoid-15                   [-1, 64]               0\n",
            "         SEModule-16           [-1, 64, 56, 56]               0\n",
            "            PReLU-17           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "            PReLU-22           [-1, 64, 56, 56]              64\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
            "           Linear-26                    [-1, 4]             260\n",
            "             ReLU-27                    [-1, 4]               0\n",
            "           Linear-28                   [-1, 64]             320\n",
            "          Sigmoid-29                   [-1, 64]               0\n",
            "         SEModule-30           [-1, 64, 56, 56]               0\n",
            "            PReLU-31           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-32           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
            "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
            "            PReLU-36           [-1, 64, 56, 56]              64\n",
            "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
            "AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0\n",
            "           Linear-40                    [-1, 4]             260\n",
            "             ReLU-41                    [-1, 4]               0\n",
            "           Linear-42                   [-1, 64]             320\n",
            "          Sigmoid-43                   [-1, 64]               0\n",
            "         SEModule-44           [-1, 64, 56, 56]               0\n",
            "            PReLU-45           [-1, 64, 56, 56]              64\n",
            "       BasicBlock-46           [-1, 64, 56, 56]               0\n",
            "      BatchNorm2d-47           [-1, 64, 56, 56]             128\n",
            "           Conv2d-48          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-49          [-1, 128, 56, 56]             256\n",
            "            PReLU-50          [-1, 128, 56, 56]             128\n",
            "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                    [-1, 8]           1,032\n",
            "             ReLU-55                    [-1, 8]               0\n",
            "           Linear-56                  [-1, 128]           1,152\n",
            "          Sigmoid-57                  [-1, 128]               0\n",
            "         SEModule-58          [-1, 128, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "            PReLU-61          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-62          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "           Conv2d-64          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
            "            PReLU-66          [-1, 128, 28, 28]             128\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0\n",
            "           Linear-70                    [-1, 8]           1,032\n",
            "             ReLU-71                    [-1, 8]               0\n",
            "           Linear-72                  [-1, 128]           1,152\n",
            "          Sigmoid-73                  [-1, 128]               0\n",
            "         SEModule-74          [-1, 128, 28, 28]               0\n",
            "            PReLU-75          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-76          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
            "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "            PReLU-80          [-1, 128, 28, 28]             128\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0\n",
            "           Linear-84                    [-1, 8]           1,032\n",
            "             ReLU-85                    [-1, 8]               0\n",
            "           Linear-86                  [-1, 128]           1,152\n",
            "          Sigmoid-87                  [-1, 128]               0\n",
            "         SEModule-88          [-1, 128, 28, 28]               0\n",
            "            PReLU-89          [-1, 128, 28, 28]             128\n",
            "       BasicBlock-90          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "            PReLU-94          [-1, 128, 28, 28]             128\n",
            "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
            "AdaptiveAvgPool2d-97            [-1, 128, 1, 1]               0\n",
            "           Linear-98                    [-1, 8]           1,032\n",
            "             ReLU-99                    [-1, 8]               0\n",
            "          Linear-100                  [-1, 128]           1,152\n",
            "         Sigmoid-101                  [-1, 128]               0\n",
            "        SEModule-102          [-1, 128, 28, 28]               0\n",
            "           PReLU-103          [-1, 128, 28, 28]             128\n",
            "      BasicBlock-104          [-1, 128, 28, 28]               0\n",
            "     BatchNorm2d-105          [-1, 128, 28, 28]             256\n",
            "          Conv2d-106          [-1, 256, 28, 28]         294,912\n",
            "     BatchNorm2d-107          [-1, 256, 28, 28]             512\n",
            "           PReLU-108          [-1, 256, 28, 28]             256\n",
            "          Conv2d-109          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-111            [-1, 256, 1, 1]               0\n",
            "          Linear-112                   [-1, 16]           4,112\n",
            "            ReLU-113                   [-1, 16]               0\n",
            "          Linear-114                  [-1, 256]           4,352\n",
            "         Sigmoid-115                  [-1, 256]               0\n",
            "        SEModule-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-118          [-1, 256, 14, 14]             512\n",
            "           PReLU-119          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-120          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-121          [-1, 256, 14, 14]             512\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "          Conv2d-125          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-127            [-1, 256, 1, 1]               0\n",
            "          Linear-128                   [-1, 16]           4,112\n",
            "            ReLU-129                   [-1, 16]               0\n",
            "          Linear-130                  [-1, 256]           4,352\n",
            "         Sigmoid-131                  [-1, 256]               0\n",
            "        SEModule-132          [-1, 256, 14, 14]               0\n",
            "           PReLU-133          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-134          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "          Conv2d-136          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-137          [-1, 256, 14, 14]             512\n",
            "           PReLU-138          [-1, 256, 14, 14]             256\n",
            "          Conv2d-139          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-141            [-1, 256, 1, 1]               0\n",
            "          Linear-142                   [-1, 16]           4,112\n",
            "            ReLU-143                   [-1, 16]               0\n",
            "          Linear-144                  [-1, 256]           4,352\n",
            "         Sigmoid-145                  [-1, 256]               0\n",
            "        SEModule-146          [-1, 256, 14, 14]               0\n",
            "           PReLU-147          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-148          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-149          [-1, 256, 14, 14]             512\n",
            "          Conv2d-150          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-151          [-1, 256, 14, 14]             512\n",
            "           PReLU-152          [-1, 256, 14, 14]             256\n",
            "          Conv2d-153          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-155            [-1, 256, 1, 1]               0\n",
            "          Linear-156                   [-1, 16]           4,112\n",
            "            ReLU-157                   [-1, 16]               0\n",
            "          Linear-158                  [-1, 256]           4,352\n",
            "         Sigmoid-159                  [-1, 256]               0\n",
            "        SEModule-160          [-1, 256, 14, 14]               0\n",
            "           PReLU-161          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-162          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-163          [-1, 256, 14, 14]             512\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "           PReLU-166          [-1, 256, 14, 14]             256\n",
            "          Conv2d-167          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-169            [-1, 256, 1, 1]               0\n",
            "          Linear-170                   [-1, 16]           4,112\n",
            "            ReLU-171                   [-1, 16]               0\n",
            "          Linear-172                  [-1, 256]           4,352\n",
            "         Sigmoid-173                  [-1, 256]               0\n",
            "        SEModule-174          [-1, 256, 14, 14]               0\n",
            "           PReLU-175          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-176          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-177          [-1, 256, 14, 14]             512\n",
            "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
            "           PReLU-180          [-1, 256, 14, 14]             256\n",
            "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "AdaptiveAvgPool2d-183            [-1, 256, 1, 1]               0\n",
            "          Linear-184                   [-1, 16]           4,112\n",
            "            ReLU-185                   [-1, 16]               0\n",
            "          Linear-186                  [-1, 256]           4,352\n",
            "         Sigmoid-187                  [-1, 256]               0\n",
            "        SEModule-188          [-1, 256, 14, 14]               0\n",
            "           PReLU-189          [-1, 256, 14, 14]             256\n",
            "      BasicBlock-190          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-191          [-1, 256, 14, 14]             512\n",
            "          Conv2d-192          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-194          [-1, 512, 14, 14]             512\n",
            "          Conv2d-195            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-196            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-197            [-1, 512, 1, 1]               0\n",
            "          Linear-198                   [-1, 32]          16,416\n",
            "            ReLU-199                   [-1, 32]               0\n",
            "          Linear-200                  [-1, 512]          16,896\n",
            "         Sigmoid-201                  [-1, 512]               0\n",
            "        SEModule-202            [-1, 512, 7, 7]               0\n",
            "          Conv2d-203            [-1, 512, 7, 7]         131,072\n",
            "     BatchNorm2d-204            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-205            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-206            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-207            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-208            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-209            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-210            [-1, 512, 7, 7]             512\n",
            "          Conv2d-211            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-212            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-213            [-1, 512, 1, 1]               0\n",
            "          Linear-214                   [-1, 32]          16,416\n",
            "            ReLU-215                   [-1, 32]               0\n",
            "          Linear-216                  [-1, 512]          16,896\n",
            "         Sigmoid-217                  [-1, 512]               0\n",
            "        SEModule-218            [-1, 512, 7, 7]               0\n",
            "           PReLU-219            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-220            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-221            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-222            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-223            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-224            [-1, 512, 7, 7]             512\n",
            "          Conv2d-225            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-226            [-1, 512, 7, 7]           1,024\n",
            "AdaptiveAvgPool2d-227            [-1, 512, 1, 1]               0\n",
            "          Linear-228                   [-1, 32]          16,416\n",
            "            ReLU-229                   [-1, 32]               0\n",
            "          Linear-230                  [-1, 512]          16,896\n",
            "         Sigmoid-231                  [-1, 512]               0\n",
            "        SEModule-232            [-1, 512, 7, 7]               0\n",
            "           PReLU-233            [-1, 512, 7, 7]             512\n",
            "      BasicBlock-234            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-235            [-1, 512, 7, 7]           1,024\n",
            "         Dropout-236            [-1, 512, 7, 7]               0\n",
            "         Flatten-237                [-1, 25088]               0\n",
            "          Linear-238                  [-1, 512]      12,845,568\n",
            "     BatchNorm1d-239                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 34,300,012\n",
            "Trainable params: 34,300,012\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 131.40\n",
            "Params size (MB): 130.84\n",
            "Estimated Total Size (MB): 262.39\n",
            "----------------------------------------------------------------\n",
            "ArcFace\n",
            "Train Epoch: 0 [00000000/00064585 (00%)]\tLoss:23.060617\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00005120/00064585 (08%)]\tLoss:21.965525\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00010240/00064585 (16%)]\tLoss:21.457512\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00015360/00064585 (24%)]\tLoss:21.057272\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00020480/00064585 (32%)]\tLoss:20.863169\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00025600/00064585 (40%)]\tLoss:20.269415\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00030720/00064585 (48%)]\tLoss:20.072203\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00035840/00064585 (55%)]\tLoss:19.863152\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00040960/00064585 (63%)]\tLoss:19.457855\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00046080/00064585 (71%)]\tLoss:19.158772\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00051200/00064585 (79%)]\tLoss:18.522419\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 0 [00056320/00064585 (87%)]\tLoss:18.505703\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 0 [00061440/00064585 (95%)]\tLoss:17.894611\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 19.998735 Accuracy: 0.000434\n",
            "Eval Epoch Average Acc: 0.7760, Average Threshold: 0.2297\n",
            "Train Epoch: 1 [00000000/00064585 (00%)]\tLoss:17.442787\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00005120/00064585 (08%)]\tLoss:17.863958\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00010240/00064585 (16%)]\tLoss:16.988262\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00015360/00064585 (24%)]\tLoss:16.216320\tAcc:0.023438 LR:0.0010000\n",
            "Train Epoch: 1 [00020480/00064585 (32%)]\tLoss:15.803205\tAcc:0.015625 LR:0.0010000\n",
            "Train Epoch: 1 [00025600/00064585 (40%)]\tLoss:16.312418\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00030720/00064585 (48%)]\tLoss:16.481771\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00035840/00064585 (55%)]\tLoss:15.026595\tAcc:0.007812 LR:0.0010000\n",
            "Train Epoch: 1 [00040960/00064585 (63%)]\tLoss:14.436570\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00046080/00064585 (71%)]\tLoss:14.377148\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00051200/00064585 (79%)]\tLoss:14.465479\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00056320/00064585 (87%)]\tLoss:13.727075\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 1 [00061440/00064585 (95%)]\tLoss:12.006949\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 15.440534 Accuracy: 0.004289\n",
            "Eval Epoch Average Acc: 0.6910, Average Threshold: 0.2717\n",
            "Train Epoch: 2 [00000000/00064585 (00%)]\tLoss:12.100058\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00005120/00064585 (08%)]\tLoss:19.012016\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00010240/00064585 (16%)]\tLoss:17.330770\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00015360/00064585 (24%)]\tLoss:19.181355\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00020480/00064585 (32%)]\tLoss:18.974688\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00025600/00064585 (40%)]\tLoss:18.693417\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00030720/00064585 (48%)]\tLoss:21.308945\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00035840/00064585 (55%)]\tLoss:20.760849\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00040960/00064585 (63%)]\tLoss:21.270943\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00046080/00064585 (71%)]\tLoss:20.337963\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00051200/00064585 (79%)]\tLoss:20.324158\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00056320/00064585 (87%)]\tLoss:19.718918\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 2 [00061440/00064585 (95%)]\tLoss:19.738773\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 19.254923 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7048, Average Threshold: 0.2193\n",
            "Train Epoch: 3 [00000000/00064585 (00%)]\tLoss:19.468580\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00005120/00064585 (08%)]\tLoss:19.585079\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00010240/00064585 (16%)]\tLoss:18.509607\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00015360/00064585 (24%)]\tLoss:19.594393\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00020480/00064585 (32%)]\tLoss:19.527464\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00025600/00064585 (40%)]\tLoss:19.725466\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00030720/00064585 (48%)]\tLoss:19.675043\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00035840/00064585 (55%)]\tLoss:21.062923\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00040960/00064585 (63%)]\tLoss:19.262634\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00046080/00064585 (71%)]\tLoss:18.275208\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00051200/00064585 (79%)]\tLoss:20.532400\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00056320/00064585 (87%)]\tLoss:20.492195\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch: 3 [00061440/00064585 (95%)]\tLoss:19.852468\tAcc:0.000000 LR:0.0010000\n",
            "Train Epoch Loss: 19.627439 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7507, Average Threshold: 0.3123\n",
            "Train Epoch: 4 [00000000/00064585 (00%)]\tLoss:20.340759\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00005120/00064585 (08%)]\tLoss:18.153488\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00010240/00064585 (16%)]\tLoss:18.568228\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00015360/00064585 (24%)]\tLoss:18.750816\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00020480/00064585 (32%)]\tLoss:20.032164\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00025600/00064585 (40%)]\tLoss:19.217836\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00030720/00064585 (48%)]\tLoss:17.284649\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00035840/00064585 (55%)]\tLoss:19.619669\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00040960/00064585 (63%)]\tLoss:18.338898\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00046080/00064585 (71%)]\tLoss:19.336033\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00051200/00064585 (79%)]\tLoss:18.359186\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00056320/00064585 (87%)]\tLoss:18.271711\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 4 [00061440/00064585 (95%)]\tLoss:19.034712\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.959991 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7503, Average Threshold: 0.3191\n",
            "Train Epoch: 5 [00000000/00064585 (00%)]\tLoss:18.661676\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00005120/00064585 (08%)]\tLoss:18.823677\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00010240/00064585 (16%)]\tLoss:20.721949\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00015360/00064585 (24%)]\tLoss:18.921354\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00020480/00064585 (32%)]\tLoss:18.724426\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00025600/00064585 (40%)]\tLoss:19.375154\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00030720/00064585 (48%)]\tLoss:18.058287\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00035840/00064585 (55%)]\tLoss:18.030714\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00040960/00064585 (63%)]\tLoss:17.519327\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00046080/00064585 (71%)]\tLoss:19.293884\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00051200/00064585 (79%)]\tLoss:15.941398\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00056320/00064585 (87%)]\tLoss:20.466204\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 5 [00061440/00064585 (95%)]\tLoss:18.792614\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.911106 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7395, Average Threshold: 0.3389\n",
            "Train Epoch: 6 [00000000/00064585 (00%)]\tLoss:18.343487\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00005120/00064585 (08%)]\tLoss:19.461660\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00010240/00064585 (16%)]\tLoss:16.592489\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00015360/00064585 (24%)]\tLoss:19.367344\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00020480/00064585 (32%)]\tLoss:19.506414\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00025600/00064585 (40%)]\tLoss:20.182146\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00030720/00064585 (48%)]\tLoss:17.318611\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00035840/00064585 (55%)]\tLoss:20.203672\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00040960/00064585 (63%)]\tLoss:20.242558\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00046080/00064585 (71%)]\tLoss:19.003441\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00051200/00064585 (79%)]\tLoss:18.401615\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00056320/00064585 (87%)]\tLoss:19.407486\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 6 [00061440/00064585 (95%)]\tLoss:17.594543\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.885996 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7017, Average Threshold: 0.2376\n",
            "Train Epoch: 7 [00000000/00064585 (00%)]\tLoss:18.049370\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00005120/00064585 (08%)]\tLoss:21.064640\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00010240/00064585 (16%)]\tLoss:18.294857\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00015360/00064585 (24%)]\tLoss:19.275373\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00020480/00064585 (32%)]\tLoss:18.883408\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00025600/00064585 (40%)]\tLoss:19.530479\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00030720/00064585 (48%)]\tLoss:18.896379\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00035840/00064585 (55%)]\tLoss:19.107075\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00040960/00064585 (63%)]\tLoss:18.809258\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00046080/00064585 (71%)]\tLoss:18.795683\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00051200/00064585 (79%)]\tLoss:19.361496\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00056320/00064585 (87%)]\tLoss:18.128767\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 7 [00061440/00064585 (95%)]\tLoss:18.863379\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.758564 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7198, Average Threshold: 0.2534\n",
            "Train Epoch: 8 [00000000/00064585 (00%)]\tLoss:18.746252\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00005120/00064585 (08%)]\tLoss:18.800053\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00010240/00064585 (16%)]\tLoss:18.287630\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00015360/00064585 (24%)]\tLoss:19.944490\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00020480/00064585 (32%)]\tLoss:19.064072\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00025600/00064585 (40%)]\tLoss:18.974396\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00030720/00064585 (48%)]\tLoss:20.160381\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00035840/00064585 (55%)]\tLoss:17.646435\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00040960/00064585 (63%)]\tLoss:19.296312\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00046080/00064585 (71%)]\tLoss:17.739775\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00051200/00064585 (79%)]\tLoss:18.422329\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00056320/00064585 (87%)]\tLoss:19.664810\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 8 [00061440/00064585 (95%)]\tLoss:18.309990\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.673319 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7205, Average Threshold: 0.2194\n",
            "Train Epoch: 9 [00000000/00064585 (00%)]\tLoss:17.469120\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00005120/00064585 (08%)]\tLoss:19.494780\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00010240/00064585 (16%)]\tLoss:19.577904\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00015360/00064585 (24%)]\tLoss:18.353960\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00020480/00064585 (32%)]\tLoss:18.646315\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00025600/00064585 (40%)]\tLoss:18.195208\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00030720/00064585 (48%)]\tLoss:19.037868\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00035840/00064585 (55%)]\tLoss:17.000902\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00040960/00064585 (63%)]\tLoss:19.567133\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00046080/00064585 (71%)]\tLoss:17.366322\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00051200/00064585 (79%)]\tLoss:19.853739\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00056320/00064585 (87%)]\tLoss:19.189253\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch: 9 [00061440/00064585 (95%)]\tLoss:17.145380\tAcc:0.000000 LR:0.0001000\n",
            "Train Epoch Loss: 18.606373 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7485, Average Threshold: 0.2374\n",
            "Train Epoch: 10 [00000000/00064585 (00%)]\tLoss:20.125023\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00005120/00064585 (08%)]\tLoss:19.687820\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00010240/00064585 (16%)]\tLoss:19.349699\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00015360/00064585 (24%)]\tLoss:17.592600\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00020480/00064585 (32%)]\tLoss:18.244211\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00025600/00064585 (40%)]\tLoss:19.287815\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00030720/00064585 (48%)]\tLoss:16.871618\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00035840/00064585 (55%)]\tLoss:19.018347\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00040960/00064585 (63%)]\tLoss:18.811226\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00046080/00064585 (71%)]\tLoss:19.088852\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00051200/00064585 (79%)]\tLoss:18.054544\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00056320/00064585 (87%)]\tLoss:19.566216\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 10 [00061440/00064585 (95%)]\tLoss:17.227085\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.571877 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7215, Average Threshold: 0.2012\n",
            "Train Epoch: 11 [00000000/00064585 (00%)]\tLoss:18.346869\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00005120/00064585 (08%)]\tLoss:17.196501\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00010240/00064585 (16%)]\tLoss:20.751169\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00015360/00064585 (24%)]\tLoss:18.751852\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00020480/00064585 (32%)]\tLoss:19.233044\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00025600/00064585 (40%)]\tLoss:17.133585\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00030720/00064585 (48%)]\tLoss:17.646931\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00035840/00064585 (55%)]\tLoss:18.025974\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00040960/00064585 (63%)]\tLoss:16.311758\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00046080/00064585 (71%)]\tLoss:18.607050\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00051200/00064585 (79%)]\tLoss:18.023832\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00056320/00064585 (87%)]\tLoss:17.902639\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 11 [00061440/00064585 (95%)]\tLoss:19.486738\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.517279 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7462, Average Threshold: 0.2608\n",
            "Train Epoch: 12 [00000000/00064585 (00%)]\tLoss:16.604275\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00005120/00064585 (08%)]\tLoss:18.207863\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00010240/00064585 (16%)]\tLoss:19.588497\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00015360/00064585 (24%)]\tLoss:18.515734\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00020480/00064585 (32%)]\tLoss:18.567066\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00025600/00064585 (40%)]\tLoss:17.736490\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00030720/00064585 (48%)]\tLoss:19.301046\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00035840/00064585 (55%)]\tLoss:19.376987\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00040960/00064585 (63%)]\tLoss:19.038683\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00046080/00064585 (71%)]\tLoss:19.247490\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00051200/00064585 (79%)]\tLoss:18.139250\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00056320/00064585 (87%)]\tLoss:17.149551\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 12 [00061440/00064585 (95%)]\tLoss:17.669561\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.482454 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7147, Average Threshold: 0.2424\n",
            "Train Epoch: 13 [00000000/00064585 (00%)]\tLoss:19.535975\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00005120/00064585 (08%)]\tLoss:18.729094\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00010240/00064585 (16%)]\tLoss:18.229923\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00015360/00064585 (24%)]\tLoss:18.714449\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00020480/00064585 (32%)]\tLoss:17.442938\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00025600/00064585 (40%)]\tLoss:19.655926\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00030720/00064585 (48%)]\tLoss:19.005468\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00035840/00064585 (55%)]\tLoss:19.272455\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00040960/00064585 (63%)]\tLoss:18.646986\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00046080/00064585 (71%)]\tLoss:18.406473\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00051200/00064585 (79%)]\tLoss:19.438959\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00056320/00064585 (87%)]\tLoss:17.931137\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 13 [00061440/00064585 (95%)]\tLoss:19.167406\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.491812 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7092, Average Threshold: 0.2118\n",
            "Train Epoch: 14 [00000000/00064585 (00%)]\tLoss:19.217690\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00005120/00064585 (08%)]\tLoss:18.987158\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00010240/00064585 (16%)]\tLoss:16.747849\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00015360/00064585 (24%)]\tLoss:18.057144\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00020480/00064585 (32%)]\tLoss:18.017963\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00025600/00064585 (40%)]\tLoss:18.432417\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00030720/00064585 (48%)]\tLoss:17.553349\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00035840/00064585 (55%)]\tLoss:18.282816\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00040960/00064585 (63%)]\tLoss:20.541771\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00046080/00064585 (71%)]\tLoss:18.423441\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00051200/00064585 (79%)]\tLoss:18.651196\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00056320/00064585 (87%)]\tLoss:19.734762\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 14 [00061440/00064585 (95%)]\tLoss:16.606657\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.463737 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7182, Average Threshold: 0.2669\n",
            "Train Epoch: 15 [00000000/00064585 (00%)]\tLoss:18.326855\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00005120/00064585 (08%)]\tLoss:18.492235\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00010240/00064585 (16%)]\tLoss:19.580061\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00015360/00064585 (24%)]\tLoss:18.186300\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00020480/00064585 (32%)]\tLoss:17.481585\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00025600/00064585 (40%)]\tLoss:17.921410\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00030720/00064585 (48%)]\tLoss:18.181194\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00035840/00064585 (55%)]\tLoss:17.900543\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00040960/00064585 (63%)]\tLoss:17.969074\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00046080/00064585 (71%)]\tLoss:17.791832\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00051200/00064585 (79%)]\tLoss:17.606066\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00056320/00064585 (87%)]\tLoss:17.618204\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 15 [00061440/00064585 (95%)]\tLoss:17.306471\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.485857 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7172, Average Threshold: 0.2526\n",
            "Train Epoch: 16 [00000000/00064585 (00%)]\tLoss:18.103565\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00005120/00064585 (08%)]\tLoss:18.528774\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00010240/00064585 (16%)]\tLoss:17.981113\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00015360/00064585 (24%)]\tLoss:18.831285\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00020480/00064585 (32%)]\tLoss:17.941193\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00025600/00064585 (40%)]\tLoss:18.881060\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00030720/00064585 (48%)]\tLoss:19.204517\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00035840/00064585 (55%)]\tLoss:19.434452\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00040960/00064585 (63%)]\tLoss:16.738611\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00046080/00064585 (71%)]\tLoss:19.011751\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00051200/00064585 (79%)]\tLoss:19.726116\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00056320/00064585 (87%)]\tLoss:20.418797\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 16 [00061440/00064585 (95%)]\tLoss:19.449175\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.450859 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7163, Average Threshold: 0.2779\n",
            "Train Epoch: 17 [00000000/00064585 (00%)]\tLoss:18.170465\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00005120/00064585 (08%)]\tLoss:19.074995\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00010240/00064585 (16%)]\tLoss:18.369177\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00015360/00064585 (24%)]\tLoss:18.944143\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00020480/00064585 (32%)]\tLoss:18.084219\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00025600/00064585 (40%)]\tLoss:17.633516\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00030720/00064585 (48%)]\tLoss:19.148106\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00035840/00064585 (55%)]\tLoss:18.120665\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00040960/00064585 (63%)]\tLoss:19.957771\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00046080/00064585 (71%)]\tLoss:18.556688\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00051200/00064585 (79%)]\tLoss:19.881575\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00056320/00064585 (87%)]\tLoss:18.943232\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch: 17 [00061440/00064585 (95%)]\tLoss:17.164869\tAcc:0.000000 LR:0.0000100\n",
            "Train Epoch Loss: 18.436008 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7182, Average Threshold: 0.2753\n",
            "Train Epoch: 18 [00000000/00064585 (00%)]\tLoss:18.629055\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00005120/00064585 (08%)]\tLoss:19.020119\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00010240/00064585 (16%)]\tLoss:19.673851\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00015360/00064585 (24%)]\tLoss:18.977894\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00020480/00064585 (32%)]\tLoss:19.244659\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00025600/00064585 (40%)]\tLoss:18.745466\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00030720/00064585 (48%)]\tLoss:18.665401\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00035840/00064585 (55%)]\tLoss:18.397186\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00040960/00064585 (63%)]\tLoss:19.351391\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00046080/00064585 (71%)]\tLoss:18.451437\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00051200/00064585 (79%)]\tLoss:19.097347\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00056320/00064585 (87%)]\tLoss:17.612930\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 18 [00061440/00064585 (95%)]\tLoss:19.311687\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 18.398008 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7197, Average Threshold: 0.2744\n",
            "Train Epoch: 19 [00000000/00064585 (00%)]\tLoss:19.384123\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00005120/00064585 (08%)]\tLoss:18.823286\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00010240/00064585 (16%)]\tLoss:18.528049\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00015360/00064585 (24%)]\tLoss:19.002960\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00020480/00064585 (32%)]\tLoss:18.539459\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00025600/00064585 (40%)]\tLoss:18.537668\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00030720/00064585 (48%)]\tLoss:18.456306\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00035840/00064585 (55%)]\tLoss:19.292740\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00040960/00064585 (63%)]\tLoss:18.937780\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00046080/00064585 (71%)]\tLoss:17.433620\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00051200/00064585 (79%)]\tLoss:16.361349\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00056320/00064585 (87%)]\tLoss:18.304111\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch: 19 [00061440/00064585 (95%)]\tLoss:19.714930\tAcc:0.000000 LR:0.0000010\n",
            "Train Epoch Loss: 18.410538 Accuracy: 0.000000\n",
            "Eval Epoch Average Acc: 0.7133, Average Threshold: 0.2453\n",
            "Best acc on LFW: 0.776, best threshold: 0.22967485714285713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPPlW55mQky",
        "colab_type": "text"
      },
      "source": [
        "**ArcFace+ResNet34-IR_Version2**\n",
        "\n",
        "Train Epoch Loss: 18.410538\n",
        "\n",
        "Accuracy: 0.000000\n",
        "\n",
        "Eval Epoch Average Acc: 0.7133\n",
        "\n",
        "Average Threshold: 0.2453\n",
        "\n",
        "Best acc on LFW: 0.776\n",
        "\n",
        "best threshold: 0.22967485714285713"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTXhhM-suc5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d500635-518e-4ba1-b2a0-f20c92ae8ca5"
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1242\tNumber of valid training triplets in epoch: 45309\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1242\tNumber of valid training triplets in epoch: 45325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1242\tNumber of valid training triplets in epoch: 45339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1242\tNumber of valid training triplets in epoch: 45357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1242\tNumber of valid training triplets in epoch: 45371\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45390\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45408\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45424\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45445\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1241\tNumber of valid training triplets in epoch: 45476\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45527\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45543\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1240\tNumber of valid training triplets in epoch: 45580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1239\tNumber of valid training triplets in epoch: 45600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1239\tNumber of valid training triplets in epoch: 45622\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1239\tNumber of valid training triplets in epoch: 45635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1239\tNumber of valid training triplets in epoch: 45649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1239\tNumber of valid training triplets in epoch: 45663\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1238\tNumber of valid training triplets in epoch: 45689\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1238\tNumber of valid training triplets in epoch: 45707\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1238\tNumber of valid training triplets in epoch: 45731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1238\tNumber of valid training triplets in epoch: 45747\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45785\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45829\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1237\tNumber of valid training triplets in epoch: 45846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1236\tNumber of valid training triplets in epoch: 45863\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1236\tNumber of valid training triplets in epoch: 45880\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1236\tNumber of valid training triplets in epoch: 45896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1236\tNumber of valid training triplets in epoch: 45914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1236\tNumber of valid training triplets in epoch: 45937\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 45954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 45974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 45998\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 46018\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 46029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1235\tNumber of valid training triplets in epoch: 46047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1234\tNumber of valid training triplets in epoch: 46064\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1234\tNumber of valid training triplets in epoch: 46090\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1234\tNumber of valid training triplets in epoch: 46113\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1233\tNumber of valid training triplets in epoch: 46129\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1233\tNumber of valid training triplets in epoch: 46151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1233\tNumber of valid training triplets in epoch: 46172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1233\tNumber of valid training triplets in epoch: 46185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46200\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46214\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46231\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46247\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46264\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46279\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1232\tNumber of valid training triplets in epoch: 46296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1231\tNumber of valid training triplets in epoch: 46318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1231\tNumber of valid training triplets in epoch: 46339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1231\tNumber of valid training triplets in epoch: 46354\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1231\tNumber of valid training triplets in epoch: 46368\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46386\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46422\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46438\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46472\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1230\tNumber of valid training triplets in epoch: 46501\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46517\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46549\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46575\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1229\tNumber of valid training triplets in epoch: 46596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1228\tNumber of valid training triplets in epoch: 46619\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1228\tNumber of valid training triplets in epoch: 46633\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1228\tNumber of valid training triplets in epoch: 46649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1228\tNumber of valid training triplets in epoch: 46664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1228\tNumber of valid training triplets in epoch: 46683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46726\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46760\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46777\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1227\tNumber of valid training triplets in epoch: 46798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46816\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46849\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46888\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1226\tNumber of valid training triplets in epoch: 46932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1225\tNumber of valid training triplets in epoch: 46953\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1225\tNumber of valid training triplets in epoch: 46970\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1225\tNumber of valid training triplets in epoch: 46983\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1225\tNumber of valid training triplets in epoch: 47004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1225\tNumber of valid training triplets in epoch: 47025\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47057\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1224\tNumber of valid training triplets in epoch: 47124\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1223\tNumber of valid training triplets in epoch: 47148\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1223\tNumber of valid training triplets in epoch: 47165\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1223\tNumber of valid training triplets in epoch: 47187\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47219\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47234\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47245\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47292\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1222\tNumber of valid training triplets in epoch: 47313\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47350\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47378\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47393\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1221\tNumber of valid training triplets in epoch: 47407\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1220\tNumber of valid training triplets in epoch: 47426\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1220\tNumber of valid training triplets in epoch: 47447\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1220\tNumber of valid training triplets in epoch: 47471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1220\tNumber of valid training triplets in epoch: 47489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1220\tNumber of valid training triplets in epoch: 47501\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1219\tNumber of valid training triplets in epoch: 47525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1219\tNumber of valid training triplets in epoch: 47543\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1219\tNumber of valid training triplets in epoch: 47559\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1219\tNumber of valid training triplets in epoch: 47579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1218\tNumber of valid training triplets in epoch: 47602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1218\tNumber of valid training triplets in epoch: 47625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1218\tNumber of valid training triplets in epoch: 47644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1218\tNumber of valid training triplets in epoch: 47660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1218\tNumber of valid training triplets in epoch: 47677\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47717\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47756\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47763\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1217\tNumber of valid training triplets in epoch: 47787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1216\tNumber of valid training triplets in epoch: 47809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1216\tNumber of valid training triplets in epoch: 47825\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1216\tNumber of valid training triplets in epoch: 47846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1216\tNumber of valid training triplets in epoch: 47865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47905\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47924\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47951\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1215\tNumber of valid training triplets in epoch: 47979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1214\tNumber of valid training triplets in epoch: 48001\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1214\tNumber of valid training triplets in epoch: 48019\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1214\tNumber of valid training triplets in epoch: 48038\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1214\tNumber of valid training triplets in epoch: 48052\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48073\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48093\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48130\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48148\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1213\tNumber of valid training triplets in epoch: 48166\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1212\tNumber of valid training triplets in epoch: 48182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1212\tNumber of valid training triplets in epoch: 48198\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1212\tNumber of valid training triplets in epoch: 48216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1212\tNumber of valid training triplets in epoch: 48239\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1212\tNumber of valid training triplets in epoch: 48255\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48310\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48321\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1211\tNumber of valid training triplets in epoch: 48357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1210\tNumber of valid training triplets in epoch: 48384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1210\tNumber of valid training triplets in epoch: 48397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1210\tNumber of valid training triplets in epoch: 48416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1210\tNumber of valid training triplets in epoch: 48433\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1210\tNumber of valid training triplets in epoch: 48448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48499\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48514\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1209\tNumber of valid training triplets in epoch: 48550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48567\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48582\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48591\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48633\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48659\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1208\tNumber of valid training triplets in epoch: 48675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1207\tNumber of valid training triplets in epoch: 48691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1207\tNumber of valid training triplets in epoch: 48716\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1207\tNumber of valid training triplets in epoch: 48734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1207\tNumber of valid training triplets in epoch: 48752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1207\tNumber of valid training triplets in epoch: 48770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1206\tNumber of valid training triplets in epoch: 48792\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1206\tNumber of valid training triplets in epoch: 48815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1206\tNumber of valid training triplets in epoch: 48832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1206\tNumber of valid training triplets in epoch: 48848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1205\tNumber of valid training triplets in epoch: 48873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1205\tNumber of valid training triplets in epoch: 48894\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1205\tNumber of valid training triplets in epoch: 48912\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1205\tNumber of valid training triplets in epoch: 48927\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 48949\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 48968\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 48984\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 49005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 49022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1204\tNumber of valid training triplets in epoch: 49036\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1203\tNumber of valid training triplets in epoch: 49047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1203\tNumber of valid training triplets in epoch: 49067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1203\tNumber of valid training triplets in epoch: 49088\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1203\tNumber of valid training triplets in epoch: 49106\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1203\tNumber of valid training triplets in epoch: 49126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49163\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1202\tNumber of valid training triplets in epoch: 49225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49262\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49282\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1201\tNumber of valid training triplets in epoch: 49330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1200\tNumber of valid training triplets in epoch: 49357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1200\tNumber of valid training triplets in epoch: 49373\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1200\tNumber of valid training triplets in epoch: 49395\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1200\tNumber of valid training triplets in epoch: 49411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49467\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49480\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49495\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1199\tNumber of valid training triplets in epoch: 49532\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49583\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49611\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1198\tNumber of valid training triplets in epoch: 49626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49650\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49688\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1197\tNumber of valid training triplets in epoch: 49735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49782\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49799\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1196\tNumber of valid training triplets in epoch: 49846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1195\tNumber of valid training triplets in epoch: 49865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1195\tNumber of valid training triplets in epoch: 49882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1195\tNumber of valid training triplets in epoch: 49899\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1195\tNumber of valid training triplets in epoch: 49922\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 49941\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 49957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 49965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 49985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 50000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 50015\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1194\tNumber of valid training triplets in epoch: 50033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1193\tNumber of valid training triplets in epoch: 50052\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1193\tNumber of valid training triplets in epoch: 50076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1193\tNumber of valid training triplets in epoch: 50097\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1193\tNumber of valid training triplets in epoch: 50119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1192\tNumber of valid training triplets in epoch: 50140\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1192\tNumber of valid training triplets in epoch: 50160\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1192\tNumber of valid training triplets in epoch: 50178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1192\tNumber of valid training triplets in epoch: 50194\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50235\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50252\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50270\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50303\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50315\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1191\tNumber of valid training triplets in epoch: 50331\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50348\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50378\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50421\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1190\tNumber of valid training triplets in epoch: 50433\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1189\tNumber of valid training triplets in epoch: 50451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1189\tNumber of valid training triplets in epoch: 50472\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1189\tNumber of valid training triplets in epoch: 50488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1189\tNumber of valid training triplets in epoch: 50511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1189\tNumber of valid training triplets in epoch: 50526\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1188\tNumber of valid training triplets in epoch: 50549\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1188\tNumber of valid training triplets in epoch: 50572\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1188\tNumber of valid training triplets in epoch: 50597\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1187\tNumber of valid training triplets in epoch: 50614\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1187\tNumber of valid training triplets in epoch: 50627\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1187\tNumber of valid training triplets in epoch: 50643\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1187\tNumber of valid training triplets in epoch: 50665\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1187\tNumber of valid training triplets in epoch: 50689\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50706\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50749\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50781\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1186\tNumber of valid training triplets in epoch: 50815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50838\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50863\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50900\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1185\tNumber of valid training triplets in epoch: 50921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 50943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 50958\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 50976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 50994\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 51004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1184\tNumber of valid training triplets in epoch: 51021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1183\tNumber of valid training triplets in epoch: 51043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1183\tNumber of valid training triplets in epoch: 51062\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1183\tNumber of valid training triplets in epoch: 51079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1183\tNumber of valid training triplets in epoch: 51096\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1183\tNumber of valid training triplets in epoch: 51119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51139\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51168\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1182\tNumber of valid training triplets in epoch: 51244\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51255\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51311\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51336\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1181\tNumber of valid training triplets in epoch: 51350\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51462\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1180\tNumber of valid training triplets in epoch: 51476\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51543\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51576\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1179\tNumber of valid training triplets in epoch: 51588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1178\tNumber of valid training triplets in epoch: 51604\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1178\tNumber of valid training triplets in epoch: 51624\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1178\tNumber of valid training triplets in epoch: 51644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1178\tNumber of valid training triplets in epoch: 51664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1178\tNumber of valid training triplets in epoch: 51681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51705\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51716\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51751\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51766\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1177\tNumber of valid training triplets in epoch: 51784\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1176\tNumber of valid training triplets in epoch: 51797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1176\tNumber of valid training triplets in epoch: 51817\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1176\tNumber of valid training triplets in epoch: 51835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1176\tNumber of valid training triplets in epoch: 51858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1176\tNumber of valid training triplets in epoch: 51877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51907\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51926\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51938\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1175\tNumber of valid training triplets in epoch: 51991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1174\tNumber of valid training triplets in epoch: 52005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1174\tNumber of valid training triplets in epoch: 52027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1174\tNumber of valid training triplets in epoch: 52049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1174\tNumber of valid training triplets in epoch: 52066\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1173\tNumber of valid training triplets in epoch: 52086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1173\tNumber of valid training triplets in epoch: 52101\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1173\tNumber of valid training triplets in epoch: 52116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1173\tNumber of valid training triplets in epoch: 52133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1173\tNumber of valid training triplets in epoch: 52159\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1172\tNumber of valid training triplets in epoch: 52178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1172\tNumber of valid training triplets in epoch: 52195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1172\tNumber of valid training triplets in epoch: 52220\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1172\tNumber of valid training triplets in epoch: 52236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1172\tNumber of valid training triplets in epoch: 52253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1171\tNumber of valid training triplets in epoch: 52269\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1171\tNumber of valid training triplets in epoch: 52285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1171\tNumber of valid training triplets in epoch: 52301\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1171\tNumber of valid training triplets in epoch: 52316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1171\tNumber of valid training triplets in epoch: 52338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52388\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52427\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1170\tNumber of valid training triplets in epoch: 52471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1169\tNumber of valid training triplets in epoch: 52486\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1169\tNumber of valid training triplets in epoch: 52508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1169\tNumber of valid training triplets in epoch: 52524\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1169\tNumber of valid training triplets in epoch: 52544\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1169\tNumber of valid training triplets in epoch: 52566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52586\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52627\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52643\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1168\tNumber of valid training triplets in epoch: 52674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1167\tNumber of valid training triplets in epoch: 52695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1167\tNumber of valid training triplets in epoch: 52710\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1167\tNumber of valid training triplets in epoch: 52734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1167\tNumber of valid training triplets in epoch: 52752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52784\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52801\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52836\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1166\tNumber of valid training triplets in epoch: 52853\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1165\tNumber of valid training triplets in epoch: 52869\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1165\tNumber of valid training triplets in epoch: 52886\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1165\tNumber of valid training triplets in epoch: 52909\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1165\tNumber of valid training triplets in epoch: 52926\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1165\tNumber of valid training triplets in epoch: 52942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 52957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 52979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53018\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53028\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53042\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1164\tNumber of valid training triplets in epoch: 53072\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53090\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53173\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1163\tNumber of valid training triplets in epoch: 53198\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53218\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53256\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53271\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1162\tNumber of valid training triplets in epoch: 53295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1161\tNumber of valid training triplets in epoch: 53312\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1161\tNumber of valid training triplets in epoch: 53335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1161\tNumber of valid training triplets in epoch: 53351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1161\tNumber of valid training triplets in epoch: 53370\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1161\tNumber of valid training triplets in epoch: 53390\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1160\tNumber of valid training triplets in epoch: 53410\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1160\tNumber of valid training triplets in epoch: 53430\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1160\tNumber of valid training triplets in epoch: 53454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1160\tNumber of valid training triplets in epoch: 53474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1160\tNumber of valid training triplets in epoch: 53493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53522\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53541\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1159\tNumber of valid training triplets in epoch: 53600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53651\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53684\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1158\tNumber of valid training triplets in epoch: 53714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1157\tNumber of valid training triplets in epoch: 53736\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1157\tNumber of valid training triplets in epoch: 53757\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1157\tNumber of valid training triplets in epoch: 53779\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1157\tNumber of valid training triplets in epoch: 53804\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53836\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53890\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1156\tNumber of valid training triplets in epoch: 53903\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1155\tNumber of valid training triplets in epoch: 53925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1155\tNumber of valid training triplets in epoch: 53943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1155\tNumber of valid training triplets in epoch: 53961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1155\tNumber of valid training triplets in epoch: 53977\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1155\tNumber of valid training triplets in epoch: 53994\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1154\tNumber of valid training triplets in epoch: 54021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1154\tNumber of valid training triplets in epoch: 54046\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1154\tNumber of valid training triplets in epoch: 54070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1154\tNumber of valid training triplets in epoch: 54087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1154\tNumber of valid training triplets in epoch: 54105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1153\tNumber of valid training triplets in epoch: 54123\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1153\tNumber of valid training triplets in epoch: 54144\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1153\tNumber of valid training triplets in epoch: 54167\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1153\tNumber of valid training triplets in epoch: 54185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1153\tNumber of valid training triplets in epoch: 54200\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54222\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54255\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1152\tNumber of valid training triplets in epoch: 54314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1151\tNumber of valid training triplets in epoch: 54327\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1151\tNumber of valid training triplets in epoch: 54349\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1151\tNumber of valid training triplets in epoch: 54372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1151\tNumber of valid training triplets in epoch: 54392\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1150\tNumber of valid training triplets in epoch: 54410\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1150\tNumber of valid training triplets in epoch: 54429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1150\tNumber of valid training triplets in epoch: 54455\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1150\tNumber of valid training triplets in epoch: 54482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1150\tNumber of valid training triplets in epoch: 54496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1149\tNumber of valid training triplets in epoch: 54515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1149\tNumber of valid training triplets in epoch: 54534\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1149\tNumber of valid training triplets in epoch: 54550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1149\tNumber of valid training triplets in epoch: 54562\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1149\tNumber of valid training triplets in epoch: 54579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1148\tNumber of valid training triplets in epoch: 54599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1148\tNumber of valid training triplets in epoch: 54616\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1148\tNumber of valid training triplets in epoch: 54637\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1148\tNumber of valid training triplets in epoch: 54656\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1148\tNumber of valid training triplets in epoch: 54676\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54692\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54748\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54763\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1147\tNumber of valid training triplets in epoch: 54781\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54816\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54834\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54867\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1146\tNumber of valid training triplets in epoch: 54883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54937\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54959\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1145\tNumber of valid training triplets in epoch: 54992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1144\tNumber of valid training triplets in epoch: 55007\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1144\tNumber of valid training triplets in epoch: 55029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1144\tNumber of valid training triplets in epoch: 55046\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1144\tNumber of valid training triplets in epoch: 55061\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1144\tNumber of valid training triplets in epoch: 55079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55100\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55113\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55127\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55147\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55165\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1143\tNumber of valid training triplets in epoch: 55190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1142\tNumber of valid training triplets in epoch: 55212\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1142\tNumber of valid training triplets in epoch: 55233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1142\tNumber of valid training triplets in epoch: 55259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55291\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55310\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55334\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1141\tNumber of valid training triplets in epoch: 55366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55383\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55459\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1140\tNumber of valid training triplets in epoch: 55474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55544\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55563\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1139\tNumber of valid training triplets in epoch: 55594\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55623\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55668\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55696\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1138\tNumber of valid training triplets in epoch: 55710\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55726\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55743\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55756\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55790\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1137\tNumber of valid training triplets in epoch: 55812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55828\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55870\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55886\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55905\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1136\tNumber of valid training triplets in epoch: 55943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1135\tNumber of valid training triplets in epoch: 55957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1135\tNumber of valid training triplets in epoch: 55975\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1135\tNumber of valid training triplets in epoch: 56003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1135\tNumber of valid training triplets in epoch: 56022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1135\tNumber of valid training triplets in epoch: 56044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56109\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1134\tNumber of valid training triplets in epoch: 56151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56169\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56238\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1133\tNumber of valid training triplets in epoch: 56257\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1132\tNumber of valid training triplets in epoch: 56273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1132\tNumber of valid training triplets in epoch: 56293\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1132\tNumber of valid training triplets in epoch: 56313\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1132\tNumber of valid training triplets in epoch: 56328\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1132\tNumber of valid training triplets in epoch: 56343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56365\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56382\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1131\tNumber of valid training triplets in epoch: 56448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56518\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1130\tNumber of valid training triplets in epoch: 56554\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56584\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56622\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1129\tNumber of valid training triplets in epoch: 56664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1128\tNumber of valid training triplets in epoch: 56679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1128\tNumber of valid training triplets in epoch: 56690\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1128\tNumber of valid training triplets in epoch: 56713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1128\tNumber of valid training triplets in epoch: 56734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1128\tNumber of valid training triplets in epoch: 56759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1127\tNumber of valid training triplets in epoch: 56774\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1127\tNumber of valid training triplets in epoch: 56791\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1127\tNumber of valid training triplets in epoch: 56807\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1127\tNumber of valid training triplets in epoch: 56823\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1127\tNumber of valid training triplets in epoch: 56845\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56867\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56919\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56937\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56953\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1126\tNumber of valid training triplets in epoch: 56965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1125\tNumber of valid training triplets in epoch: 56990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1125\tNumber of valid training triplets in epoch: 57011\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1125\tNumber of valid training triplets in epoch: 57027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1125\tNumber of valid training triplets in epoch: 57047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1125\tNumber of valid training triplets in epoch: 57067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57100\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57125\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57163\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1124\tNumber of valid training triplets in epoch: 57181\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57210\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57226\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57256\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1123\tNumber of valid training triplets in epoch: 57276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1122\tNumber of valid training triplets in epoch: 57304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1122\tNumber of valid training triplets in epoch: 57323\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1122\tNumber of valid training triplets in epoch: 57342\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1122\tNumber of valid training triplets in epoch: 57364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1121\tNumber of valid training triplets in epoch: 57384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1121\tNumber of valid training triplets in epoch: 57399\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1121\tNumber of valid training triplets in epoch: 57416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1121\tNumber of valid training triplets in epoch: 57432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1121\tNumber of valid training triplets in epoch: 57450\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57490\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57537\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57554\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1120\tNumber of valid training triplets in epoch: 57567\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57585\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57618\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57657\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57669\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57686\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1119\tNumber of valid training triplets in epoch: 57702\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57739\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57784\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1118\tNumber of valid training triplets in epoch: 57814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57868\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57899\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1117\tNumber of valid training triplets in epoch: 57921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1116\tNumber of valid training triplets in epoch: 57943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1116\tNumber of valid training triplets in epoch: 57966\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1116\tNumber of valid training triplets in epoch: 57988\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1116\tNumber of valid training triplets in epoch: 58005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1115\tNumber of valid training triplets in epoch: 58020\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1115\tNumber of valid training triplets in epoch: 58038\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1115\tNumber of valid training triplets in epoch: 58057\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1115\tNumber of valid training triplets in epoch: 58079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1115\tNumber of valid training triplets in epoch: 58095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58111\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58132\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58148\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58166\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1114\tNumber of valid training triplets in epoch: 58198\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58219\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58234\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58272\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58305\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1113\tNumber of valid training triplets in epoch: 58322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58387\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58406\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1112\tNumber of valid training triplets in epoch: 58422\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58440\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58483\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58526\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1111\tNumber of valid training triplets in epoch: 58550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58572\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58583\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58650\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1110\tNumber of valid training triplets in epoch: 58680\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58699\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58773\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58792\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1109\tNumber of valid training triplets in epoch: 58819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1108\tNumber of valid training triplets in epoch: 58841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1108\tNumber of valid training triplets in epoch: 58861\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1108\tNumber of valid training triplets in epoch: 58883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1108\tNumber of valid training triplets in epoch: 58901\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 58918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 58934\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 58953\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 58974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 58989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1107\tNumber of valid training triplets in epoch: 59009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59025\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59056\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59075\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59092\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1106\tNumber of valid training triplets in epoch: 59109\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1105\tNumber of valid training triplets in epoch: 59130\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1105\tNumber of valid training triplets in epoch: 59152\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1105\tNumber of valid training triplets in epoch: 59171\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1105\tNumber of valid training triplets in epoch: 59193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1105\tNumber of valid training triplets in epoch: 59210\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1104\tNumber of valid training triplets in epoch: 59233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1104\tNumber of valid training triplets in epoch: 59253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1104\tNumber of valid training triplets in epoch: 59270\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1104\tNumber of valid training triplets in epoch: 59285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1104\tNumber of valid training triplets in epoch: 59305\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59328\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59347\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59363\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59413\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1103\tNumber of valid training triplets in epoch: 59446\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1102\tNumber of valid training triplets in epoch: 59463\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1102\tNumber of valid training triplets in epoch: 59482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1102\tNumber of valid training triplets in epoch: 59503\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1102\tNumber of valid training triplets in epoch: 59523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1102\tNumber of valid training triplets in epoch: 59536\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1101\tNumber of valid training triplets in epoch: 59557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1101\tNumber of valid training triplets in epoch: 59576\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1101\tNumber of valid training triplets in epoch: 59592\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1101\tNumber of valid training triplets in epoch: 59615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1101\tNumber of valid training triplets in epoch: 59646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59661\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59682\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1100\tNumber of valid training triplets in epoch: 59761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59777\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1099\tNumber of valid training triplets in epoch: 59868\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1098\tNumber of valid training triplets in epoch: 59890\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1098\tNumber of valid training triplets in epoch: 59911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1098\tNumber of valid training triplets in epoch: 59928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1098\tNumber of valid training triplets in epoch: 59949\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1098\tNumber of valid training triplets in epoch: 59967\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 59981\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 59992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 60005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 60029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 60047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1097\tNumber of valid training triplets in epoch: 60067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1096\tNumber of valid training triplets in epoch: 60086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1096\tNumber of valid training triplets in epoch: 60103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1096\tNumber of valid training triplets in epoch: 60122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1096\tNumber of valid training triplets in epoch: 60143\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1096\tNumber of valid training triplets in epoch: 60164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60214\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60265\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1095\tNumber of valid training triplets in epoch: 60281\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60319\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60333\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60370\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60387\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1094\tNumber of valid training triplets in epoch: 60411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60446\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60466\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60486\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60504\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1093\tNumber of valid training triplets in epoch: 60523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60575\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1092\tNumber of valid training triplets in epoch: 60624\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1091\tNumber of valid training triplets in epoch: 60643\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1091\tNumber of valid training triplets in epoch: 60662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1091\tNumber of valid training triplets in epoch: 60681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1091\tNumber of valid training triplets in epoch: 60708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1091\tNumber of valid training triplets in epoch: 60722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60762\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60792\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60805\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1090\tNumber of valid training triplets in epoch: 60824\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60856\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60871\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60905\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1089\tNumber of valid training triplets in epoch: 60927\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 60947\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 60964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 60989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 61003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 61016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1088\tNumber of valid training triplets in epoch: 61036\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61061\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61081\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61102\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61134\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61155\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61189\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1087\tNumber of valid training triplets in epoch: 61216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61231\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61251\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1086\tNumber of valid training triplets in epoch: 61339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1085\tNumber of valid training triplets in epoch: 61366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1085\tNumber of valid training triplets in epoch: 61385\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1085\tNumber of valid training triplets in epoch: 61402\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1085\tNumber of valid training triplets in epoch: 61427\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61462\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61532\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1084\tNumber of valid training triplets in epoch: 61548\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61583\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61606\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61629\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61648\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1083\tNumber of valid training triplets in epoch: 61668\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61727\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61779\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61813\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1082\tNumber of valid training triplets in epoch: 61826\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61845\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61866\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1081\tNumber of valid training triplets in epoch: 61940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 61961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 61978\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 61999\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 62019\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 62034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1080\tNumber of valid training triplets in epoch: 62051\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62068\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62088\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62097\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62134\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62153\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1079\tNumber of valid training triplets in epoch: 62170\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62191\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62206\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62297\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1078\tNumber of valid training triplets in epoch: 62317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1077\tNumber of valid training triplets in epoch: 62340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1077\tNumber of valid training triplets in epoch: 62358\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1077\tNumber of valid training triplets in epoch: 62379\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1077\tNumber of valid training triplets in epoch: 62399\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1077\tNumber of valid training triplets in epoch: 62420\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62437\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62462\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62502\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1076\tNumber of valid training triplets in epoch: 62532\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62548\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62586\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62623\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1075\tNumber of valid training triplets in epoch: 62651\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62737\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62757\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1074\tNumber of valid training triplets in epoch: 62770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1073\tNumber of valid training triplets in epoch: 62789\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1073\tNumber of valid training triplets in epoch: 62809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1073\tNumber of valid training triplets in epoch: 62830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1073\tNumber of valid training triplets in epoch: 62854\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1073\tNumber of valid training triplets in epoch: 62875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62922\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62937\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1072\tNumber of valid training triplets in epoch: 62977\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1071\tNumber of valid training triplets in epoch: 63000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1071\tNumber of valid training triplets in epoch: 63015\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1071\tNumber of valid training triplets in epoch: 63030\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1071\tNumber of valid training triplets in epoch: 63062\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1071\tNumber of valid training triplets in epoch: 63080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63099\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63140\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63159\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1070\tNumber of valid training triplets in epoch: 63204\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63221\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63241\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63284\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63301\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1069\tNumber of valid training triplets in epoch: 63319\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63373\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63406\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1068\tNumber of valid training triplets in epoch: 63425\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63446\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63462\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1067\tNumber of valid training triplets in epoch: 63543\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63558\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63593\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1066\tNumber of valid training triplets in epoch: 63645\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63665\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63678\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63692\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63705\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63724\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63739\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63756\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1065\tNumber of valid training triplets in epoch: 63791\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1064\tNumber of valid training triplets in epoch: 63816\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1064\tNumber of valid training triplets in epoch: 63835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1064\tNumber of valid training triplets in epoch: 63856\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1064\tNumber of valid training triplets in epoch: 63866\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1064\tNumber of valid training triplets in epoch: 63887\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 63910\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 63925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 63948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 63965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 63985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1063\tNumber of valid training triplets in epoch: 64003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64024\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64099\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1062\tNumber of valid training triplets in epoch: 64116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64165\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64199\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64218\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1061\tNumber of valid training triplets in epoch: 64233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64264\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64277\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64305\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64326\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1060\tNumber of valid training triplets in epoch: 64343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64380\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64399\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64446\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1059\tNumber of valid training triplets in epoch: 64491\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64510\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64522\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64539\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64556\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1058\tNumber of valid training triplets in epoch: 64608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64641\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64671\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64687\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64710\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1057\tNumber of valid training triplets in epoch: 64724\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64763\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64777\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64810\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1056\tNumber of valid training triplets in epoch: 64834\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64923\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64947\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1055\tNumber of valid training triplets in epoch: 64974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1054\tNumber of valid training triplets in epoch: 64990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1054\tNumber of valid training triplets in epoch: 65009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1054\tNumber of valid training triplets in epoch: 65029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1054\tNumber of valid training triplets in epoch: 65051\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1054\tNumber of valid training triplets in epoch: 65070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65088\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65113\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65129\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1053\tNumber of valid training triplets in epoch: 65197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1052\tNumber of valid training triplets in epoch: 65221\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1052\tNumber of valid training triplets in epoch: 65242\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1052\tNumber of valid training triplets in epoch: 65263\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1052\tNumber of valid training triplets in epoch: 65285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65320\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65333\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65349\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65385\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65410\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1051\tNumber of valid training triplets in epoch: 65425\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65447\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65481\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65528\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65546\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1050\tNumber of valid training triplets in epoch: 65561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1049\tNumber of valid training triplets in epoch: 65582\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1049\tNumber of valid training triplets in epoch: 65606\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1049\tNumber of valid training triplets in epoch: 65621\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1049\tNumber of valid training triplets in epoch: 65638\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1049\tNumber of valid training triplets in epoch: 65662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1048\tNumber of valid training triplets in epoch: 65683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1048\tNumber of valid training triplets in epoch: 65698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1048\tNumber of valid training triplets in epoch: 65713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1048\tNumber of valid training triplets in epoch: 65734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1048\tNumber of valid training triplets in epoch: 65754\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65777\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65791\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65853\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1047\tNumber of valid training triplets in epoch: 65877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1046\tNumber of valid training triplets in epoch: 65896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1046\tNumber of valid training triplets in epoch: 65915\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1046\tNumber of valid training triplets in epoch: 65935\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1046\tNumber of valid training triplets in epoch: 65955\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1046\tNumber of valid training triplets in epoch: 65982\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66024\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66094\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1045\tNumber of valid training triplets in epoch: 66114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1044\tNumber of valid training triplets in epoch: 66133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1044\tNumber of valid training triplets in epoch: 66157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1044\tNumber of valid training triplets in epoch: 66174\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1044\tNumber of valid training triplets in epoch: 66195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1044\tNumber of valid training triplets in epoch: 66217\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66282\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66323\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1043\tNumber of valid training triplets in epoch: 66341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1042\tNumber of valid training triplets in epoch: 66356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1042\tNumber of valid training triplets in epoch: 66380\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1042\tNumber of valid training triplets in epoch: 66403\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1042\tNumber of valid training triplets in epoch: 66419\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1042\tNumber of valid training triplets in epoch: 66442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66475\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66505\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66517\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1041\tNumber of valid training triplets in epoch: 66573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66647\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1040\tNumber of valid training triplets in epoch: 66682\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66712\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66751\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66774\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66789\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1039\tNumber of valid training triplets in epoch: 66807\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1038\tNumber of valid training triplets in epoch: 66831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1038\tNumber of valid training triplets in epoch: 66846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1038\tNumber of valid training triplets in epoch: 66861\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1038\tNumber of valid training triplets in epoch: 66882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1038\tNumber of valid training triplets in epoch: 66906\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 66925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 66948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 66970\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 66987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 67006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1037\tNumber of valid training triplets in epoch: 67022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67055\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67099\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67117\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1036\tNumber of valid training triplets in epoch: 67140\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67167\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67184\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67246\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1035\tNumber of valid training triplets in epoch: 67283\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1034\tNumber of valid training triplets in epoch: 67299\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1034\tNumber of valid training triplets in epoch: 67318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1034\tNumber of valid training triplets in epoch: 67340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1034\tNumber of valid training triplets in epoch: 67360\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1034\tNumber of valid training triplets in epoch: 67381\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67398\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67463\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67498\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1033\tNumber of valid training triplets in epoch: 67511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67570\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67592\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67606\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1032\tNumber of valid training triplets in epoch: 67627\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67645\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67703\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67717\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1031\tNumber of valid training triplets in epoch: 67771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67793\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67833\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67864\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1030\tNumber of valid training triplets in epoch: 67882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67915\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1029\tNumber of valid training triplets in epoch: 67991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68013\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68031\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1028\tNumber of valid training triplets in epoch: 68110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68131\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68170\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68188\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68203\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68218\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1027\tNumber of valid training triplets in epoch: 68237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68319\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1026\tNumber of valid training triplets in epoch: 68355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68371\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68410\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68421\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68440\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68458\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1025\tNumber of valid training triplets in epoch: 68472\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68539\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1024\tNumber of valid training triplets in epoch: 68604\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68621\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68665\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1023\tNumber of valid training triplets in epoch: 68715\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68737\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68774\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1022\tNumber of valid training triplets in epoch: 68848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68862\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68897\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68951\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1021\tNumber of valid training triplets in epoch: 68986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69023\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1020\tNumber of valid training triplets in epoch: 69103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69162\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1019\tNumber of valid training triplets in epoch: 69218\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69311\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1018\tNumber of valid training triplets in epoch: 69334\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69450\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1017\tNumber of valid training triplets in epoch: 69475\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1016\tNumber of valid training triplets in epoch: 69500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1016\tNumber of valid training triplets in epoch: 69520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1016\tNumber of valid training triplets in epoch: 69544\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1016\tNumber of valid training triplets in epoch: 69572\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1015\tNumber of valid training triplets in epoch: 69596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1015\tNumber of valid training triplets in epoch: 69624\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1015\tNumber of valid training triplets in epoch: 69640\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1015\tNumber of valid training triplets in epoch: 69662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1015\tNumber of valid training triplets in epoch: 69674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69693\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69764\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69785\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1014\tNumber of valid training triplets in epoch: 69802\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69813\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69867\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1013\tNumber of valid training triplets in epoch: 69907\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 69925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 69945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 69960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 69981\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 69995\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 70016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1012\tNumber of valid training triplets in epoch: 70032\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70135\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1011\tNumber of valid training triplets in epoch: 70151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70168\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70189\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70222\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70267\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1010\tNumber of valid training triplets in epoch: 70285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70328\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70373\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70390\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1009\tNumber of valid training triplets in epoch: 70418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70437\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70457\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70472\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70514\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1008\tNumber of valid training triplets in epoch: 70547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70585\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70601\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70636\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70657\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1007\tNumber of valid training triplets in epoch: 70683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70715\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70732\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70747\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70766\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1006\tNumber of valid training triplets in epoch: 70804\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70821\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70838\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70872\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70887\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70920\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1005\tNumber of valid training triplets in epoch: 70935\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 70950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 70966\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 70985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 71008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 71033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1004\tNumber of valid training triplets in epoch: 71048\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71064\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71083\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71123\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1003\tNumber of valid training triplets in epoch: 71165\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71184\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1002\tNumber of valid training triplets in epoch: 71277\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71334\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1001\tNumber of valid training triplets in epoch: 71408\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71430\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71444\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71545\n",
            "Epoch 1:\tAverage Triplet Loss: 0.1000\tNumber of valid training triplets in epoch: 71560\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71583\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71603\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71618\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71639\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0999\tNumber of valid training triplets in epoch: 71683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71701\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71776\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0998\tNumber of valid training triplets in epoch: 71795\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0997\tNumber of valid training triplets in epoch: 71823\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0997\tNumber of valid training triplets in epoch: 71844\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0997\tNumber of valid training triplets in epoch: 71872\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0997\tNumber of valid training triplets in epoch: 71891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0997\tNumber of valid training triplets in epoch: 71911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 71931\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 71943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 71961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 71985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 72007\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 72023\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 72040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0996\tNumber of valid training triplets in epoch: 72059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72153\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72166\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0995\tNumber of valid training triplets in epoch: 72185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72220\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72239\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0994\tNumber of valid training triplets in epoch: 72316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72333\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72353\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72371\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72402\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72422\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0993\tNumber of valid training triplets in epoch: 72439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72458\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72499\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0992\tNumber of valid training triplets in epoch: 72558\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72574\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72592\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72611\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72629\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72647\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72659\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0991\tNumber of valid training triplets in epoch: 72675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72706\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72727\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72763\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0990\tNumber of valid training triplets in epoch: 72808\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72827\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0989\tNumber of valid training triplets in epoch: 72929\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 72946\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 72956\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 72976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 72993\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73007\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73025\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73056\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73072\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0988\tNumber of valid training triplets in epoch: 73093\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73136\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73155\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73171\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73191\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0987\tNumber of valid training triplets in epoch: 73207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73290\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0986\tNumber of valid training triplets in epoch: 73337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73354\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0985\tNumber of valid training triplets in epoch: 73474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73532\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73590\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0984\tNumber of valid training triplets in epoch: 73608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73623\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73642\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73665\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73682\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73703\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0983\tNumber of valid training triplets in epoch: 73745\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0982\tNumber of valid training triplets in epoch: 73771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0982\tNumber of valid training triplets in epoch: 73790\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0982\tNumber of valid training triplets in epoch: 73814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0982\tNumber of valid training triplets in epoch: 73830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0982\tNumber of valid training triplets in epoch: 73859\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73910\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73944\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73959\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0981\tNumber of valid training triplets in epoch: 73979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74081\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0980\tNumber of valid training triplets in epoch: 74104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74132\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74153\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74169\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74206\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0979\tNumber of valid training triplets in epoch: 74243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74265\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74306\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74370\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0978\tNumber of valid training triplets in epoch: 74385\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74407\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74424\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74447\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74469\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0977\tNumber of valid training triplets in epoch: 74509\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74549\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74571\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74583\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0976\tNumber of valid training triplets in epoch: 74630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74711\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74738\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0975\tNumber of valid training triplets in epoch: 74758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74780\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74843\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74862\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0974\tNumber of valid training triplets in epoch: 74891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74952\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74968\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0973\tNumber of valid training triplets in epoch: 74998\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75039\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75075\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75089\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75111\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0972\tNumber of valid training triplets in epoch: 75146\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0971\tNumber of valid training triplets in epoch: 75171\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0971\tNumber of valid training triplets in epoch: 75193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0971\tNumber of valid training triplets in epoch: 75218\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0971\tNumber of valid training triplets in epoch: 75237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0971\tNumber of valid training triplets in epoch: 75268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75290\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75313\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75354\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0970\tNumber of valid training triplets in epoch: 75393\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75502\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0969\tNumber of valid training triplets in epoch: 75535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75551\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75571\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75584\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75620\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75639\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75653\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0968\tNumber of valid training triplets in epoch: 75673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75692\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75707\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75740\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75778\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0967\tNumber of valid training triplets in epoch: 75797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75838\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75857\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75881\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75908\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0966\tNumber of valid training triplets in epoch: 75922\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 75941\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 75954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 75977\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 75990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 76012\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 76030\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0965\tNumber of valid training triplets in epoch: 76048\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76071\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76127\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76143\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0964\tNumber of valid training triplets in epoch: 76175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76192\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76230\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76247\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76277\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76312\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0963\tNumber of valid training triplets in epoch: 76329\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76350\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76361\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76420\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0962\tNumber of valid training triplets in epoch: 76461\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76483\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76518\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76534\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76582\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76594\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0961\tNumber of valid training triplets in epoch: 76616\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76639\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76661\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76696\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76712\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76732\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0960\tNumber of valid training triplets in epoch: 76769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76810\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76824\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76843\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76861\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76874\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0959\tNumber of valid training triplets in epoch: 76893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 76916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 76932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 76948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 76969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 76991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 77010\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0958\tNumber of valid training triplets in epoch: 77032\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77074\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0957\tNumber of valid training triplets in epoch: 77150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77217\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77251\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77283\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0956\tNumber of valid training triplets in epoch: 77297\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77375\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77395\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0955\tNumber of valid training triplets in epoch: 77416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0954\tNumber of valid training triplets in epoch: 77442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0954\tNumber of valid training triplets in epoch: 77464\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0954\tNumber of valid training triplets in epoch: 77484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0954\tNumber of valid training triplets in epoch: 77507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0954\tNumber of valid training triplets in epoch: 77529\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77575\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77597\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0953\tNumber of valid training triplets in epoch: 77676\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77737\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77808\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0952\tNumber of valid training triplets in epoch: 77825\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77860\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77887\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77901\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77917\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0951\tNumber of valid training triplets in epoch: 77952\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 77974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 77991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 78014\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 78039\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 78063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0950\tNumber of valid training triplets in epoch: 78079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78123\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78144\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0949\tNumber of valid training triplets in epoch: 78201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0948\tNumber of valid training triplets in epoch: 78223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0948\tNumber of valid training triplets in epoch: 78243\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0948\tNumber of valid training triplets in epoch: 78269\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0948\tNumber of valid training triplets in epoch: 78287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0948\tNumber of valid training triplets in epoch: 78312\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78358\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78399\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78434\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0947\tNumber of valid training triplets in epoch: 78458\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78487\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78504\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78574\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0946\tNumber of valid training triplets in epoch: 78587\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78619\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78640\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78657\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78688\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0945\tNumber of valid training triplets in epoch: 78741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78780\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78801\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78823\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78839\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0944\tNumber of valid training triplets in epoch: 78861\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78907\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78929\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0943\tNumber of valid training triplets in epoch: 78995\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79036\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79113\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79124\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0942\tNumber of valid training triplets in epoch: 79137\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79179\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79219\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79256\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79272\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0941\tNumber of valid training triplets in epoch: 79287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79398\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79415\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0940\tNumber of valid training triplets in epoch: 79452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79548\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79560\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0939\tNumber of valid training triplets in epoch: 79580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79642\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79687\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0938\tNumber of valid training triplets in epoch: 79705\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79743\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79764\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79776\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0937\tNumber of valid training triplets in epoch: 79833\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79857\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0936\tNumber of valid training triplets in epoch: 79989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80028\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80096\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80117\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0935\tNumber of valid training triplets in epoch: 80142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80160\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80176\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80189\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80249\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0934\tNumber of valid training triplets in epoch: 80289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80310\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80349\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80367\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0933\tNumber of valid training triplets in epoch: 80414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80434\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80490\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80509\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0932\tNumber of valid training triplets in epoch: 80535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80559\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80648\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80658\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0931\tNumber of valid training triplets in epoch: 80691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80730\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80824\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0930\tNumber of valid training triplets in epoch: 80841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80862\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80941\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0929\tNumber of valid training triplets in epoch: 80960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 80985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81011\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81026\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81066\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0928\tNumber of valid training triplets in epoch: 81105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0927\tNumber of valid training triplets in epoch: 81129\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0927\tNumber of valid training triplets in epoch: 81156\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0927\tNumber of valid training triplets in epoch: 81183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0927\tNumber of valid training triplets in epoch: 81201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0927\tNumber of valid training triplets in epoch: 81223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81271\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81283\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81301\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0926\tNumber of valid training triplets in epoch: 81373\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81405\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81438\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81457\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0925\tNumber of valid training triplets in epoch: 81512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81592\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81612\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0924\tNumber of valid training triplets in epoch: 81651\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81688\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81704\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81726\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81748\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0923\tNumber of valid training triplets in epoch: 81788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81825\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81868\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81886\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81900\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0922\tNumber of valid training triplets in epoch: 81933\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 81958\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 81971\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 81989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 82018\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 82033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 82055\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0921\tNumber of valid training triplets in epoch: 82074\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82096\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82137\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82156\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82171\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0920\tNumber of valid training triplets in epoch: 82228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82249\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82284\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82334\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0919\tNumber of valid training triplets in epoch: 82348\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82443\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82463\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82480\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0918\tNumber of valid training triplets in epoch: 82497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82513\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82529\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82581\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0917\tNumber of valid training triplets in epoch: 82664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82678\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0916\tNumber of valid training triplets in epoch: 82822\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82894\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0915\tNumber of valid training triplets in epoch: 82956\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 82971\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 82990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83026\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83061\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83102\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0914\tNumber of valid training triplets in epoch: 83121\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83187\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83238\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0913\tNumber of valid training triplets in epoch: 83264\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83353\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83376\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0912\tNumber of valid training triplets in epoch: 83398\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83470\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83518\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0911\tNumber of valid training triplets in epoch: 83537\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83582\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83612\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83633\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83648\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0910\tNumber of valid training triplets in epoch: 83669\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83733\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0909\tNumber of valid training triplets in epoch: 83818\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83837\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83854\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83899\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83924\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83962\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0908\tNumber of valid training triplets in epoch: 83978\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 83998\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84026\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84046\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84083\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0907\tNumber of valid training triplets in epoch: 84116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84138\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84162\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0906\tNumber of valid training triplets in epoch: 84253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84300\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0905\tNumber of valid training triplets in epoch: 84417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84455\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84518\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0904\tNumber of valid training triplets in epoch: 84559\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84620\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84637\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84656\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0903\tNumber of valid training triplets in epoch: 84694\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84765\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84821\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0902\tNumber of valid training triplets in epoch: 84838\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84854\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84927\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0901\tNumber of valid training triplets in epoch: 84985\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85057\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85102\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0900\tNumber of valid training triplets in epoch: 85122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85187\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85241\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0899\tNumber of valid training triplets in epoch: 85258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85279\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0898\tNumber of valid training triplets in epoch: 85388\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85459\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85505\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0897\tNumber of valid training triplets in epoch: 85530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85628\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85648\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0896\tNumber of valid training triplets in epoch: 85684\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85719\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85740\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85777\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85829\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0895\tNumber of valid training triplets in epoch: 85842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85856\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0894\tNumber of valid training triplets in epoch: 85983\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86019\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86035\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86055\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86094\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86131\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0893\tNumber of valid training triplets in epoch: 86150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86199\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86219\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0892\tNumber of valid training triplets in epoch: 86286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86312\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86345\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86373\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86393\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0891\tNumber of valid training triplets in epoch: 86409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86453\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86470\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86536\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0890\tNumber of valid training triplets in epoch: 86557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86638\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86656\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0889\tNumber of valid training triplets in epoch: 86717\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86739\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86779\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86799\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86818\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86834\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0888\tNumber of valid training triplets in epoch: 86877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 86901\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 86918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 86942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 86964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 86992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0887\tNumber of valid training triplets in epoch: 87014\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87038\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87071\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87093\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87130\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0886\tNumber of valid training triplets in epoch: 87164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87184\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87246\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87306\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0885\tNumber of valid training triplets in epoch: 87322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87375\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87436\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0884\tNumber of valid training triplets in epoch: 87477\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87562\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0883\tNumber of valid training triplets in epoch: 87626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87663\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87705\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87728\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0882\tNumber of valid training triplets in epoch: 87780\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87798\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87848\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87871\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87900\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87934\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0881\tNumber of valid training triplets in epoch: 87955\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 87968\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 87986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88026\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88042\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88077\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0880\tNumber of valid training triplets in epoch: 88098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88153\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88170\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88239\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88251\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0879\tNumber of valid training triplets in epoch: 88274\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88371\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0878\tNumber of valid training triplets in epoch: 88413\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88470\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0877\tNumber of valid training triplets in epoch: 88578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88591\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88607\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88641\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88661\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88688\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88706\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88726\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0876\tNumber of valid training triplets in epoch: 88744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88765\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88785\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88829\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88843\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88864\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88883\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0875\tNumber of valid training triplets in epoch: 88894\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 88914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 88930\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 88950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 88969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 88987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 89006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 89027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0874\tNumber of valid training triplets in epoch: 89045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89161\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89176\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0873\tNumber of valid training triplets in epoch: 89195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89215\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89235\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89277\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89297\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89321\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0872\tNumber of valid training triplets in epoch: 89362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89383\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89478\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89495\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0871\tNumber of valid training triplets in epoch: 89520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89559\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89658\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0870\tNumber of valid training triplets in epoch: 89681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89754\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89800\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89823\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0869\tNumber of valid training triplets in epoch: 89843\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89862\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89876\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89913\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89933\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89977\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0868\tNumber of valid training triplets in epoch: 89991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90052\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90131\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0867\tNumber of valid training triplets in epoch: 90167\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90191\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90250\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90270\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0866\tNumber of valid training triplets in epoch: 90322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90370\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90392\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90413\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90433\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90469\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0865\tNumber of valid training triplets in epoch: 90488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90562\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90571\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90607\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0864\tNumber of valid training triplets in epoch: 90650\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90696\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90711\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90751\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90768\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90804\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0863\tNumber of valid training triplets in epoch: 90820\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90840\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90873\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0862\tNumber of valid training triplets in epoch: 90983\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91025\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91083\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91103\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0861\tNumber of valid training triplets in epoch: 91161\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91198\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91221\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91244\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91301\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0860\tNumber of valid training triplets in epoch: 91322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91345\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91367\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91419\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91437\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91477\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0859\tNumber of valid training triplets in epoch: 91491\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91539\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91636\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91658\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0858\tNumber of valid training triplets in epoch: 91675\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91747\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0857\tNumber of valid training triplets in epoch: 91815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91849\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91872\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91944\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0856\tNumber of valid training triplets in epoch: 91974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 91989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92032\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92048\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92069\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0855\tNumber of valid training triplets in epoch: 92134\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92155\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92176\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92199\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92222\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0854\tNumber of valid training triplets in epoch: 92295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92375\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92413\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0853\tNumber of valid training triplets in epoch: 92460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92480\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92514\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92545\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92570\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92585\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92605\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0852\tNumber of valid training triplets in epoch: 92638\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92689\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92754\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92779\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92801\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0851\tNumber of valid training triplets in epoch: 92819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92836\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92894\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92938\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92958\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0850\tNumber of valid training triplets in epoch: 92971\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 92992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93010\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93032\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93052\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93066\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93094\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0849\tNumber of valid training triplets in epoch: 93142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93167\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93185\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93245\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93263\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93288\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0848\tNumber of valid training triplets in epoch: 93303\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93412\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93444\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0847\tNumber of valid training triplets in epoch: 93465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93551\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93572\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93591\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0846\tNumber of valid training triplets in epoch: 93633\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93676\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93699\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93760\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0845\tNumber of valid training triplets in epoch: 93787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93825\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93868\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93886\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93907\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93931\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0844\tNumber of valid training triplets in epoch: 93966\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 93992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94024\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94078\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94097\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94117\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0843\tNumber of valid training triplets in epoch: 94152\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94194\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94214\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94233\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94292\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0842\tNumber of valid training triplets in epoch: 94317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94450\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94466\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0841\tNumber of valid training triplets in epoch: 94489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94534\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94590\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94629\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94667\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0840\tNumber of valid training triplets in epoch: 94681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94743\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94766\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94781\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94803\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94821\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94839\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0839\tNumber of valid training triplets in epoch: 94855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94876\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94897\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94933\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 94987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 95006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 95024\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0838\tNumber of valid training triplets in epoch: 95043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95097\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95148\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0837\tNumber of valid training triplets in epoch: 95187\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95229\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95249\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95265\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95358\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0836\tNumber of valid training triplets in epoch: 95377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95440\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0835\tNumber of valid training triplets in epoch: 95556\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95571\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95590\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95632\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95677\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95706\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0834\tNumber of valid training triplets in epoch: 95732\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95757\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95781\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95803\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95827\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95856\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95876\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0833\tNumber of valid training triplets in epoch: 95898\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 95917\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 95936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 95953\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 95976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 96000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 96021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 96044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0832\tNumber of valid training triplets in epoch: 96070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96088\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96125\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0831\tNumber of valid training triplets in epoch: 96250\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96306\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96344\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96379\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0830\tNumber of valid training triplets in epoch: 96440\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96458\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96475\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96503\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96521\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96548\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96581\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96597\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0829\tNumber of valid training triplets in epoch: 96617\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96637\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96663\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96678\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96702\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0828\tNumber of valid training triplets in epoch: 96770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96793\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96895\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96913\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96935\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0827\tNumber of valid training triplets in epoch: 96958\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 96975\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97035\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97058\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97074\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97101\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0826\tNumber of valid training triplets in epoch: 97123\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97141\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97169\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97184\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97206\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97269\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0825\tNumber of valid training triplets in epoch: 97308\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97327\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97467\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0824\tNumber of valid training triplets in epoch: 97487\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97545\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97566\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97586\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97607\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97653\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0823\tNumber of valid training triplets in epoch: 97676\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97704\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97721\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97765\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0822\tNumber of valid training triplets in epoch: 97856\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97908\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97949\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 97990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 98009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 98034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0821\tNumber of valid training triplets in epoch: 98053\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98089\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98134\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0820\tNumber of valid training triplets in epoch: 98230\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0819\tNumber of valid training triplets in epoch: 98396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98420\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98461\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98485\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98539\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0818\tNumber of valid training triplets in epoch: 98560\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98711\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0817\tNumber of valid training triplets in epoch: 98731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98753\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98770\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98790\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98826\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98862\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98880\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98899\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0816\tNumber of valid training triplets in epoch: 98940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 98956\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 98972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 98992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99030\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99053\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99101\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0815\tNumber of valid training triplets in epoch: 99138\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99156\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99208\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99271\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99319\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0814\tNumber of valid training triplets in epoch: 99339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99361\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99379\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99395\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99505\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0813\tNumber of valid training triplets in epoch: 99530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99568\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99627\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0812\tNumber of valid training triplets in epoch: 99702\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99789\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99836\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99860\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99874\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0811\tNumber of valid training triplets in epoch: 99892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 99915\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 99936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 99960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 99972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 99999\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 100020\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 100039\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 100055\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0810\tNumber of valid training triplets in epoch: 100075\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100093\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100123\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100229\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0809\tNumber of valid training triplets in epoch: 100278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100297\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100376\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100399\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100436\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0808\tNumber of valid training triplets in epoch: 100478\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100584\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100629\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0807\tNumber of valid training triplets in epoch: 100649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100671\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100692\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100716\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100753\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100778\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0806\tNumber of valid training triplets in epoch: 100840\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100859\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100888\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100915\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100951\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100971\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 100993\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0805\tNumber of valid training triplets in epoch: 101016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101061\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101084\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101108\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101147\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101160\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101174\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101196\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0804\tNumber of valid training triplets in epoch: 101214\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101238\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101262\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101290\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101329\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0803\tNumber of valid training triplets in epoch: 101381\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101467\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101537\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0802\tNumber of valid training triplets in epoch: 101564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101581\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101645\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101669\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101685\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101710\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101730\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0801\tNumber of valid training triplets in epoch: 101751\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101837\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101860\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0800\tNumber of valid training triplets in epoch: 101936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 101964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 101980\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102064\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102083\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102106\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102127\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0799\tNumber of valid training triplets in epoch: 102145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102163\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102235\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102279\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102293\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102312\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0798\tNumber of valid training triplets in epoch: 102331\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102353\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102402\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102426\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102464\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102480\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102499\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102519\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0797\tNumber of valid training triplets in epoch: 102540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102591\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102616\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102638\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102685\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102702\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0796\tNumber of valid training triplets in epoch: 102727\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102791\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102810\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102872\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102917\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0795\tNumber of valid training triplets in epoch: 102929\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 102947\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 102970\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 102991\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103010\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103031\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103056\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103075\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103092\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0794\tNumber of valid training triplets in epoch: 103116\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103132\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103174\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103234\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103274\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103293\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0793\tNumber of valid training triplets in epoch: 103314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103360\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103381\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103413\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103438\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0792\tNumber of valid training triplets in epoch: 103489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103513\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103553\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103576\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103604\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103621\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103640\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103658\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0791\tNumber of valid training triplets in epoch: 103691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103712\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103740\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103780\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103799\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103822\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0790\tNumber of valid training triplets in epoch: 103866\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 103894\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 103918\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 103936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 103954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 103983\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 104006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 104033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 104054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0789\tNumber of valid training triplets in epoch: 104082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104121\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104143\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104161\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104210\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104227\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104250\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0788\tNumber of valid training triplets in epoch: 104273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104310\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104353\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104387\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104408\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0787\tNumber of valid training triplets in epoch: 104474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104536\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104576\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104619\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104639\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0786\tNumber of valid training triplets in epoch: 104662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104717\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104739\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104839\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0785\tNumber of valid training triplets in epoch: 104860\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104903\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104939\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104959\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 104982\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 105005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 105022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 105040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0784\tNumber of valid training triplets in epoch: 105063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105083\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105101\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105120\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105133\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105181\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0783\tNumber of valid training triplets in epoch: 105272\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105299\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105317\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105336\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105354\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105392\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105432\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105453\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0782\tNumber of valid training triplets in epoch: 105476\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105498\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105571\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105654\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0781\tNumber of valid training triplets in epoch: 105695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105717\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105810\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105834\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105852\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105870\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0780\tNumber of valid training triplets in epoch: 105893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 105917\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 105936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 105960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 105978\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 105997\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106053\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106097\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0779\tNumber of valid training triplets in epoch: 106119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106137\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106158\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106203\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106269\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0778\tNumber of valid training triplets in epoch: 106324\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106343\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106382\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106426\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106448\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106486\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106506\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106529\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0777\tNumber of valid training triplets in epoch: 106549\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106590\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106654\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106694\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0776\tNumber of valid training triplets in epoch: 106751\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106828\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106852\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106867\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106890\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106905\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106920\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106944\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0775\tNumber of valid training triplets in epoch: 106963\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 106988\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107007\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107130\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107147\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0774\tNumber of valid training triplets in epoch: 107169\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107196\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107214\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107308\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107326\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0773\tNumber of valid training triplets in epoch: 107368\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107438\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107493\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107510\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107530\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107568\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0772\tNumber of valid training triplets in epoch: 107588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107655\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107747\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107764\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0771\tNumber of valid training triplets in epoch: 107786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107807\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107882\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107908\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107931\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107955\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0770\tNumber of valid training triplets in epoch: 107986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108050\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108089\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108131\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0769\tNumber of valid training triplets in epoch: 108211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108250\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108267\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108291\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108308\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108329\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108350\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108393\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0768\tNumber of valid training triplets in epoch: 108416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108435\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108475\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108490\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108506\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108552\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108576\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108594\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108616\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108632\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0767\tNumber of valid training triplets in epoch: 108653\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108694\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108719\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108827\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108845\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0766\tNumber of valid training triplets in epoch: 108869\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108890\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108934\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108954\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 108993\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 109010\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 109036\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 109051\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 109073\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0765\tNumber of valid training triplets in epoch: 109091\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109106\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109174\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109217\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0764\tNumber of valid training triplets in epoch: 109295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109333\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109350\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109367\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109385\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109425\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109464\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109502\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0763\tNumber of valid training triplets in epoch: 109523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109562\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109579\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109619\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109638\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109653\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109686\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109702\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0762\tNumber of valid training triplets in epoch: 109750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109809\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109897\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109920\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0761\tNumber of valid training triplets in epoch: 109983\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 109997\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110013\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110037\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110069\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110136\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110160\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110179\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0760\tNumber of valid training triplets in epoch: 110196\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110221\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110293\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110311\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110391\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0759\tNumber of valid training triplets in epoch: 110415\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110461\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110500\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110570\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0758\tNumber of valid training triplets in epoch: 110614\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110648\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110687\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110704\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110726\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110768\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110784\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110805\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110829\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0757\tNumber of valid training triplets in epoch: 110851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110874\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110896\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110937\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110977\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 110993\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 111008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 111033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0756\tNumber of valid training triplets in epoch: 111054\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111085\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111102\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111146\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111165\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111183\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111241\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111283\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0755\tNumber of valid training triplets in epoch: 111297\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111324\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111344\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111356\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111375\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111462\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111486\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111504\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111517\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0754\tNumber of valid training triplets in epoch: 111541\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111577\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111599\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111621\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111687\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111733\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0753\tNumber of valid training triplets in epoch: 111756\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111776\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111796\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111826\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111866\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111906\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111923\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111939\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0752\tNumber of valid training triplets in epoch: 111979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112020\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112064\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112125\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112173\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0751\tNumber of valid training triplets in epoch: 112216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112259\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112303\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112352\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112388\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0750\tNumber of valid training triplets in epoch: 112449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112470\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112483\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112502\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112525\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112542\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112593\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112620\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112642\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0749\tNumber of valid training triplets in epoch: 112662\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112687\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112707\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112760\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112783\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112805\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112824\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112863\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112887\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0748\tNumber of valid training triplets in epoch: 112907\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 112933\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 112950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 112965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 112982\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113000\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113024\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113073\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0747\tNumber of valid training triplets in epoch: 113143\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113204\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113225\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113244\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113265\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113313\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113330\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0746\tNumber of valid training triplets in epoch: 113374\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113392\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113433\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113498\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113534\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113553\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113574\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0745\tNumber of valid training triplets in epoch: 113596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113615\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113636\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113699\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113736\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113779\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0744\tNumber of valid training triplets in epoch: 113814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113838\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113866\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113917\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113940\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113963\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 113987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 114006\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 114027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0743\tNumber of valid training triplets in epoch: 114048\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114067\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114127\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114144\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114188\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114206\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114227\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114253\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0742\tNumber of valid training triplets in epoch: 114279\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114299\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114313\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114336\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114375\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114392\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114411\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114430\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114475\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114491\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0741\tNumber of valid training triplets in epoch: 114511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114536\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114630\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114658\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114682\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114704\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0740\tNumber of valid training triplets in epoch: 114752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114789\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114808\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114849\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114870\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114962\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0739\tNumber of valid training triplets in epoch: 114987\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115011\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115033\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115094\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115149\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115181\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115198\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115217\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115235\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0738\tNumber of valid training triplets in epoch: 115254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115319\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0737\tNumber of valid training triplets in epoch: 115507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115552\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115592\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115624\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115647\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115667\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115716\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0736\tNumber of valid training triplets in epoch: 115752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115768\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115818\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115837\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115885\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 115984\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0735\tNumber of valid training triplets in epoch: 116005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116025\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116062\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116106\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116127\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116147\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116166\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116229\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0734\tNumber of valid training triplets in epoch: 116249\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116263\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116281\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116303\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116320\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116407\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116431\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116454\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116471\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0733\tNumber of valid training triplets in epoch: 116512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116620\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116637\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116698\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116723\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0732\tNumber of valid training triplets in epoch: 116744\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116765\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116782\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116800\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116820\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116864\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116906\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116944\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 116980\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0731\tNumber of valid training triplets in epoch: 117003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117020\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117037\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117057\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117078\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117100\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117173\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117227\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0730\tNumber of valid training triplets in epoch: 117247\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117277\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117324\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117388\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117406\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117430\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117450\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0729\tNumber of valid training triplets in epoch: 117488\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117508\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117554\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117607\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117627\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117650\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117734\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0728\tNumber of valid training triplets in epoch: 117750\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117772\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117788\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117808\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117828\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117849\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117869\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117889\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117942\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117961\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0727\tNumber of valid training triplets in epoch: 117997\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118051\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118120\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118140\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118159\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118177\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118219\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0726\tNumber of valid training triplets in epoch: 118240\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118262\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118338\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118358\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118380\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118451\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0725\tNumber of valid training triplets in epoch: 118468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118495\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118557\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118575\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118596\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118616\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118654\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118677\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118728\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0724\tNumber of valid training triplets in epoch: 118746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118768\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118787\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118804\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118818\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118846\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118898\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118919\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118952\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118968\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0723\tNumber of valid training triplets in epoch: 118996\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119023\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119043\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119071\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119089\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119108\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119144\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119166\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119190\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119212\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0722\tNumber of valid training triplets in epoch: 119235\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119300\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119402\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0721\tNumber of valid training triplets in epoch: 119469\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119492\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119509\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119532\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119550\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119570\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119589\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119635\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119656\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119693\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119711\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0720\tNumber of valid training triplets in epoch: 119737\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119774\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119792\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119835\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119854\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119880\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119906\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119929\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119949\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0719\tNumber of valid training triplets in epoch: 119990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120010\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120050\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120075\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120099\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120118\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120139\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0718\tNumber of valid training triplets in epoch: 120252\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120293\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120359\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120372\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120423\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0717\tNumber of valid training triplets in epoch: 120491\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120514\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120565\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120612\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120641\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120660\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120676\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120703\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120722\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0716\tNumber of valid training triplets in epoch: 120737\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120761\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120781\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120795\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120817\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120829\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120850\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120874\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120891\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120912\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120970\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 120990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0715\tNumber of valid training triplets in epoch: 121009\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121031\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121058\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121073\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121095\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121114\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121137\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121252\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0714\tNumber of valid training triplets in epoch: 121268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121377\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121420\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121515\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0713\tNumber of valid training triplets in epoch: 121538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121563\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121606\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121647\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121685\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121704\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121728\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121748\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0712\tNumber of valid training triplets in epoch: 121793\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121839\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121857\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121875\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121892\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121913\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121933\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121959\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 121998\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 122017\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 122041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0711\tNumber of valid training triplets in epoch: 122060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122078\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122129\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122151\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122195\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122231\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122278\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122295\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0710\tNumber of valid training triplets in epoch: 122320\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122387\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122405\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122450\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122469\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122509\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0709\tNumber of valid training triplets in epoch: 122581\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122642\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122665\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122686\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122705\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122728\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122748\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122773\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122795\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0708\tNumber of valid training triplets in epoch: 122814\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122857\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122880\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122927\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122968\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 122982\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 123005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 123027\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 123047\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 123070\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0707\tNumber of valid training triplets in epoch: 123089\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123112\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123132\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123152\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123172\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123194\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123231\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123257\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123323\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123342\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0706\tNumber of valid training triplets in epoch: 123362\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123381\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123424\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123452\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123474\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123501\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123523\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123546\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123568\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123593\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0705\tNumber of valid training triplets in epoch: 123613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123654\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123677\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123696\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123773\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123790\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123819\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123912\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0704\tNumber of valid training triplets in epoch: 123926\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 123945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 123964\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 123984\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124008\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124078\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124105\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124143\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124161\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0703\tNumber of valid training triplets in epoch: 124178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124229\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124286\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124304\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124322\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124359\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124380\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124401\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124418\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124434\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0702\tNumber of valid training triplets in epoch: 124460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124481\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124504\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124551\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124570\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124587\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124608\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124632\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124667\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0701\tNumber of valid training triplets in epoch: 124749\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124795\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124811\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124878\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124898\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124922\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124941\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124959\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 124980\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0700\tNumber of valid training triplets in epoch: 125011\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125030\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125062\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125132\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125201\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125222\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125241\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0699\tNumber of valid training triplets in epoch: 125281\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125299\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125351\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125382\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125405\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125426\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125459\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125477\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125496\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125521\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125543\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0698\tNumber of valid training triplets in epoch: 125562\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125586\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125606\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125626\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125659\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125681\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125695\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125797\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125816\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125840\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0697\tNumber of valid training triplets in epoch: 125858\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125879\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125904\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125932\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125950\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125976\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 125998\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126014\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126032\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126053\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126110\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0696\tNumber of valid training triplets in epoch: 126135\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126181\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126200\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126216\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126254\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126273\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126290\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126308\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126331\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126378\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126414\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0695\tNumber of valid training triplets in epoch: 126435\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126453\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126470\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126491\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126533\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126544\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126563\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126585\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126612\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126674\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126718\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0694\tNumber of valid training triplets in epoch: 126740\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126759\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126775\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126792\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126815\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126837\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126857\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126880\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126900\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126919\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126941\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126966\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 126989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 127003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0693\tNumber of valid training triplets in epoch: 127022\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127040\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127061\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127076\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127099\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127120\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127139\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127160\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127228\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127250\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127288\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0692\tNumber of valid training triplets in epoch: 127303\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127345\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127364\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127382\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127402\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127433\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127459\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127483\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127497\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127517\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127539\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127563\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127584\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0691\tNumber of valid training triplets in epoch: 127598\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127621\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127644\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127740\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127790\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127816\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127836\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0690\tNumber of valid training triplets in epoch: 127864\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127885\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127901\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127919\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127939\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127957\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127973\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 127992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128019\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128034\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128055\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128077\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128094\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128108\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128125\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128141\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0689\tNumber of valid training triplets in epoch: 128164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128186\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128212\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128237\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128260\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128283\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128298\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128316\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128339\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128359\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128380\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128406\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128422\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128438\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0688\tNumber of valid training triplets in epoch: 128455\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128477\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128516\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128587\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128597\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128620\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128636\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128657\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128677\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128697\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128716\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128742\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0687\tNumber of valid training triplets in epoch: 128762\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128783\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128808\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128827\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128909\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128931\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128963\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 128986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 129004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 129029\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0686\tNumber of valid training triplets in epoch: 129059\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129081\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129104\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129141\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129184\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129227\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129249\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129266\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129305\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129326\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0685\tNumber of valid training triplets in epoch: 129366\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129383\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129405\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129428\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129449\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129464\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129484\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129513\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129531\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129551\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129561\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129634\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129652\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129667\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0684\tNumber of valid training triplets in epoch: 129691\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129713\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129755\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129778\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129791\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129805\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129822\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129841\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129861\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129910\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129934\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0683\tNumber of valid training triplets in epoch: 129986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130005\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130046\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130069\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130090\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130117\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130135\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130159\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130177\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130197\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130224\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130245\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0682\tNumber of valid training triplets in epoch: 130261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130282\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130300\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130337\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130357\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130379\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130427\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130444\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130487\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130505\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130526\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130545\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0681\tNumber of valid training triplets in epoch: 130564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130587\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130609\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130628\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130647\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130671\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130684\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130765\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130799\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0680\tNumber of valid training triplets in epoch: 130878\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130899\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130914\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130936\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130952\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130974\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 130992\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131088\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131111\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131128\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131146\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131182\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0679\tNumber of valid training triplets in epoch: 131207\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131227\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131244\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131268\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131289\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131308\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131332\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131378\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131394\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131419\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131441\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131466\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131499\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0678\tNumber of valid training triplets in epoch: 131517\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131538\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131587\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131612\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131629\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131652\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131686\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131708\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131724\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131758\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131784\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0677\tNumber of valid training triplets in epoch: 131806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131854\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131876\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131902\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131924\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131962\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 131979\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132004\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132026\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132065\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132102\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0676\tNumber of valid training triplets in epoch: 132122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132144\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132162\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132226\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132245\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132302\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132320\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132341\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132365\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132381\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132397\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0675\tNumber of valid training triplets in epoch: 132437\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132473\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132512\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132535\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132555\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132578\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132600\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132623\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132683\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132701\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132721\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132747\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0674\tNumber of valid training triplets in epoch: 132763\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132783\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132805\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132833\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132855\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132897\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132916\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132945\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132967\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 132986\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 133003\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 133023\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 133049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 133063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0673\tNumber of valid training triplets in epoch: 133078\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133101\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133141\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133175\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133199\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133220\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133239\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133256\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133270\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133285\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133307\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133325\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133349\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0672\tNumber of valid training triplets in epoch: 133404\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133424\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133465\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133487\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133528\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133549\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133575\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133595\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133625\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133670\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133689\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0671\tNumber of valid training triplets in epoch: 133714\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133729\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133752\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133771\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133789\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133812\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133831\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133851\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133870\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133888\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133911\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133939\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133955\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133973\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 133994\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 134016\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0670\tNumber of valid training triplets in epoch: 134041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134062\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134086\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134112\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134135\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134159\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134180\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134203\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134226\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134248\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134300\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134324\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134346\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0669\tNumber of valid training triplets in epoch: 134367\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134398\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134415\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134458\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134482\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134504\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134526\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134548\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134569\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134597\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134613\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134632\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134651\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0668\tNumber of valid training triplets in epoch: 134671\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134693\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134720\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134741\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134754\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134773\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134794\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134810\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134825\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134840\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134864\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134905\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134928\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134948\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134969\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 134996\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0667\tNumber of valid training triplets in epoch: 135015\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135039\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135057\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135079\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135124\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135150\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135173\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135192\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135211\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135230\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135256\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0666\tNumber of valid training triplets in epoch: 135318\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135340\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135363\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135382\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135400\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135417\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135464\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135485\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135505\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135526\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135547\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135588\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135609\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135631\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135649\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0665\tNumber of valid training triplets in epoch: 135673\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135694\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135712\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135731\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135753\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135769\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135785\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135830\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135847\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135865\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135893\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135925\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135944\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135960\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135981\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0664\tNumber of valid training triplets in epoch: 135995\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136020\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136045\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136064\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136087\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136106\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136119\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136142\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136164\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136181\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136202\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136220\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136236\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136251\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136270\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136287\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136309\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0663\tNumber of valid training triplets in epoch: 136331\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136352\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136369\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136384\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136405\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136428\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136460\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136479\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136498\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136518\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136541\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136564\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136581\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0662\tNumber of valid training triplets in epoch: 136624\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136646\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136664\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136682\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136700\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136725\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136764\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136793\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136813\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136832\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136849\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136867\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136884\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136901\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136921\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136943\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0661\tNumber of valid training triplets in epoch: 136967\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 136989\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137011\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137030\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137049\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137069\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137107\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137126\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137145\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137161\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137194\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137213\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137234\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137261\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137280\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0660\tNumber of valid training triplets in epoch: 137309\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137334\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137355\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137370\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137389\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137409\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137429\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137455\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137468\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137489\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137507\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137528\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137551\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137573\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137593\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137610\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137632\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0659\tNumber of valid training triplets in epoch: 137652\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137672\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137694\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137709\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137727\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137746\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137767\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137786\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137801\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137823\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137842\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137863\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137895\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137910\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137931\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137949\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137965\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 137988\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0658\tNumber of valid training triplets in epoch: 138002\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138021\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138041\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138063\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138080\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138096\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138111\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138131\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138157\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138179\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138193\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138209\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138223\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138245\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138262\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138276\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138296\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138314\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138335\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0657\tNumber of valid training triplets in epoch: 138358\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138376\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138403\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138425\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138442\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138461\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138481\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138494\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138520\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138540\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138558\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138580\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138602\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138622\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138645\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138666\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138679\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138701\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0656\tNumber of valid training triplets in epoch: 138719\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138735\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138756\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138783\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138806\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138821\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138840\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138863\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138877\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138903\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138924\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138946\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138972\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 138990\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 139015\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0655\tNumber of valid training triplets in epoch: 139044\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139060\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139082\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139098\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139122\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139139\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139154\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139178\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139205\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139217\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139238\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139258\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139275\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139299\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139324\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139348\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0654\tNumber of valid training triplets in epoch: 139371\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139396\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139416\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139439\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139456\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139480\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139511\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139529\n",
            "Epoch 1:\tAverage Triplet Loss: 0.0653\tNumber of valid training triplets in epoch: 139547\n",
            "Eval Epoch Average Acc: 0.7900, Average Threshold: 0.2629\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0310\tNumber of valid training triplets in epoch: 23\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0222\tNumber of valid training triplets in epoch: 51\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0301\tNumber of valid training triplets in epoch: 63\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0273\tNumber of valid training triplets in epoch: 92\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0292\tNumber of valid training triplets in epoch: 106\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0267\tNumber of valid training triplets in epoch: 131\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 156\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 173\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0245\tNumber of valid training triplets in epoch: 199\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0233\tNumber of valid training triplets in epoch: 224\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0241\tNumber of valid training triplets in epoch: 242\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0241\tNumber of valid training triplets in epoch: 264\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0238\tNumber of valid training triplets in epoch: 285\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0239\tNumber of valid training triplets in epoch: 302\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0242\tNumber of valid training triplets in epoch: 325\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 336\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 355\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 370\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 387\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 401\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 423\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 444\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 465\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 487\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 503\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 525\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 548\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 570\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0252\tNumber of valid training triplets in epoch: 591\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 616\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0251\tNumber of valid training triplets in epoch: 632\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0251\tNumber of valid training triplets in epoch: 653\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0251\tNumber of valid training triplets in epoch: 672\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 689\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 705\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 726\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 748\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 770\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 794\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 811\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0252\tNumber of valid training triplets in epoch: 828\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 846\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0252\tNumber of valid training triplets in epoch: 866\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0252\tNumber of valid training triplets in epoch: 894\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 908\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 926\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 947\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 966\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 982\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0259\tNumber of valid training triplets in epoch: 998\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1020\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1039\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1062\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1086\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1105\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1123\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0260\tNumber of valid training triplets in epoch: 1136\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0259\tNumber of valid training triplets in epoch: 1158\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0259\tNumber of valid training triplets in epoch: 1175\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1196\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1216\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0260\tNumber of valid training triplets in epoch: 1238\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1266\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1293\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1313\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1334\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1346\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1365\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1384\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1410\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1428\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1445\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1460\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1479\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1497\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1521\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1546\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1565\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1590\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1604\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0259\tNumber of valid training triplets in epoch: 1618\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1643\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1665\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1682\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1703\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1722\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0258\tNumber of valid training triplets in epoch: 1741\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0259\tNumber of valid training triplets in epoch: 1760\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1786\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0257\tNumber of valid training triplets in epoch: 1811\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1835\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1852\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0256\tNumber of valid training triplets in epoch: 1878\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1896\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1917\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1938\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1962\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0255\tNumber of valid training triplets in epoch: 1984\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 2010\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 2028\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 2048\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 2071\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0254\tNumber of valid training triplets in epoch: 2093\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 2118\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 2137\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 2155\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 2172\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0253\tNumber of valid training triplets in epoch: 2191\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0251\tNumber of valid training triplets in epoch: 2222\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2247\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2268\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2286\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2311\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2324\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 2350\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2376\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2396\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2415\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 2439\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2452\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2471\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2491\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2514\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 2536\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 2557\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0245\tNumber of valid training triplets in epoch: 2582\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0245\tNumber of valid training triplets in epoch: 2603\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 2616\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0246\tNumber of valid training triplets in epoch: 2634\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2650\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2672\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2693\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 2710\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0247\tNumber of valid training triplets in epoch: 2728\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 2750\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 2769\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0248\tNumber of valid training triplets in epoch: 2786\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2798\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2816\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2835\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2857\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2871\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2898\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2912\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0250\tNumber of valid training triplets in epoch: 2931\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2954\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2978\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 2999\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3015\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3034\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3049\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3073\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3094\n",
            "Epoch 2:\tAverage Triplet Loss: 0.0249\tNumber of valid training triplets in epoch: 3119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzDfvn44x9vY",
        "colab_type": "text"
      },
      "source": [
        "**Triplet（Scratch）CosFace+ResNet18-IR_Version2**\n",
        "\n",
        "455595"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6X8myCmqor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79604d58-1646-400c-b2d5-b373757e7462"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntN0b4TSn8CB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e029708a-112f-4d1e-c549-e6bbad0c8d3e"
      },
      "source": [
        "!cd checkpoints;ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " se_resnet18_512_ArcFace_0000.pth\n",
            " se_resnet18_512_ArcFace_0001.pth\n",
            " se_resnet18_512_ArcFace_0002.pth\n",
            " se_resnet18_512_ArcFace_0003.pth\n",
            " se_resnet18_512_ArcFace_0004.pth\n",
            " se_resnet18_512_ArcFace_0005.pth\n",
            " se_resnet18_512_ArcFace_0006.pth\n",
            " se_resnet18_512_ArcFace_0007.pth\n",
            " se_resnet18_512_ArcFace_0008.pth\n",
            " se_resnet18_512_ArcFace_0009.pth\n",
            " se_resnet18_512_ArcFace_0010.pth\n",
            " se_resnet18_512_ArcFace_0011.pth\n",
            " se_resnet18_512_ArcFace_0012.pth\n",
            " se_resnet18_512_ArcFace_0013.pth\n",
            " se_resnet18_512_ArcFace_0014.pth\n",
            " se_resnet18_512_ArcFace_0015.pth\n",
            " se_resnet18_512_ArcFace_0016.pth\n",
            " se_resnet18_512_ArcFace_0017.pth\n",
            " se_resnet18_512_ArcFace_0018.pth\n",
            " se_resnet18_512_ArcFace_0019.pth\n",
            " se_resnet18_512_ArcFace_best.pth\n",
            " se_resnet18_512_CosFace_0000.pth\n",
            " se_resnet18_512_CosFace_0001.pth\n",
            " se_resnet18_512_CosFace_0002.pth\n",
            " se_resnet18_512_CosFace_0003.pth\n",
            " se_resnet18_512_CosFace_0004.pth\n",
            " se_resnet18_512_CosFace_0005.pth\n",
            " se_resnet18_512_CosFace_0006.pth\n",
            " se_resnet18_512_CosFace_0007.pth\n",
            " se_resnet18_512_CosFace_0008.pth\n",
            " se_resnet18_512_CosFace_0009.pth\n",
            " se_resnet18_512_CosFace_0010.pth\n",
            " se_resnet18_512_CosFace_0011.pth\n",
            " se_resnet18_512_CosFace_0012.pth\n",
            " se_resnet18_512_CosFace_0013.pth\n",
            " se_resnet18_512_CosFace_0014.pth\n",
            " se_resnet18_512_CosFace_0015.pth\n",
            " se_resnet18_512_CosFace_0016.pth\n",
            " se_resnet18_512_CosFace_0017.pth\n",
            " se_resnet18_512_CosFace_0018.pth\n",
            " se_resnet18_512_CosFace_0019.pth\n",
            " se_resnet18_512_CosFace_best.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0000.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0001.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0002.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0003.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0004.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0005.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0006.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0007.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0008.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0009.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0010.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0011.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0012.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0013.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0014.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0015.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0016.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0017.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0018.pth\n",
            " se_resnet18_512_CosFace_Contrastive_0019.pth\n",
            " se_resnet18_512_CosFace_Contrastive_best.pth\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0000.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0001.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0002.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0003.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0004.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0005.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0006.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0007.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0008.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0009.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0010.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0011.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0012.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0013.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0014.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0015.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0016.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0017.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0018.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_0019.pth'\n",
            "'se_resnet18_512_Linear(in_features=512, out_features=775, bias=True)_best.pth'\n",
            " se_resnet18_512_NormFace_0000.pth\n",
            " se_resnet18_512_NormFace_0001.pth\n",
            " se_resnet18_512_NormFace_0002.pth\n",
            " se_resnet18_512_NormFace_0003.pth\n",
            " se_resnet18_512_NormFace_0004.pth\n",
            " se_resnet18_512_NormFace_0005.pth\n",
            " se_resnet18_512_NormFace_0006.pth\n",
            " se_resnet18_512_NormFace_0007.pth\n",
            " se_resnet18_512_NormFace_0008.pth\n",
            " se_resnet18_512_NormFace_0009.pth\n",
            " se_resnet18_512_NormFace_0010.pth\n",
            " se_resnet18_512_NormFace_0011.pth\n",
            " se_resnet18_512_NormFace_0012.pth\n",
            " se_resnet18_512_NormFace_0013.pth\n",
            " se_resnet18_512_NormFace_0014.pth\n",
            " se_resnet18_512_NormFace_0015.pth\n",
            " se_resnet18_512_NormFace_0016.pth\n",
            " se_resnet18_512_NormFace_0017.pth\n",
            " se_resnet18_512_NormFace_0018.pth\n",
            " se_resnet18_512_NormFace_0019.pth\n",
            " se_resnet18_512_NormFace_best.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0000.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0001.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0002.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0003.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0004.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0005.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0006.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0007.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0008.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0009.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0010.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0011.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0012.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0013.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0014.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0015.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0016.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0017.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0018.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_0019.pth\n",
            " se_resnet18_512_NormFace_FocalLoss_best.pth\n",
            " se_resnet18_512_NormFace_OHEM_0000.pth\n",
            " se_resnet18_512_NormFace_OHEM_0001.pth\n",
            " se_resnet18_512_NormFace_OHEM_0002.pth\n",
            " se_resnet18_512_NormFace_OHEM_0003.pth\n",
            " se_resnet18_512_NormFace_OHEM_0004.pth\n",
            " se_resnet18_512_NormFace_OHEM_0005.pth\n",
            " se_resnet18_512_NormFace_OHEM_0006.pth\n",
            " se_resnet18_512_NormFace_OHEM_0007.pth\n",
            " se_resnet18_512_NormFace_OHEM_0008.pth\n",
            " se_resnet18_512_NormFace_OHEM_0009.pth\n",
            " se_resnet18_512_NormFace_OHEM_0010.pth\n",
            " se_resnet18_512_NormFace_OHEM_0011.pth\n",
            " se_resnet18_512_NormFace_OHEM_0012.pth\n",
            " se_resnet18_512_NormFace_OHEM_0013.pth\n",
            " se_resnet18_512_NormFace_OHEM_0014.pth\n",
            " se_resnet18_512_NormFace_OHEM_0015.pth\n",
            " se_resnet18_512_NormFace_OHEM_0016.pth\n",
            " se_resnet18_512_NormFace_OHEM_0017.pth\n",
            " se_resnet18_512_NormFace_OHEM_0018.pth\n",
            " se_resnet18_512_NormFace_OHEM_0019.pth\n",
            " se_resnet18_512_NormFace_OHEM_best.pth\n",
            " se_resnet18_512_SphereFace_0000.pth\n",
            " se_resnet18_512_SphereFace_0001.pth\n",
            " se_resnet18_512_SphereFace_0002.pth\n",
            " se_resnet18_512_SphereFace_0003.pth\n",
            " se_resnet18_512_SphereFace_0004.pth\n",
            " se_resnet18_512_SphereFace_0005.pth\n",
            " se_resnet18_512_SphereFace_0006.pth\n",
            " se_resnet18_512_SphereFace_0007.pth\n",
            " se_resnet18_512_SphereFace_0008.pth\n",
            " se_resnet18_512_SphereFace_0009.pth\n",
            " se_resnet18_512_SphereFace_0010.pth\n",
            " se_resnet18_512_SphereFace_0011.pth\n",
            " se_resnet18_512_SphereFace_0012.pth\n",
            " se_resnet18_512_SphereFace_0013.pth\n",
            " se_resnet18_512_SphereFace_0014.pth\n",
            " se_resnet18_512_SphereFace_0015.pth\n",
            " se_resnet18_512_SphereFace_0016.pth\n",
            " se_resnet18_512_SphereFace_0017.pth\n",
            " se_resnet18_512_SphereFace_0018.pth\n",
            " se_resnet18_512_SphereFace_0019.pth\n",
            " se_resnet18_512_SphereFace_best.pth\n",
            " se_resnet18_ArcFace_0000.pth\n",
            " se_resnet18_ArcFace_0001.pth\n",
            " se_resnet18_ArcFace_0002.pth\n",
            " se_resnet18_ArcFace_0003.pth\n",
            " se_resnet18_ArcFace_0004.pth\n",
            " se_resnet18_ArcFace_0005.pth\n",
            " se_resnet18_ArcFace_0006.pth\n",
            " se_resnet18_ArcFace_0007.pth\n",
            " se_resnet18_ArcFace_0008.pth\n",
            " se_resnet18_ArcFace_0009.pth\n",
            " se_resnet18_ArcFace_0010.pth\n",
            " se_resnet18_ArcFace_0011.pth\n",
            " se_resnet18_ArcFace_0012.pth\n",
            " se_resnet18_ArcFace_0013.pth\n",
            " se_resnet18_ArcFace_0014.pth\n",
            " se_resnet18_ArcFace_0015.pth\n",
            " se_resnet18_ArcFace_0016.pth\n",
            " se_resnet18_ArcFace_0017.pth\n",
            " se_resnet18_ArcFace_0018.pth\n",
            " se_resnet18_ArcFace_0019.pth\n",
            " se_resnet18_ArcFace_best.pth\n",
            " se_resnet18_CosFace_0000.pth\n",
            " se_resnet18_CosFace_0001.pth\n",
            " se_resnet18_CosFace_0002.pth\n",
            " se_resnet18_CosFace_0003.pth\n",
            " se_resnet18_CosFace_0004.pth\n",
            " se_resnet18_CosFace_0005.pth\n",
            " se_resnet18_CosFace_0006.pth\n",
            " se_resnet18_CosFace_0007.pth\n",
            " se_resnet18_CosFace_0008.pth\n",
            " se_resnet18_CosFace_0009.pth\n",
            " se_resnet18_CosFace_0010.pth\n",
            " se_resnet18_CosFace_0011.pth\n",
            " se_resnet18_CosFace_0012.pth\n",
            " se_resnet18_CosFace_0013.pth\n",
            " se_resnet18_CosFace_0014.pth\n",
            " se_resnet18_CosFace_0015.pth\n",
            " se_resnet18_CosFace_0016.pth\n",
            " se_resnet18_CosFace_0017.pth\n",
            " se_resnet18_CosFace_0018.pth\n",
            " se_resnet18_CosFace_0019.pth\n",
            " se_resnet18_CosFace_best.pth\n",
            " se_resnet18_CosFace_Contrastive_0000.pth\n",
            " se_resnet18_CosFace_Contrastive_0001.pth\n",
            " se_resnet18_CosFace_Contrastive_0002.pth\n",
            " se_resnet18_CosFace_Contrastive_0003.pth\n",
            " se_resnet18_CosFace_Contrastive_0004.pth\n",
            " se_resnet18_CosFace_Contrastive_0005.pth\n",
            " se_resnet18_CosFace_Contrastive_0006.pth\n",
            " se_resnet18_CosFace_Contrastive_0007.pth\n",
            " se_resnet18_CosFace_Contrastive_0008.pth\n",
            " se_resnet18_CosFace_Contrastive_0009.pth\n",
            " se_resnet18_CosFace_Contrastive_0010.pth\n",
            " se_resnet18_CosFace_Contrastive_0011.pth\n",
            " se_resnet18_CosFace_Contrastive_0012.pth\n",
            " se_resnet18_CosFace_Contrastive_0013.pth\n",
            " se_resnet18_CosFace_Contrastive_0014.pth\n",
            " se_resnet18_CosFace_Contrastive_0015.pth\n",
            " se_resnet18_CosFace_Contrastive_0016.pth\n",
            " se_resnet18_CosFace_Contrastive_0017.pth\n",
            " se_resnet18_CosFace_Contrastive_0018.pth\n",
            " se_resnet18_CosFace_Contrastive_0019.pth\n",
            " se_resnet18_CosFace_Contrastive_best.pth\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0000.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0001.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0002.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0003.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0004.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0005.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0006.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0007.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0008.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0009.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0010.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0011.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0012.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0013.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0014.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0015.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0016.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0017.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0018.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_0019.pth'\n",
            "'se_resnet18_Linear(in_features=512, out_features=775, bias=True)_best.pth'\n",
            " se_resnet18_NormFace_0000.pth\n",
            " se_resnet18_NormFace_0001.pth\n",
            " se_resnet18_NormFace_0002.pth\n",
            " se_resnet18_NormFace_0003.pth\n",
            " se_resnet18_NormFace_0004.pth\n",
            " se_resnet18_NormFace_0005.pth\n",
            " se_resnet18_NormFace_0006.pth\n",
            " se_resnet18_NormFace_0007.pth\n",
            " se_resnet18_NormFace_0008.pth\n",
            " se_resnet18_NormFace_0009.pth\n",
            " se_resnet18_NormFace_0010.pth\n",
            " se_resnet18_NormFace_0011.pth\n",
            " se_resnet18_NormFace_0012.pth\n",
            " se_resnet18_NormFace_0013.pth\n",
            " se_resnet18_NormFace_0014.pth\n",
            " se_resnet18_NormFace_0015.pth\n",
            " se_resnet18_NormFace_0016.pth\n",
            " se_resnet18_NormFace_0017.pth\n",
            " se_resnet18_NormFace_0018.pth\n",
            " se_resnet18_NormFace_0019.pth\n",
            " se_resnet18_NormFace_best.pth\n",
            " se_resnet18_NormFace_FocalLoss_0000.pth\n",
            " se_resnet18_NormFace_FocalLoss_0001.pth\n",
            " se_resnet18_NormFace_FocalLoss_0002.pth\n",
            " se_resnet18_NormFace_FocalLoss_0003.pth\n",
            " se_resnet18_NormFace_FocalLoss_0004.pth\n",
            " se_resnet18_NormFace_FocalLoss_0005.pth\n",
            " se_resnet18_NormFace_FocalLoss_0006.pth\n",
            " se_resnet18_NormFace_FocalLoss_0007.pth\n",
            " se_resnet18_NormFace_FocalLoss_0008.pth\n",
            " se_resnet18_NormFace_FocalLoss_0009.pth\n",
            " se_resnet18_NormFace_FocalLoss_0010.pth\n",
            " se_resnet18_NormFace_FocalLoss_0011.pth\n",
            " se_resnet18_NormFace_FocalLoss_0012.pth\n",
            " se_resnet18_NormFace_FocalLoss_0013.pth\n",
            " se_resnet18_NormFace_FocalLoss_0014.pth\n",
            " se_resnet18_NormFace_FocalLoss_0015.pth\n",
            " se_resnet18_NormFace_FocalLoss_0016.pth\n",
            " se_resnet18_NormFace_FocalLoss_0017.pth\n",
            " se_resnet18_NormFace_FocalLoss_0018.pth\n",
            " se_resnet18_NormFace_FocalLoss_0019.pth\n",
            " se_resnet18_NormFace_FocalLoss_best.pth\n",
            " se_resnet18_NormFace_OHEM_0000.pth\n",
            " se_resnet18_NormFace_OHEM_0001.pth\n",
            " se_resnet18_NormFace_OHEM_0002.pth\n",
            " se_resnet18_NormFace_OHEM_0003.pth\n",
            " se_resnet18_NormFace_OHEM_0004.pth\n",
            " se_resnet18_NormFace_OHEM_0005.pth\n",
            " se_resnet18_NormFace_OHEM_0006.pth\n",
            " se_resnet18_NormFace_OHEM_0007.pth\n",
            " se_resnet18_NormFace_OHEM_0008.pth\n",
            " se_resnet18_NormFace_OHEM_0009.pth\n",
            " se_resnet18_NormFace_OHEM_0010.pth\n",
            " se_resnet18_NormFace_OHEM_0011.pth\n",
            " se_resnet18_NormFace_OHEM_0012.pth\n",
            " se_resnet18_NormFace_OHEM_0013.pth\n",
            " se_resnet18_NormFace_OHEM_0014.pth\n",
            " se_resnet18_NormFace_OHEM_0015.pth\n",
            " se_resnet18_NormFace_OHEM_0016.pth\n",
            " se_resnet18_NormFace_OHEM_0017.pth\n",
            " se_resnet18_NormFace_OHEM_0018.pth\n",
            " se_resnet18_NormFace_OHEM_0019.pth\n",
            " se_resnet18_NormFace_OHEM_best.pth\n",
            " se_resnet18_SphereFace_0000.pth\n",
            " se_resnet18_SphereFace_0001.pth\n",
            " se_resnet18_SphereFace_0002.pth\n",
            " se_resnet18_SphereFace_0003.pth\n",
            " se_resnet18_SphereFace_0004.pth\n",
            " se_resnet18_SphereFace_0005.pth\n",
            " se_resnet18_SphereFace_0006.pth\n",
            " se_resnet18_SphereFace_0007.pth\n",
            " se_resnet18_SphereFace_0008.pth\n",
            " se_resnet18_SphereFace_0009.pth\n",
            " se_resnet18_SphereFace_0010.pth\n",
            " se_resnet18_SphereFace_0011.pth\n",
            " se_resnet18_SphereFace_0012.pth\n",
            " se_resnet18_SphereFace_0013.pth\n",
            " se_resnet18_SphereFace_0014.pth\n",
            " se_resnet18_SphereFace_0015.pth\n",
            " se_resnet18_SphereFace_0016.pth\n",
            " se_resnet18_SphereFace_0017.pth\n",
            " se_resnet18_SphereFace_0018.pth\n",
            " se_resnet18_SphereFace_0019.pth\n",
            " se_resnet18_SphereFace_best.pth\n",
            " se_resnet34_512_ArcFace_0000.pth\n",
            " se_resnet34_512_ArcFace_0001.pth\n",
            " se_resnet34_512_ArcFace_0002.pth\n",
            " se_resnet34_512_ArcFace_0003.pth\n",
            " se_resnet34_512_ArcFace_0004.pth\n",
            " se_resnet34_512_ArcFace_0005.pth\n",
            " se_resnet34_512_ArcFace_0006.pth\n",
            " se_resnet34_512_ArcFace_0007.pth\n",
            " se_resnet34_512_ArcFace_0008.pth\n",
            " se_resnet34_512_ArcFace_0009.pth\n",
            " se_resnet34_512_ArcFace_0010.pth\n",
            " se_resnet34_512_ArcFace_0011.pth\n",
            " se_resnet34_512_ArcFace_0012.pth\n",
            " se_resnet34_512_ArcFace_0013.pth\n",
            " se_resnet34_512_ArcFace_0014.pth\n",
            " se_resnet34_512_ArcFace_0015.pth\n",
            " se_resnet34_512_ArcFace_0016.pth\n",
            " se_resnet34_512_ArcFace_0017.pth\n",
            " se_resnet34_512_ArcFace_0018.pth\n",
            " se_resnet34_512_ArcFace_0019.pth\n",
            " se_resnet34_512_ArcFace_best.pth\n",
            " se_resnet34_512_CosFace_0000.pth\n",
            " se_resnet34_512_CosFace_0001.pth\n",
            " se_resnet34_512_CosFace_0002.pth\n",
            " se_resnet34_512_CosFace_0003.pth\n",
            " se_resnet34_512_CosFace_0004.pth\n",
            " se_resnet34_512_CosFace_0005.pth\n",
            " se_resnet34_512_CosFace_0006.pth\n",
            " se_resnet34_512_CosFace_0007.pth\n",
            " se_resnet34_512_CosFace_0008.pth\n",
            " se_resnet34_512_CosFace_0009.pth\n",
            " se_resnet34_512_CosFace_0010.pth\n",
            " se_resnet34_512_CosFace_0011.pth\n",
            " se_resnet34_512_CosFace_0012.pth\n",
            " se_resnet34_512_CosFace_0013.pth\n",
            " se_resnet34_512_CosFace_0014.pth\n",
            " se_resnet34_512_CosFace_0015.pth\n",
            " se_resnet34_512_CosFace_0016.pth\n",
            " se_resnet34_512_CosFace_0017.pth\n",
            " se_resnet34_512_CosFace_0018.pth\n",
            " se_resnet34_512_CosFace_0019.pth\n",
            " se_resnet34_512_CosFace_best.pth\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0000.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0001.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0002.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0003.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0004.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0005.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0006.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0007.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0008.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0009.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0010.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0011.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0012.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0013.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0014.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0015.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0016.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0017.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0018.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_0019.pth'\n",
            "'se_resnet34_512_Linear(in_features=512, out_features=775, bias=True)_best.pth'\n",
            " se_resnet34_512_NormFace_0000.pth\n",
            " se_resnet34_512_NormFace_0001.pth\n",
            " se_resnet34_512_NormFace_0002.pth\n",
            " se_resnet34_512_NormFace_0003.pth\n",
            " se_resnet34_512_NormFace_0004.pth\n",
            " se_resnet34_512_NormFace_0005.pth\n",
            " se_resnet34_512_NormFace_0006.pth\n",
            " se_resnet34_512_NormFace_0007.pth\n",
            " se_resnet34_512_NormFace_0008.pth\n",
            " se_resnet34_512_NormFace_0009.pth\n",
            " se_resnet34_512_NormFace_0010.pth\n",
            " se_resnet34_512_NormFace_0011.pth\n",
            " se_resnet34_512_NormFace_0012.pth\n",
            " se_resnet34_512_NormFace_0013.pth\n",
            " se_resnet34_512_NormFace_0014.pth\n",
            " se_resnet34_512_NormFace_0015.pth\n",
            " se_resnet34_512_NormFace_0016.pth\n",
            " se_resnet34_512_NormFace_0017.pth\n",
            " se_resnet34_512_NormFace_0018.pth\n",
            " se_resnet34_512_NormFace_0019.pth\n",
            " se_resnet34_512_NormFace_best.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0000.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0001.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0002.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0003.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0004.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0005.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0006.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0007.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0008.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0009.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0010.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0011.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0012.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0013.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0014.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0015.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0016.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0017.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0018.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_0019.pth\n",
            " se_resnet34_512_NormFace_FocalLoss_best.pth\n",
            " se_resnet34_512_NormFace_OHEM_0000.pth\n",
            " se_resnet34_512_NormFace_OHEM_0001.pth\n",
            " se_resnet34_512_NormFace_OHEM_0002.pth\n",
            " se_resnet34_512_NormFace_OHEM_0003.pth\n",
            " se_resnet34_512_NormFace_OHEM_0004.pth\n",
            " se_resnet34_512_NormFace_OHEM_0005.pth\n",
            " se_resnet34_512_NormFace_OHEM_0006.pth\n",
            " se_resnet34_512_NormFace_OHEM_0007.pth\n",
            " se_resnet34_512_NormFace_OHEM_0008.pth\n",
            " se_resnet34_512_NormFace_OHEM_0009.pth\n",
            " se_resnet34_512_NormFace_OHEM_0010.pth\n",
            " se_resnet34_512_NormFace_OHEM_0011.pth\n",
            " se_resnet34_512_NormFace_OHEM_0012.pth\n",
            " se_resnet34_512_NormFace_OHEM_0013.pth\n",
            " se_resnet34_512_NormFace_OHEM_0014.pth\n",
            " se_resnet34_512_NormFace_OHEM_0015.pth\n",
            " se_resnet34_512_NormFace_OHEM_0016.pth\n",
            " se_resnet34_512_NormFace_OHEM_0017.pth\n",
            " se_resnet34_512_NormFace_OHEM_0018.pth\n",
            " se_resnet34_512_NormFace_OHEM_0019.pth\n",
            " se_resnet34_512_NormFace_OHEM_best.pth\n",
            " se_resnet34_512_SphereFace_0000.pth\n",
            " se_resnet34_512_SphereFace_0001.pth\n",
            " se_resnet34_512_SphereFace_0002.pth\n",
            " se_resnet34_512_SphereFace_0003.pth\n",
            " se_resnet34_512_SphereFace_0004.pth\n",
            " se_resnet34_512_SphereFace_0005.pth\n",
            " se_resnet34_512_SphereFace_0006.pth\n",
            " se_resnet34_512_SphereFace_0007.pth\n",
            " se_resnet34_512_SphereFace_0008.pth\n",
            " se_resnet34_512_SphereFace_0009.pth\n",
            " se_resnet34_512_SphereFace_0010.pth\n",
            " se_resnet34_512_SphereFace_0011.pth\n",
            " se_resnet34_512_SphereFace_0012.pth\n",
            " se_resnet34_512_SphereFace_0013.pth\n",
            " se_resnet34_512_SphereFace_0014.pth\n",
            " se_resnet34_512_SphereFace_0015.pth\n",
            " se_resnet34_512_SphereFace_0016.pth\n",
            " se_resnet34_512_SphereFace_0017.pth\n",
            " se_resnet34_512_SphereFace_0018.pth\n",
            " se_resnet34_512_SphereFace_0019.pth\n",
            " se_resnet34_512_SphereFace_best.pth\n",
            " se_resnet34_ArcFace_0000.pth\n",
            " se_resnet34_ArcFace_0001.pth\n",
            " se_resnet34_ArcFace_0002.pth\n",
            " se_resnet34_ArcFace_0003.pth\n",
            " se_resnet34_ArcFace_0004.pth\n",
            " se_resnet34_ArcFace_0005.pth\n",
            " se_resnet34_ArcFace_0006.pth\n",
            " se_resnet34_ArcFace_0007.pth\n",
            " se_resnet34_ArcFace_0008.pth\n",
            " se_resnet34_ArcFace_0009.pth\n",
            " se_resnet34_ArcFace_0010.pth\n",
            " se_resnet34_ArcFace_0011.pth\n",
            " se_resnet34_ArcFace_0012.pth\n",
            " se_resnet34_ArcFace_0013.pth\n",
            " se_resnet34_ArcFace_0014.pth\n",
            " se_resnet34_ArcFace_0015.pth\n",
            " se_resnet34_ArcFace_0016.pth\n",
            " se_resnet34_ArcFace_0017.pth\n",
            " se_resnet34_ArcFace_0018.pth\n",
            " se_resnet34_ArcFace_0019.pth\n",
            " se_resnet34_ArcFace_best.pth\n",
            " se_resnet34_CosFace_0000.pth\n",
            " se_resnet34_CosFace_0001.pth\n",
            " se_resnet34_CosFace_0002.pth\n",
            " se_resnet34_CosFace_0003.pth\n",
            " se_resnet34_CosFace_0004.pth\n",
            " se_resnet34_CosFace_0005.pth\n",
            " se_resnet34_CosFace_0006.pth\n",
            " se_resnet34_CosFace_0007.pth\n",
            " se_resnet34_CosFace_0008.pth\n",
            " se_resnet34_CosFace_0009.pth\n",
            " se_resnet34_CosFace_0010.pth\n",
            " se_resnet34_CosFace_0011.pth\n",
            " se_resnet34_CosFace_0012.pth\n",
            " se_resnet34_CosFace_0013.pth\n",
            " se_resnet34_CosFace_0014.pth\n",
            " se_resnet34_CosFace_0015.pth\n",
            " se_resnet34_CosFace_0016.pth\n",
            " se_resnet34_CosFace_0017.pth\n",
            " se_resnet34_CosFace_0018.pth\n",
            " se_resnet34_CosFace_0019.pth\n",
            " se_resnet34_CosFace_best.pth\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0000.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0001.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0002.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0003.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0004.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0005.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0006.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0007.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0008.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0009.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0010.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0011.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0012.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0013.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0014.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0015.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0016.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0017.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0018.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_0019.pth'\n",
            "'se_resnet34_Linear(in_features=512, out_features=775, bias=True)_best.pth'\n",
            " se_resnet34_NormFace_0000.pth\n",
            " se_resnet34_NormFace_0001.pth\n",
            " se_resnet34_NormFace_0002.pth\n",
            " se_resnet34_NormFace_0003.pth\n",
            " se_resnet34_NormFace_0004.pth\n",
            " se_resnet34_NormFace_0005.pth\n",
            " se_resnet34_NormFace_0006.pth\n",
            " se_resnet34_NormFace_0007.pth\n",
            " se_resnet34_NormFace_0008.pth\n",
            " se_resnet34_NormFace_0009.pth\n",
            " se_resnet34_NormFace_0010.pth\n",
            " se_resnet34_NormFace_0011.pth\n",
            " se_resnet34_NormFace_0012.pth\n",
            " se_resnet34_NormFace_0013.pth\n",
            " se_resnet34_NormFace_0014.pth\n",
            " se_resnet34_NormFace_0015.pth\n",
            " se_resnet34_NormFace_0016.pth\n",
            " se_resnet34_NormFace_0017.pth\n",
            " se_resnet34_NormFace_0018.pth\n",
            " se_resnet34_NormFace_0019.pth\n",
            " se_resnet34_NormFace_best.pth\n",
            " se_resnet34_NormFace_FocalLoss_0000.pth\n",
            " se_resnet34_NormFace_FocalLoss_0001.pth\n",
            " se_resnet34_NormFace_FocalLoss_0002.pth\n",
            " se_resnet34_NormFace_FocalLoss_0003.pth\n",
            " se_resnet34_NormFace_FocalLoss_0004.pth\n",
            " se_resnet34_NormFace_FocalLoss_0005.pth\n",
            " se_resnet34_NormFace_FocalLoss_0006.pth\n",
            " se_resnet34_NormFace_FocalLoss_0007.pth\n",
            " se_resnet34_NormFace_FocalLoss_0008.pth\n",
            " se_resnet34_NormFace_FocalLoss_0009.pth\n",
            " se_resnet34_NormFace_FocalLoss_0010.pth\n",
            " se_resnet34_NormFace_FocalLoss_0011.pth\n",
            " se_resnet34_NormFace_FocalLoss_0012.pth\n",
            " se_resnet34_NormFace_FocalLoss_0013.pth\n",
            " se_resnet34_NormFace_FocalLoss_0014.pth\n",
            " se_resnet34_NormFace_FocalLoss_0015.pth\n",
            " se_resnet34_NormFace_FocalLoss_0016.pth\n",
            " se_resnet34_NormFace_FocalLoss_0017.pth\n",
            " se_resnet34_NormFace_FocalLoss_0018.pth\n",
            " se_resnet34_NormFace_FocalLoss_0019.pth\n",
            " se_resnet34_NormFace_FocalLoss_best.pth\n",
            " se_resnet34_NormFace_OHEM_0000.pth\n",
            " se_resnet34_NormFace_OHEM_0001.pth\n",
            " se_resnet34_NormFace_OHEM_0002.pth\n",
            " se_resnet34_NormFace_OHEM_0003.pth\n",
            " se_resnet34_NormFace_OHEM_0004.pth\n",
            " se_resnet34_NormFace_OHEM_0005.pth\n",
            " se_resnet34_NormFace_OHEM_0006.pth\n",
            " se_resnet34_NormFace_OHEM_0007.pth\n",
            " se_resnet34_NormFace_OHEM_0008.pth\n",
            " se_resnet34_NormFace_OHEM_0009.pth\n",
            " se_resnet34_NormFace_OHEM_0010.pth\n",
            " se_resnet34_NormFace_OHEM_0011.pth\n",
            " se_resnet34_NormFace_OHEM_0012.pth\n",
            " se_resnet34_NormFace_OHEM_0013.pth\n",
            " se_resnet34_NormFace_OHEM_0014.pth\n",
            " se_resnet34_NormFace_OHEM_0015.pth\n",
            " se_resnet34_NormFace_OHEM_0016.pth\n",
            " se_resnet34_NormFace_OHEM_0017.pth\n",
            " se_resnet34_NormFace_OHEM_0018.pth\n",
            " se_resnet34_NormFace_OHEM_0019.pth\n",
            " se_resnet34_NormFace_OHEM_best.pth\n",
            " se_resnet34_SphereFace_0000.pth\n",
            " se_resnet34_SphereFace_0001.pth\n",
            " se_resnet34_SphereFace_0002.pth\n",
            " se_resnet34_SphereFace_0003.pth\n",
            " se_resnet34_SphereFace_0004.pth\n",
            " se_resnet34_SphereFace_0005.pth\n",
            " se_resnet34_SphereFace_0006.pth\n",
            " se_resnet34_SphereFace_0007.pth\n",
            " se_resnet34_SphereFace_0008.pth\n",
            " se_resnet34_SphereFace_0009.pth\n",
            " se_resnet34_SphereFace_0010.pth\n",
            " se_resnet34_SphereFace_0011.pth\n",
            " se_resnet34_SphereFace_0012.pth\n",
            " se_resnet34_SphereFace_0013.pth\n",
            " se_resnet34_SphereFace_0014.pth\n",
            " se_resnet34_SphereFace_0015.pth\n",
            " se_resnet34_SphereFace_0016.pth\n",
            " se_resnet34_SphereFace_0017.pth\n",
            " se_resnet34_SphereFace_0018.pth\n",
            " se_resnet34_SphereFace_0019.pth\n",
            " se_resnet34_SphereFace_best.pth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}